Requirement already satisfied: torchvision in ./.local/lib/python3.7/site-packages (0.5.0)Requirement already satisfied: pillow>=4.1.1 in ./.local/lib/python3.7/site-packages (from torchvision) (7.0.0)Requirement already satisfied: six in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from torchvision) (1.12.0)Requirement already satisfied: numpy in ./.local/lib/python3.7/site-packages (from torchvision) (1.18.1)Requirement already satisfied: torch==1.4.0 in ./.local/lib/python3.7/site-packages (from torchvision) (1.4.0)###########################################################################################################{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'batch_size': 4, 'workers': 2, 'model_archi': 33, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=33, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)Files already downloaded and verifiedFiles already downloaded and verifiedcuda:012500 deer truck   car  frog['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']Model wide_resnet101_2 LoadedResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): Bottleneck(      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer2): Sequential(    (0): Bottleneck(      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer3): Sequential(    (0): Bottleneck(      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (4): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (5): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (6): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (7): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (8): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (9): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (10): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (11): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (12): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (13): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (14): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (15): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (16): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (17): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (18): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (19): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (20): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (21): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (22): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer4): Sequential(    (0): Bottleneck(      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=2048, out_features=10, bias=True))Model wide_resnet101_2 ReshapedSending model to GPULearning Rate: 0.001, Weight Decay: 0, Momentum: 0Defined <class 'torch.optim.sgd.SGD'> OptimizerStarting Training at 1582574517.5621574[1,   500] loss: 2.62941325.346823930740356[1,  1000] loss: 2.62117148.62659478187561[1,  1500] loss: 2.58821972.25617265701294[1,  2000] loss: 2.49671495.78286528587341[1,  2500] loss: 2.460970118.52918767929077[1,  3000] loss: 2.459719140.97769355773926[1,  3500] loss: 2.453105163.621919631958[1,  4000] loss: 2.389689186.18430876731873[1,  4500] loss: 2.377524208.91612029075623[1,  5000] loss: 2.344446231.66262984275818[1,  5500] loss: 2.380969254.3022541999817[1,  6000] loss: 2.352058276.8711543083191[1,  6500] loss: 2.321996299.44316601753235[1,  7000] loss: 2.323188321.9143421649933[1,  7500] loss: 2.341229344.47910928726196[1,  8000] loss: 2.307796367.08110189437866[1,  8500] loss: 2.284991389.84906029701233[1,  9000] loss: 2.308888412.2166266441345[1,  9500] loss: 2.292622434.6047856807709[1, 10000] loss: 2.234069457.04818177223206[1, 10500] loss: 2.287225480.09852409362793[1, 11000] loss: 2.285833503.4534523487091[1, 11500] loss: 2.248647526.2663724422455[1, 12000] loss: 2.291537548.8635444641113[1, 12500] loss: 2.291205571.4445745944977Epoch [1] loss: 7442336.783264[2,   500] loss: 2.301148594.1804580688477[2,  1000] loss: 2.279969616.678929567337[2,  1500] loss: 2.234142639.3578696250916[2,  2000] loss: 2.232947661.9086601734161[2,  2500] loss: 2.265630684.5917959213257[2,  3000] loss: 2.256438707.6196553707123[2,  3500] loss: 2.258754730.2961406707764[2,  4000] loss: 2.269867752.7391216754913[2,  4500] loss: 2.286543775.3478434085846[2,  5000] loss: 2.274844797.8746702671051[2,  5500] loss: 2.239763820.4407622814178[2,  6000] loss: 2.230338843.0729818344116[2,  6500] loss: 2.240065865.7997515201569[2,  7000] loss: 2.230530888.4929113388062[2,  7500] loss: 2.260802911.0974459648132[2,  8000] loss: 2.295563933.6335070133209[2,  8500] loss: 2.234125956.1423575878143[2,  9000] loss: 2.283839978.6890110969543[2,  9500] loss: 2.2431231001.3293113708496[2, 10000] loss: 2.2578511023.9123809337616[2, 10500] loss: 2.2429311046.4955723285675[2, 11000] loss: 2.2410151069.0726850032806[2, 11500] loss: 2.2774111091.7070350646973[2, 12000] loss: 2.2381791114.254382610321[2, 12500] loss: 2.2327001136.8154859542847Epoch [2] loss: 7074820.774906[3,   500] loss: 2.2310291159.4233973026276[3,  1000] loss: 2.1705031181.8958492279053[3,  1500] loss: 2.2043111204.591640472412[3,  2000] loss: 2.1736161227.626323223114[3,  2500] loss: 2.2175851250.3125[3,  3000] loss: 2.1761081272.855565071106[3,  3500] loss: 2.2174651295.462239265442[3,  4000] loss: 2.1649951318.1043746471405[3,  4500] loss: 2.1750521340.8349726200104[3,  5000] loss: 2.1817531363.2921278476715[3,  5500] loss: 2.1497071386.019151210785[3,  6000] loss: 2.1483091408.7698709964752[3,  6500] loss: 2.1673671431.7467741966248[3,  7000] loss: 2.1112541454.3326683044434[3,  7500] loss: 2.2871181476.759262561798[3,  8000] loss: 2.2464611499.1286489963531[3,  8500] loss: 2.2366841521.6945850849152[3,  9000] loss: 2.2020411544.180892944336[3,  9500] loss: 2.2104071567.3433179855347[3, 10000] loss: 2.2056471590.2481474876404[3, 10500] loss: 2.1771451613.045908689499[3, 11000] loss: 2.1887511635.6467306613922[3, 11500] loss: 2.1955111659.4048805236816[3, 12000] loss: 2.1520091681.8661260604858[3, 12500] loss: 2.2136871704.461047410965Epoch [3] loss: 6874690.734072[4,   500] loss: 2.1552811727.3293902873993[4,  1000] loss: 2.1430831749.8694152832031[4,  1500] loss: 2.1753041772.6014318466187[4,  2000] loss: 2.1337581795.2112369537354[4,  2500] loss: 2.1685991817.812829732895[4,  3000] loss: 2.1570771840.4484877586365[4,  3500] loss: 2.1365941863.2655036449432[4,  4000] loss: 2.1516591885.9420804977417[4,  4500] loss: 2.1308251908.7823684215546[4,  5000] loss: 2.1555991931.4010663032532[4,  5500] loss: 2.1939111953.8861815929413[4,  6000] loss: 2.1328631976.6688108444214[4,  6500] loss: 2.1271911999.2270839214325[4,  7000] loss: 2.1424672021.7832715511322[4,  7500] loss: 2.1098492044.4784488677979[4,  8000] loss: 2.1123512067.143612384796[4,  8500] loss: 2.0965912089.70810008049[4,  9000] loss: 2.1532002112.4978029727936[4,  9500] loss: 2.1304302135.0142509937286[4, 10000] loss: 2.1118192157.920163154602[4, 10500] loss: 2.1231142181.0400671958923[4, 11000] loss: 2.0880142204.1357805728912[4, 11500] loss: 2.1200472226.7058408260345[4, 12000] loss: 2.0823142249.461931705475[4, 12500] loss: 2.0858342272.551571369171Epoch [4] loss: 6692255.684338[5,   500] loss: 2.0670902295.61546254158[5,  1000] loss: 2.0859162318.412049770355[5,  1500] loss: 2.0794172340.9546723365784[5,  2000] loss: 2.0740152363.550310611725[5,  2500] loss: 2.0681822386.213641166687[5,  3000] loss: 2.0972382408.9236211776733[5,  3500] loss: 2.1260302432.0397005081177[5,  4000] loss: 2.1014362454.652054786682[5,  4500] loss: 2.0359212477.291622877121[5,  5000] loss: 2.1053492500.0698573589325[5,  5500] loss: 2.0547962522.66158080101[5,  6000] loss: 2.0636182545.244073152542[5,  6500] loss: 2.0652952568.3182475566864[5,  7000] loss: 2.0768282590.9251914024353[5,  7500] loss: 2.0742502613.454414367676[5,  8000] loss: 2.0645742636.131744146347[5,  8500] loss: 2.0403482658.776360273361[5,  9000] loss: 2.0673352681.638189315796[5,  9500] loss: 2.0266882704.3183987140656[5, 10000] loss: 2.0328512727.011394739151[5, 10500] loss: 2.0463252749.596834421158[5, 11000] loss: 2.0199832772.1074562072754[5, 11500] loss: 2.0759192794.710522890091[5, 12000] loss: 2.0129792817.295552968979[5, 12500] loss: 2.0252222840.012440443039Epoch [5] loss: 6467400.973417[6,   500] loss: 2.0666322863.1449913978577[6,  1000] loss: 2.0790922886.068451166153[6,  1500] loss: 2.1024532908.609122276306[6,  2000] loss: 2.0354342931.698576450348[6,  2500] loss: 2.0556392954.2932512760162[6,  3000] loss: 2.0367972976.8302009105682[6,  3500] loss: 2.0683672999.3735704421997[6,  4000] loss: 2.0283503021.8221945762634[6,  4500] loss: 2.0559913044.3479907512665[6,  5000] loss: 2.0231593066.974673509598[6,  5500] loss: 2.0359863089.5682690143585[6,  6000] loss: 2.0360303112.5611913204193[6,  6500] loss: 2.0034193135.0309388637543[6,  7000] loss: 2.0448753157.614204645157[6,  7500] loss: 2.0537013180.349945783615[6,  8000] loss: 2.0119013203.10449886322[6,  8500] loss: 2.0239503225.818785905838[6,  9000] loss: 2.0209513248.3881986141205[6,  9500] loss: 1.9856753271.0667328834534[6, 10000] loss: 2.0256733294.1930060386658[6, 10500] loss: 2.0361913317.066517353058[6, 11000] loss: 1.9879123339.83900141716[6, 11500] loss: 2.0089303362.3695158958435[6, 12000] loss: 2.0565243385.0086381435394[6, 12500] loss: 2.0890173407.6749782562256Epoch [6] loss: 6359939.198245[7,   500] loss: 2.1422753430.6491425037384[7,  1000] loss: 2.0923963453.105288505554[7,  1500] loss: 2.0716353475.642054080963[7,  2000] loss: 2.0860703498.3590915203094[7,  2500] loss: 2.1019203521.0441155433655[7,  3000] loss: 2.1021263543.76371049881[7,  3500] loss: 2.1408253566.4220242500305[7,  4000] loss: 2.1176923589.1802258491516[7,  4500] loss: 2.1524613611.5383927822113[7,  5000] loss: 2.0870583633.9583897590637[7,  5500] loss: 2.0543433656.484963655472[7,  6000] loss: 2.0928363679.048088312149[7,  6500] loss: 2.1104683701.869065284729[7,  7000] loss: 2.0674533724.4767787456512[7,  7500] loss: 2.0557413747.294711828232[7,  8000] loss: 2.0644203769.90216088295[7,  8500] loss: 2.0420703792.3603308200836[7,  9000] loss: 2.0668743814.8991367816925[7,  9500] loss: 2.0058213837.444258451462[7, 10000] loss: 2.0306783860.0645282268524[7, 10500] loss: 2.0148103882.753199338913[7, 11000] loss: 1.9809183905.6004679203033[7, 11500] loss: 2.0626413928.3344447612762[7, 12000] loss: 2.1071153951.0990567207336[7, 12500] loss: 2.0194203973.5682661533356Epoch [7] loss: 6502828.958920[8,   500] loss: 2.0578723996.684868335724[8,  1000] loss: 2.0017414019.4145636558533[8,  1500] loss: 2.0225494042.4060456752777[8,  2000] loss: 2.0294424065.0613400936127[8,  2500] loss: 2.0551244087.834133863449[8,  3000] loss: 2.0973734110.925270795822[8,  3500] loss: 2.0808774133.428690433502[8,  4000] loss: 2.0665024156.034437179565[8,  4500] loss: 2.1180724178.647623062134[8,  5000] loss: 2.2638164201.36395406723[8,  5500] loss: 2.2532284224.222873926163[8,  6000] loss: 2.2436554246.831351518631[8,  6500] loss: 2.2091114269.410906553268[8,  7000] loss: 2.2179404292.01949429512[8,  7500] loss: 2.1862274314.828847646713[8,  8000] loss: 2.1828484337.472215175629[8,  8500] loss: 2.1644944360.219279527664[8,  9000] loss: 2.1866754382.803082227707[8,  9500] loss: 2.1833204405.42612361908[8, 10000] loss: 2.1847524428.0302493572235[8, 10500] loss: 2.1842684450.627673864365[8, 11000] loss: 2.1680924473.183564186096[8, 11500] loss: 2.1357654495.895450115204[8, 12000] loss: 2.1386664518.470850229263[8, 12500] loss: 2.1224974541.083957910538Epoch [8] loss: 6697573.810491[9,   500] loss: 2.1120564564.342080593109[9,  1000] loss: 2.2047774586.92552781105[9,  1500] loss: 2.2007564609.643644332886[9,  2000] loss: 2.2026744632.411087751389[9,  2500] loss: 2.1646454655.175965309143[9,  3000] loss: 2.1667864677.761009216309[9,  3500] loss: 2.1629094700.305628299713[9,  4000] loss: 2.1490024722.892550945282[9,  4500] loss: 2.1420944745.508034467697[9,  5000] loss: 2.1756084768.129538059235[9,  5500] loss: 2.1796974790.734162807465[9,  6000] loss: 2.1519694813.515047311783[9,  6500] loss: 2.1632104836.206914663315[9,  7000] loss: 2.1404994858.83820772171[9,  7500] loss: 2.1248574881.888867139816[9,  8000] loss: 2.1380084904.816889762878[9,  8500] loss: 2.1675334927.644575357437[9,  9000] loss: 2.1432544950.487177848816[9,  9500] loss: 2.1286934973.088050127029[9, 10000] loss: 2.1165014995.6585648059845[9, 10500] loss: 2.1766215018.533212184906[9, 11000] loss: 2.2123965041.070152282715[9, 11500] loss: 2.1871205063.68714594841[9, 12000] loss: 2.1822675086.394817829132[9, 12500] loss: 2.1953195109.004381656647Epoch [9] loss: 6771718.178041[10,   500] loss: 2.1811615131.648126840591[10,  1000] loss: 2.1372025154.480946063995[10,  1500] loss: 2.1253525177.502229213715[10,  2000] loss: 2.1349425200.053324222565[10,  2500] loss: 2.1192035222.889752864838[10,  3000] loss: 2.1188845245.356068134308[10,  3500] loss: 2.1435185267.924701929092[10,  4000] loss: 2.1440865290.647060394287[10,  4500] loss: 2.0967555313.183438062668[10,  5000] loss: 2.1189985335.732752084732[10,  5500] loss: 2.0845225358.221460103989[10,  6000] loss: 2.1100235381.257415533066[10,  6500] loss: 2.1284865404.095795631409[10,  7000] loss: 2.2368725426.78115105629[10,  7500] loss: 2.1982905449.360090970993[10,  8000] loss: 2.1498555472.030558109283[10,  8500] loss: 2.1363845494.520549058914[10,  9000] loss: 2.1435755517.136969089508[10,  9500] loss: 2.1423505539.670059680939[10, 10000] loss: 2.1522015562.726020812988[10, 10500] loss: 2.1313095585.3478083610535[10, 11000] loss: 2.1603695607.980488061905[10, 11500] loss: 2.1292075630.704790115356[10, 12000] loss: 2.1197555653.281673192978[10, 12500] loss: 2.1157755676.575202226639Epoch [10] loss: 6690745.243379[11,   500] loss: 2.1237115699.2853190898895[11,  1000] loss: 2.1727695721.859359741211[11,  1500] loss: 2.2193015744.4289882183075[11,  2000] loss: 2.1968355766.879630565643[11,  2500] loss: 2.1706155789.381403684616[11,  3000] loss: 2.1897365812.6475529670715[11,  3500] loss: 2.1329355835.923951625824[11,  4000] loss: 2.1318305858.389040708542[11,  4500] loss: 2.1170415881.751179933548[11,  5000] loss: 2.1469815904.536164283752[11,  5500] loss: 2.0990785927.442582368851[11,  6000] loss: 2.1082395950.272321939468[11,  6500] loss: 2.1219695972.951697349548[11,  7000] loss: 2.0945555996.055996417999[11,  7500] loss: 2.0824666018.6398940086365[11,  8000] loss: 2.1035146041.324875593185[11,  8500] loss: 2.0801126064.118999958038[11,  9000] loss: 2.0880576086.692965269089[11,  9500] loss: 2.0874516109.261739015579[11, 10000] loss: 2.0763666131.920359134674[11, 10500] loss: 2.1043416154.62030506134[11, 11000] loss: 2.1101186177.2375955581665[11, 11500] loss: 2.1041456199.890399932861[11, 12000] loss: 2.0848706222.612815141678[11, 12500] loss: 2.0895646245.52522444725Epoch [11] loss: 6645230.754588[12,   500] loss: 2.1043606268.63757276535[12,  1000] loss: 2.0595616291.4444234371185[12,  1500] loss: 2.0656716314.132905483246[12,  2000] loss: 2.0673096336.910328865051[12,  2500] loss: 2.0770196359.519941329956[12,  3000] loss: 2.0677046382.4350526332855[12,  3500] loss: 2.0613466405.04089474678[12,  4000] loss: 2.0404056427.919983625412[12,  4500] loss: 2.0239616450.909569740295[12,  5000] loss: 2.0428876473.877658605576[12,  5500] loss: 1.9913576496.540492773056[12,  6000] loss: 2.0265996519.265284538269[12,  6500] loss: 2.0630686542.0676391124725[12,  7000] loss: 2.0292246564.554436683655[12,  7500] loss: 2.0126426587.04047870636[12,  8000] loss: 2.0059396609.647593975067[12,  8500] loss: 2.0490496632.186534166336[12,  9000] loss: 2.0323096654.899647951126[12,  9500] loss: 2.0404356677.415514945984[12, 10000] loss: 2.0174776700.0483729839325[12, 10500] loss: 2.0105016722.712144136429[12, 11000] loss: 2.0023126745.427824258804[12, 11500] loss: 1.9769426768.158665895462[12, 12000] loss: 1.9740576791.218218564987[12, 12500] loss: 1.9946786814.11034488678Epoch [12] loss: 6362919.464282[13,   500] loss: 2.0003086836.8489112854[13,  1000] loss: 1.9914976859.649877309799[13,  1500] loss: 1.9861456882.485924243927[13,  2000] loss: 1.9896406905.391445636749[13,  2500] loss: 1.9881226928.485458135605[13,  3000] loss: 1.9891776951.384012460709[13,  3500] loss: 1.9711066974.342923879623[13,  4000] loss: 1.9767966997.1278166770935[13,  4500] loss: 1.9906007020.066690206528[13,  5000] loss: 1.9703597042.927864551544[13,  5500] loss: 1.9561287065.609358072281[13,  6000] loss: 2.0122167088.48744225502[13,  6500] loss: 1.9560507111.494697570801[13,  7000] loss: 2.0513327134.403424978256[13,  7500] loss: 2.0546957157.225070238113[13,  8000] loss: 2.0491947180.174999475479[13,  8500] loss: 2.0389807202.754199743271[13,  9000] loss: 2.0455287225.459443569183[13,  9500] loss: 2.0618017248.18865776062[13, 10000] loss: 2.0426467270.9387402534485[13, 10500] loss: 2.0306607293.668567657471[13, 11000] loss: 2.0213607316.477406024933[13, 11500] loss: 1.9924917339.017791986465[13, 12000] loss: 2.0140097361.596732616425[13, 12500] loss: 1.9873547384.643758773804Epoch [13] loss: 6275247.771942[14,   500] loss: 1.9977527407.297623872757[14,  1000] loss: 2.0113857429.9306836128235[14,  1500] loss: 2.0110047452.777026891708[14,  2000] loss: 1.9898277475.520576238632[14,  2500] loss: 1.9891587498.1626489162445[14,  3000] loss: 1.9929957520.864273786545[14,  3500] loss: 1.9834717543.654047012329[14,  4000] loss: 2.0027477566.157978773117[14,  4500] loss: 1.9791417589.2541036605835[14,  5000] loss: 1.9958467611.95360660553[14,  5500] loss: 1.9573007634.702481985092[14,  6000] loss: 1.9948507657.601510763168[14,  6500] loss: 1.9405187680.264955043793[14,  7000] loss: 1.9486007702.854200601578[14,  7500] loss: 1.9438007725.584273576736[14,  8000] loss: 1.9309587748.715531110764[14,  8500] loss: 1.9068307771.644280910492[14,  9000] loss: 1.9733007794.796292066574[14,  9500] loss: 1.9868297817.419952869415[14, 10000] loss: 1.9340307840.190404891968[14, 10500] loss: 1.9509277863.164112091064[14, 11000] loss: 1.9707957888.346033334732[14, 11500] loss: 1.9121107911.316055059433[14, 12000] loss: 1.9327487934.452785015106[14, 12500] loss: 1.9405377957.162336349487Epoch [14] loss: 6160013.427822[15,   500] loss: 1.9336977984.188962697983[15,  1000] loss: 1.9473898006.930123090744[15,  1500] loss: 1.9242458029.688338518143[15,  2000] loss: 1.9184848052.382691860199[15,  2500] loss: 1.8810958075.391649961472[15,  3000] loss: 1.9109948098.341364860535[15,  3500] loss: 1.9231578120.999233484268[15,  4000] loss: 1.9114958143.710253715515[15,  4500] loss: 1.8745648166.553369283676[15,  5000] loss: 1.9297688189.3779237270355[15,  5500] loss: 1.9124998212.299587965012[15,  6000] loss: 1.8983158234.890445947647[15,  6500] loss: 1.9223888257.57068324089[15,  7000] loss: 1.9090568280.41580581665[15,  7500] loss: 1.8925298303.934632778168[15,  8000] loss: 1.8632488327.0914645195[15,  8500] loss: 1.8958998349.751410961151[15,  9000] loss: 1.8505918372.905273675919[15,  9500] loss: 1.9020518395.47060751915[15, 10000] loss: 1.8782168418.186641216278[15, 10500] loss: 1.8742918441.380583524704[15, 11000] loss: 1.9037058465.151251792908[15, 11500] loss: 1.9311458488.24751162529[15, 12000] loss: 1.8862078511.645946264267[15, 12500] loss: 1.9030628534.336044311523Epoch [15] loss: 5973053.366757[16,   500] loss: 1.8651718557.617929697037[16,  1000] loss: 1.8815298580.524780035019[16,  1500] loss: 1.8648468603.343128442764[16,  2000] loss: 1.8699358626.233912944794[16,  2500] loss: 1.8772068649.44772529602[16,  3000] loss: 1.8822198672.153370141983[16,  3500] loss: 1.8571408695.240242242813[16,  4000] loss: 1.8719148718.163936853409[16,  4500] loss: 1.8673138741.133415699005[16,  5000] loss: 1.8636538764.273627519608[16,  5500] loss: 1.8705648786.909125089645[16,  6000] loss: 1.8631978809.48515343666[16,  6500] loss: 1.9113608832.322682857513[16,  7000] loss: 1.8693438855.217905759811[16,  7500] loss: 1.8611048877.84301519394[16,  8000] loss: 1.8771598901.025487184525[16,  8500] loss: 1.8621148923.796998262405[16,  9000] loss: 1.8526938946.806401252747[16,  9500] loss: 1.8521758969.4466714859[16, 10000] loss: 1.8738828992.098433971405[16, 10500] loss: 1.8676659014.90926361084[16, 11000] loss: 1.8459359037.573462963104[16, 11500] loss: 1.8665919060.31594467163[16, 12000] loss: 1.8243149083.055387973785[16, 12500] loss: 1.8739479105.855236768723Epoch [16] loss: 5842659.242802[17,   500] loss: 1.8726819129.045575141907[17,  1000] loss: 1.8817489151.843878030777[17,  1500] loss: 1.8607709174.7669672966[17,  2000] loss: 1.8510119197.660264968872[17,  2500] loss: 1.8777179220.581681251526[17,  3000] loss: 1.8299949243.329067468643[17,  3500] loss: 1.8077339266.210684299469[17,  4000] loss: 1.8464879289.044819831848[17,  4500] loss: 1.8145999311.865683555603[17,  5000] loss: 1.8021349334.621535778046[17,  5500] loss: 1.8485929357.38280081749[17,  6000] loss: 1.8380749380.351134300232[17,  6500] loss: 1.8542519403.196688890457[17,  7000] loss: 1.8097319426.006103038788[17,  7500] loss: 1.8104629448.8303399086[17,  8000] loss: 1.8312799471.385844707489[17,  8500] loss: 1.7948299494.311012268066[17,  9000] loss: 1.8162779517.43502998352[17,  9500] loss: 1.8394849540.724895954132[17, 10000] loss: 1.8012379563.536741733551[17, 10500] loss: 1.8007959586.220160722733[17, 11000] loss: 1.8242859608.964532136917[17, 11500] loss: 1.8606879631.709643602371[17, 12000] loss: 1.8054519654.496764659882[17, 12500] loss: 1.8360999677.226867198944Epoch [17] loss: 5741212.144589[18,   500] loss: 1.7780589700.261835813522[18,  1000] loss: 1.8018489723.030370950699[18,  1500] loss: 1.7917489745.81370139122[18,  2000] loss: 1.7985309768.481998443604[18,  2500] loss: 1.8148159791.29906988144[18,  3000] loss: 1.8008369814.411271333694[18,  3500] loss: 1.8316309837.009349822998[18,  4000] loss: 1.8110249859.488085508347[18,  4500] loss: 1.7783299882.077591896057[18,  5000] loss: 1.8161569904.937225103378[18,  5500] loss: 1.8016209927.612990617752[18,  6000] loss: 1.7700359950.378140449524[18,  6500] loss: 1.8113959973.26342535019[18,  7000] loss: 1.7776379996.164707422256[18,  7500] loss: 1.77166110018.93499827385[18,  8000] loss: 1.77453810041.838294267654[18,  8500] loss: 1.74254210064.720771074295[18,  9000] loss: 1.73270210087.750428438187[18,  9500] loss: 1.76958210110.928386926651[18, 10000] loss: 1.78291410133.46657037735[18, 10500] loss: 1.78867010156.001238107681[18, 11000] loss: 1.75481110178.611052751541[18, 11500] loss: 1.79047710201.281552314758[18, 12000] loss: 1.78631210224.023465871811[18, 12500] loss: 1.74337010246.635311126709Epoch [18] loss: 5581684.140092[19,   500] loss: 1.77083110269.521662950516[19,  1000] loss: 1.77015510292.380264997482[19,  1500] loss: 1.77343610315.143051624298[19,  2000] loss: 1.74706510337.861729621887[19,  2500] loss: 1.74945410360.428888320923[19,  3000] loss: 1.77014510383.055749177933[19,  3500] loss: 1.80702610405.560710668564[19,  4000] loss: 1.73786810428.522998809814[19,  4500] loss: 1.74658110451.55839252472[19,  5000] loss: 1.69367810474.625269651413[19,  5500] loss: 1.77225110497.166759252548[19,  6000] loss: 1.77466210519.989121198654[19,  6500] loss: 1.71786210542.60750246048[19,  7000] loss: 1.74401510565.011420488358[19,  7500] loss: 1.81648510587.610797643661[19,  8000] loss: 1.74044910610.804809570312[19,  8500] loss: 1.78051510633.181817293167[19,  9000] loss: 1.72936710655.598370313644[19,  9500] loss: 1.75642810678.096650362015[19, 10000] loss: 1.76851710700.77692580223[19, 10500] loss: 1.73144510723.332902669907[19, 11000] loss: 1.76543210745.979274988174[19, 11500] loss: 1.79736110768.517337560654[19, 12000] loss: 1.77823810791.132400274277[19, 12500] loss: 1.74445410813.600977182388Epoch [19] loss: 5528232.279303[20,   500] loss: 1.73652210836.239129543304[20,  1000] loss: 1.71109810859.027343511581[20,  1500] loss: 1.75926710881.57473039627[20,  2000] loss: 1.77151810904.221339941025[20,  2500] loss: 1.69914410926.757330179214[20,  3000] loss: 1.78396310949.21358203888[20,  3500] loss: 1.76995210971.698498010635[20,  4000] loss: 1.75795810994.130110025406[20,  4500] loss: 1.77333911016.647144317627[20,  5000] loss: 1.68618511039.544922351837[20,  5500] loss: 1.74999711062.042895078659[20,  6000] loss: 1.73679111084.898882389069[20,  6500] loss: 1.71645211107.315937519073[20,  7000] loss: 1.76504011129.794045209885[20,  7500] loss: 1.69775011152.252049922943[20,  8000] loss: 1.76106211174.996840000153[20,  8500] loss: 1.69088811197.532766342163[20,  9000] loss: 1.71694411220.007195711136[20,  9500] loss: 1.71262511242.324493408203[20, 10000] loss: 1.72830511265.192030668259[20, 10500] loss: 1.70377711287.697889089584[20, 11000] loss: 1.71569411310.381761789322[20, 11500] loss: 1.74301811333.273948192596[20, 12000] loss: 1.74557511356.415370225906[20, 12500] loss: 1.69549011379.624421834946Epoch [20] loss: 5419526.798743Finished TrainingSaving model to /data/s4091221/trained-models/wide_resnet101_22020-02-25 00:11:37.242498GroundTruth:    cat  ship  ship planeSending data to GPUSending model to GPUtensor([[-1.4983e+00, -3.6263e-02, -6.5186e-01,  7.2864e-01,  1.1494e+00,          8.9544e-01,  1.7119e+00,  2.4686e-01, -1.6505e+00, -1.6446e+00],        [ 2.1172e+00,  2.3764e+00, -2.1182e+00, -2.3144e+00, -7.4983e-01,         -2.6090e+00, -1.6164e+00, -1.1563e+00,  2.2591e+00,  2.0667e+00],        [ 1.2323e+00,  1.8630e+00, -1.5927e+00, -1.9715e+00, -4.3390e-01,         -2.0371e+00, -9.6588e-01, -8.6580e-01,  1.3930e+00,  1.1941e+00],        [ 1.8100e+00,  1.9380e+00, -1.7396e+00, -2.0832e+00, -3.0318e-04,         -2.2440e+00, -9.9068e-01, -9.0008e-01,  1.5581e+00,  9.5424e-01]],       device='cuda:0', grad_fn=<AddmmBackward>)Predicted:   frog   car   car   carAccuracy of the network on the 4000.0 test images: 39 %###########################################################################################################{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': True, 'batch_size': 4, 'workers': 2, 'model_archi': 33, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=33, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=True, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)Files already downloaded and verifiedFiles already downloaded and verifiedcuda:0Downloading: "https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth" to /home/s4091221/.cache/torch/checkpoints/wide_resnet101_2-32ee1156.pth12500  dog  bird  ship  bird['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']Model wide_resnet101_2 LoadedResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): Bottleneck(      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer2): Sequential(    (0): Bottleneck(      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer3): Sequential(    (0): Bottleneck(      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (4): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (5): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (6): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (7): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (8): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (9): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (10): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (11): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (12): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (13): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (14): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (15): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (16): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (17): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (18): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (19): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (20): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (21): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (22): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer4): Sequential(    (0): Bottleneck(      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=2048, out_features=10, bias=True))Model wide_resnet101_2 ReshapedSending model to GPULearning Rate: 0.001, Weight Decay: 0, Momentum: 0Defined <class 'torch.optim.sgd.SGD'> OptimizerStarting Training at 1582586014.3742623[1,   500] loss: 2.20810525.27526831626892[1,  1000] loss: 2.14946050.07013130187988[1,  1500] loss: 2.13103374.22423434257507[1,  2000] loss: 2.07742896.83355760574341[1,  2500] loss: 2.045995119.40871000289917[1,  3000] loss: 2.030965141.8870120048523[1,  3500] loss: 2.019331164.4351086616516[1,  4000] loss: 1.968926186.75483870506287[1,  4500] loss: 1.930804209.23921537399292[1,  5000] loss: 1.868732231.6622838973999[1,  5500] loss: 1.888012254.04066801071167[1,  6000] loss: 1.891968276.8219096660614[1,  6500] loss: 1.883617299.2192232608795[1,  7000] loss: 1.824988321.8175048828125[1,  7500] loss: 1.777298344.3182408809662[1,  8000] loss: 1.780803366.8574273586273[1,  8500] loss: 1.757423389.3368079662323[1,  9000] loss: 1.688265411.7682557106018[1,  9500] loss: 1.772312434.2170600891113[1, 10000] loss: 1.724684456.8359248638153[1, 10500] loss: 1.716028479.29416060447693[1, 11000] loss: 1.739176501.87616205215454[1, 11500] loss: 1.737095524.3173182010651[1, 12000] loss: 1.621940546.7443826198578[1, 12500] loss: 1.605673569.0740828514099Epoch [1] loss: 5882189.131768[2,   500] loss: 1.673046591.6824004650116[2,  1000] loss: 1.625209614.1630749702454[2,  1500] loss: 1.607644636.8256375789642[2,  2000] loss: 1.566118659.3698093891144[2,  2500] loss: 1.556580681.7512352466583[2,  3000] loss: 1.575694704.3747718334198[2,  3500] loss: 1.536936726.8357322216034[2,  4000] loss: 1.508872749.2224972248077[2,  4500] loss: 1.513469771.6043765544891[2,  5000] loss: 1.494768794.0633718967438[2,  5500] loss: 1.557239816.5709245204926[2,  6000] loss: 1.515777839.9369575977325[2,  6500] loss: 1.503028862.5391061306[2,  7000] loss: 1.512214884.9306635856628[2,  7500] loss: 1.460172907.394588470459[2,  8000] loss: 1.465299930.1884663105011[2,  8500] loss: 1.446572953.0071411132812[2,  9000] loss: 1.461871975.4612948894501[2,  9500] loss: 1.442967997.9045658111572[2, 10000] loss: 1.4095891020.3452219963074[2, 10500] loss: 1.4339011042.804480791092[2, 11000] loss: 1.3984481065.151333808899[2, 11500] loss: 1.3769251087.7313401699066[2, 12000] loss: 1.3912931110.2143568992615[2, 12500] loss: 1.3714951132.7702865600586Epoch [2] loss: 4696082.508647[3,   500] loss: 1.3388071155.5137798786163[3,  1000] loss: 1.3471671178.070532798767[3,  1500] loss: 1.2956151200.5926563739777[3,  2000] loss: 1.3073191223.0960717201233[3,  2500] loss: 1.2933591245.4197237491608[3,  3000] loss: 1.3102841267.8946568965912[3,  3500] loss: 1.2530681290.595697402954[3,  4000] loss: 1.2977541313.483826160431[3,  4500] loss: 1.2568941336.0803084373474[3,  5000] loss: 1.2783351358.908923149109[3,  5500] loss: 1.2554781381.4259510040283[3,  6000] loss: 1.2494501404.5861666202545[3,  6500] loss: 1.2550861427.2235012054443[3,  7000] loss: 1.1986441449.9283781051636[3,  7500] loss: 1.2617681472.4143114089966[3,  8000] loss: 1.2493791495.1074669361115[3,  8500] loss: 1.2501911517.5236296653748[3,  9000] loss: 1.2121111539.9973349571228[3,  9500] loss: 1.1714931563.0413303375244[3, 10000] loss: 1.1701481585.5635051727295[3, 10500] loss: 1.1314911608.4232261180878[3, 11000] loss: 1.1760321631.4161477088928[3, 11500] loss: 1.1935461653.793857574463[3, 12000] loss: 1.1472931676.130968093872[3, 12500] loss: 1.1812981698.620279788971Epoch [3] loss: 3891262.723340[4,   500] loss: 1.1130531721.4103698730469[4,  1000] loss: 1.1398461744.1672613620758[4,  1500] loss: 1.1052691766.9597029685974[4,  2000] loss: 1.1537441789.5297453403473[4,  2500] loss: 1.0623551812.0189852714539[4,  3000] loss: 1.1230861834.544938325882[4,  3500] loss: 1.0918161857.0549249649048[4,  4000] loss: 1.1429721879.486472606659[4,  4500] loss: 1.0289511902.0810313224792[4,  5000] loss: 1.0527071924.7095038890839[4,  5500] loss: 1.0742261947.2036774158478[4,  6000] loss: 1.0364581969.4837131500244[4,  6500] loss: 1.1073601992.0172045230865[4,  7000] loss: 1.0518582014.9170625209808[4,  7500] loss: 1.1041962037.2268431186676[4,  8000] loss: 1.0621622059.658394098282[4,  8500] loss: 1.0568232082.0866611003876[4,  9000] loss: 1.0042372104.6318271160126[4,  9500] loss: 1.0190442127.084493160248[4, 10000] loss: 1.0728332149.884438276291[4, 10500] loss: 1.0577332172.71524810791[4, 11000] loss: 1.0638252195.2758996486664[4, 11500] loss: 1.0268362217.8366367816925[4, 12000] loss: 1.0194882241.0551784038544[4, 12500] loss: 1.0564482263.6050639152527Epoch [4] loss: 3372801.495300[5,   500] loss: 0.9334112286.1807696819305[5,  1000] loss: 0.9826622308.6476883888245[5,  1500] loss: 0.9756392331.275577545166[5,  2000] loss: 0.9981052353.657960653305[5,  2500] loss: 0.9863312376.3284866809845[5,  3000] loss: 0.9955022398.7509021759033[5,  3500] loss: 0.9671282421.2640330791473[5,  4000] loss: 0.9604742443.554418325424[5,  4500] loss: 0.9883542465.9208760261536[5,  5000] loss: 0.9202612488.2734529972076[5,  5500] loss: 1.0033592510.7494111061096[5,  6000] loss: 0.9617882533.3992142677307[5,  6500] loss: 0.9143502557.5036976337433[5,  7000] loss: 0.9392392580.342953443527[5,  7500] loss: 0.9642622602.8669872283936[5,  8000] loss: 0.9017242625.445719718933[5,  8500] loss: 0.9309712648.006695985794[5,  9000] loss: 0.9114872670.783987045288[5,  9500] loss: 0.9037432693.377788543701[5, 10000] loss: 0.9024152716.095944404602[5, 10500] loss: 0.9564542738.8970556259155[5, 11000] loss: 0.9351172761.329250574112[5, 11500] loss: 0.8944612783.8758788108826[5, 12000] loss: 0.9160842806.3984775543213[5, 12500] loss: 0.9266312828.928631067276Epoch [5] loss: 2960301.265605[6,   500] loss: 0.8521242851.631475687027[6,  1000] loss: 0.8963552874.124626636505[6,  1500] loss: 0.8479912896.7136664390564[6,  2000] loss: 0.8595982919.063009738922[6,  2500] loss: 0.8223552941.6127910614014[6,  3000] loss: 0.8709142964.0112166404724[6,  3500] loss: 0.9072222986.4478175640106[6,  4000] loss: 0.8926563008.8336226940155[6,  4500] loss: 0.8284303031.2312145233154[6,  5000] loss: 0.8563193053.741469144821[6,  5500] loss: 0.8347683076.492948770523[6,  6000] loss: 0.8595283099.1820895671844[6,  6500] loss: 0.8415823121.7362785339355[6,  7000] loss: 0.8761833144.317884683609[6,  7500] loss: 0.8283053167.1580622196198[6,  8000] loss: 0.8321743190.385879278183[6,  8500] loss: 0.8294773212.9499955177307[6,  9000] loss: 0.8068713235.471894264221[6,  9500] loss: 0.8063783257.9349598884583[6, 10000] loss: 0.8409793280.4825990200043[6, 10500] loss: 0.8619003302.9816558361053[6, 11000] loss: 0.8393843326.1002762317657[6, 11500] loss: 0.8026553348.5941441059113[6, 12000] loss: 0.8356083371.100394964218[6, 12500] loss: 0.8501023393.656749486923Epoch [6] loss: 2669366.643442[7,   500] loss: 0.7709853416.6627366542816[7,  1000] loss: 0.8390123439.1054866313934[7,  1500] loss: 0.7305123462.5603351593018[7,  2000] loss: 0.7091713485.853304862976[7,  2500] loss: 0.7374103508.2133634090424[7,  3000] loss: 0.7667003530.676394224167[7,  3500] loss: 0.7611923553.1169307231903[7,  4000] loss: 0.7462293575.4232127666473[7,  4500] loss: 0.8009363598.4912552833557[7,  5000] loss: 0.7126273621.2973206043243[7,  5500] loss: 0.7787133643.92751455307[7,  6000] loss: 0.7218013666.733407020569[7,  6500] loss: 0.7865283689.3240127563477[7,  7000] loss: 0.7313103711.9123520851135[7,  7500] loss: 0.7947783734.9682111740112[7,  8000] loss: 0.7413603757.345114707947[7,  8500] loss: 0.7589163779.6573598384857[7,  9000] loss: 0.7805093802.3256702423096[7,  9500] loss: 0.7898453824.726423740387[7, 10000] loss: 0.7794293847.2128970623016[7, 10500] loss: 0.7399343876.143945455551[7, 11000] loss: 0.7951643899.9145209789276[7, 11500] loss: 0.7816093922.6704988479614[7, 12000] loss: 0.7247823945.1184203624725[7, 12500] loss: 0.7223753967.770349740982Epoch [7] loss: 2398265.739322[8,   500] loss: 0.6423193991.4765813350677[8,  1000] loss: 0.6968294014.048700094223[8,  1500] loss: 0.6997864037.031082868576[8,  2000] loss: 0.6948364059.5615894794464[8,  2500] loss: 0.6973834082.1303312778473[8,  3000] loss: 0.6812494104.6335418224335[8,  3500] loss: 0.6529194127.17205119133[8,  4000] loss: 0.7062364149.880685567856[8,  4500] loss: 0.6919894172.262663125992[8,  5000] loss: 0.6274514195.216672420502[8,  5500] loss: 0.6538794217.9492955207825[8,  6000] loss: 0.6981514240.459534883499[8,  6500] loss: 0.6536674262.7949986457825[8,  7000] loss: 0.6819534285.600123167038[8,  7500] loss: 0.6374634308.141680240631[8,  8000] loss: 0.6986914330.723928928375[8,  8500] loss: 0.7392034353.2798454761505[8,  9000] loss: 0.7255484375.747059106827[8,  9500] loss: 0.6876284398.301930904388[8, 10000] loss: 0.7545644420.730150938034[8, 10500] loss: 0.7032564443.299919605255[8, 11000] loss: 0.7362654465.677495479584[8, 11500] loss: 0.7005534488.19127368927[8, 12000] loss: 0.7222714510.759857416153[8, 12500] loss: 0.6690794533.541102647781Epoch [8] loss: 2164144.454015[9,   500] loss: 0.6192304556.535041570663[9,  1000] loss: 0.6285034579.285640478134[9,  1500] loss: 0.5797364602.44375371933[9,  2000] loss: 0.6381154624.950041770935[9,  2500] loss: 0.6210374647.616966247559[9,  3000] loss: 0.6062934670.158907413483[9,  3500] loss: 0.6682994692.871803760529[9,  4000] loss: 0.6537554715.387400388718[9,  4500] loss: 0.6443654737.962376832962[9,  5000] loss: 0.6057394760.503986120224[9,  5500] loss: 0.6482654782.922273397446[9,  6000] loss: 0.6304064805.417695760727[9,  6500] loss: 0.6072804827.92681312561[9,  7000] loss: 0.6769044850.6008677482605[9,  7500] loss: 0.6040894873.126281261444[9,  8000] loss: 0.5979944895.805747747421[9,  8500] loss: 0.6403944918.274619579315[9,  9000] loss: 0.6050054940.815446615219[9,  9500] loss: 0.6407084963.621780872345[9, 10000] loss: 0.6675704986.118525028229[9, 10500] loss: 0.6403585008.589015007019[9, 11000] loss: 0.6328275031.211826324463[9, 11500] loss: 0.6376815053.786008358002[9, 12000] loss: 0.6633565076.178730726242[9, 12500] loss: 0.6490065098.786623954773Epoch [9] loss: 1975218.770274[10,   500] loss: 0.5497305121.357950925827[10,  1000] loss: 0.5637985143.89755821228[10,  1500] loss: 0.5984745166.301438093185[10,  2000] loss: 0.5965045188.839207172394[10,  2500] loss: 0.5721665212.255002260208[10,  3000] loss: 0.5555945234.702355623245[10,  3500] loss: 0.5284875257.151442050934[10,  4000] loss: 0.5455835279.4599578380585[10,  4500] loss: 0.5527485302.658087730408[10,  5000] loss: 0.6230395325.332136631012[10,  5500] loss: 0.5652325347.79178237915[10,  6000] loss: 0.5610635370.4301562309265[10,  6500] loss: 0.5641655392.8957550525665[10,  7000] loss: 0.5580925415.503129005432[10,  7500] loss: 0.6138375438.429772853851[10,  8000] loss: 0.5477545461.576586961746[10,  8500] loss: 0.5431475483.995344877243[10,  9000] loss: 0.5336355506.661576032639[10,  9500] loss: 0.6258715529.234962940216[10, 10000] loss: 0.6034805551.686762332916[10, 10500] loss: 0.6048645574.0868237018585[10, 11000] loss: 0.6053205596.3982491493225[10, 11500] loss: 0.5869935618.9660856723785[10, 12000] loss: 0.5385795641.496650218964[10, 12500] loss: 0.5961675664.01713013649Epoch [10] loss: 1793883.442494[11,   500] loss: 0.4818105686.665861129761[11,  1000] loss: 0.4978805709.0229477882385[11,  1500] loss: 0.4832505731.815818786621[11,  2000] loss: 0.5012135754.457442998886[11,  2500] loss: 0.5061875777.029125452042[11,  3000] loss: 0.4940555799.620803356171[11,  3500] loss: 0.5289715822.365441560745[11,  4000] loss: 0.4877435845.006238698959[11,  4500] loss: 0.5135025867.635551691055[11,  5000] loss: 0.5075405890.432391166687[11,  5500] loss: 0.5568325913.383167982101[11,  6000] loss: 0.5142125936.5710508823395[11,  6500] loss: 0.5321965959.213143348694[11,  7000] loss: 0.4833035982.023108720779[11,  7500] loss: 0.5347186004.866983413696[11,  8000] loss: 0.5354266027.459730863571[11,  8500] loss: 0.5177106049.891722917557[11,  9000] loss: 0.5731086072.272488832474[11,  9500] loss: 0.5317566094.986327886581[11, 10000] loss: 0.5297916117.843487739563[11, 10500] loss: 0.5133746140.569660186768[11, 11000] loss: 0.5746046162.960278749466[11, 11500] loss: 0.5353156186.161428689957[11, 12000] loss: 0.5328716208.935471057892[11, 12500] loss: 0.5133426231.7996191978455Epoch [11] loss: 1620545.670054[12,   500] loss: 0.4614226254.3227434158325[12,  1000] loss: 0.4463806277.3849840164185[12,  1500] loss: 0.4484286300.121030330658[12,  2000] loss: 0.4735996323.101022481918[12,  2500] loss: 0.4422946346.14115691185[12,  3000] loss: 0.4351066369.426369428635[12,  3500] loss: 0.4713056392.447482824326[12,  4000] loss: 0.4413446415.459681272507[12,  4500] loss: 0.4561116438.421581745148[12,  5000] loss: 0.4793756461.214942455292[12,  5500] loss: 0.4948276483.80439043045[12,  6000] loss: 0.4568356506.408265352249[12,  6500] loss: 0.4783226529.092817544937[12,  7000] loss: 0.4647306551.556047439575[12,  7500] loss: 0.4621766574.321924686432[12,  8000] loss: 0.4549676596.910684585571[12,  8500] loss: 0.4766066619.3060212135315[12,  9000] loss: 0.5023866642.026716470718[12,  9500] loss: 0.4914176664.445407629013[12, 10000] loss: 0.4969736687.17101931572[12, 10500] loss: 0.4797796710.053373336792[12, 11000] loss: 0.4625376732.530575990677[12, 11500] loss: 0.5118346755.0227591991425[12, 12000] loss: 0.4953706777.410884857178[12, 12500] loss: 0.4836366799.809572458267Epoch [12] loss: 1472741.029822[13,   500] loss: 0.3812246822.643435716629[13,  1000] loss: 0.4107046845.180909156799[13,  1500] loss: 0.3850016867.9666538238525[13,  2000] loss: 0.3970936890.702087163925[13,  2500] loss: 0.4208696913.363924980164[13,  3000] loss: 0.4276276936.367679357529[13,  3500] loss: 0.4305716958.772254228592[13,  4000] loss: 0.4169486981.323842525482[13,  4500] loss: 0.4370307003.900316953659[13,  5000] loss: 0.4669647026.265905618668[13,  5500] loss: 0.4494237048.758453845978[13,  6000] loss: 0.4424177071.2584393024445[13,  6500] loss: 0.4178907093.820419311523[13,  7000] loss: 0.4158937116.693213939667[13,  7500] loss: 0.4810067139.788146972656[13,  8000] loss: 0.4049947162.26198720932[13,  8500] loss: 0.4039117184.781514167786[13,  9000] loss: 0.4468117207.391650676727[13,  9500] loss: 0.4352347230.056659698486[13, 10000] loss: 0.4614337252.8616943359375[13, 10500] loss: 0.4670507275.2915987968445[13, 11000] loss: 0.4183177297.626147985458[13, 11500] loss: 0.4364887320.0846066474915[13, 12000] loss: 0.4340607342.606057167053[13, 12500] loss: 0.4106127365.113842487335Epoch [13] loss: 1345130.249730[14,   500] loss: 0.3657587387.678797483444[14,  1000] loss: 0.3734907410.068875312805[14,  1500] loss: 0.3813767432.671386003494[14,  2000] loss: 0.3714697455.061027288437[14,  2500] loss: 0.3753847477.457813978195[14,  3000] loss: 0.3886237499.932646036148[14,  3500] loss: 0.3657377522.441651344299[14,  4000] loss: 0.3792917545.2764275074005[14,  4500] loss: 0.4243697567.8608412742615[14,  5000] loss: 0.3956397590.762842416763[14,  5500] loss: 0.4213217613.134695529938[14,  6000] loss: 0.3789257635.71068072319[14,  6500] loss: 0.4100387658.100260019302[14,  7000] loss: 0.3927857680.680979251862[14,  7500] loss: 0.3788537703.085259437561[14,  8000] loss: 0.3806917725.4604778289795[14,  8500] loss: 0.4233727747.998415470123[14,  9000] loss: 0.3711047771.429117679596[14,  9500] loss: 0.3945767794.058495521545[14, 10000] loss: 0.3557447816.469344615936[14, 10500] loss: 0.3805947839.065411090851[14, 11000] loss: 0.3959867861.547365903854[14, 11500] loss: 0.3824267890.58776140213[14, 12000] loss: 0.4178607914.888054609299[14, 12500] loss: 0.3786467937.339538574219Epoch [14] loss: 1190771.022597[15,   500] loss: 0.3371927961.434085607529[15,  1000] loss: 0.3217667984.046774148941[15,  1500] loss: 0.3590998006.955791950226[15,  2000] loss: 0.3297268029.472573041916[15,  2500] loss: 0.3129158052.22904753685[15,  3000] loss: 0.3335048074.750741958618[15,  3500] loss: 0.3109048097.178176164627[15,  4000] loss: 0.3407988120.2260892391205[15,  4500] loss: 0.3219868142.68699836731[15,  5000] loss: 0.3605008165.300713062286[15,  5500] loss: 0.3631518187.871461868286[15,  6000] loss: 0.3545048210.767501592636[15,  6500] loss: 0.3595778233.718108654022[15,  7000] loss: 0.3451238256.12932920456[15,  7500] loss: 0.3494138278.743952512741[15,  8000] loss: 0.3871288301.293934822083[15,  8500] loss: 0.3750688323.832354545593[15,  9000] loss: 0.3698848346.591557741165[15,  9500] loss: 0.3707998369.440899848938[15, 10000] loss: 0.3841338392.062081813812[15, 10500] loss: 0.3630778415.019049406052[15, 11000] loss: 0.3482258437.737788200378[15, 11500] loss: 0.4241138460.281330108643[15, 12000] loss: 0.3762538482.588643789291[15, 12500] loss: 0.3463398505.267046928406Epoch [15] loss: 1096952.570367[16,   500] loss: 0.3007938527.963852405548[16,  1000] loss: 0.3200098550.799497842789[16,  1500] loss: 0.3337968573.381956338882[16,  2000] loss: 0.3221268596.098474025726[16,  2500] loss: 0.3153578618.508007526398[16,  3000] loss: 0.3229528640.939566373825[16,  3500] loss: 0.3159738663.296999692917[16,  4000] loss: 0.2970748685.796490907669[16,  4500] loss: 0.3052248708.418298006058[16,  5000] loss: 0.2723928730.889622211456[16,  5500] loss: 0.3302278753.257678031921[16,  6000] loss: 0.3340338775.716793298721[16,  6500] loss: 0.2963078798.141518831253[16,  7000] loss: 0.3127908820.750968694687[16,  7500] loss: 0.3356068843.230085611343[16,  8000] loss: 0.3015098865.745558738708[16,  8500] loss: 0.3208678888.257732152939[16,  9000] loss: 0.3507748910.800213098526[16,  9500] loss: 0.3003518933.145069360733[16, 10000] loss: 0.3567528955.617374420166[16, 10500] loss: 0.3148178977.885521173477[16, 11000] loss: 0.3467699000.319643497467[16, 11500] loss: 0.3397819022.875637292862[16, 12000] loss: 0.3498019045.310730934143[16, 12500] loss: 0.3381229067.898225784302Epoch [16] loss: 1004009.208270[17,   500] loss: 0.2808249090.48359489441[17,  1000] loss: 0.2571419112.941989660263[17,  1500] loss: 0.2858589135.617598772049[17,  2000] loss: 0.2872409158.062511205673[17,  2500] loss: 0.2618729180.366285800934[17,  3000] loss: 0.2547639202.712269306183[17,  3500] loss: 0.2891499225.143176317215[17,  4000] loss: 0.2692609247.718117237091[17,  4500] loss: 0.2593959270.152653455734[17,  5000] loss: 0.2406929292.591012239456[17,  5500] loss: 0.2850999315.24445438385[17,  6000] loss: 0.2999709337.796191453934[17,  6500] loss: 0.2806069360.250964164734[17,  7000] loss: 0.3032749382.70607829094[17,  7500] loss: 0.3201779405.258854150772[17,  8000] loss: 0.3548269427.772664308548[17,  8500] loss: 0.3134199450.304812908173[17,  9000] loss: 0.3190549472.827853441238[17,  9500] loss: 0.3112959495.415655374527[17, 10000] loss: 0.3446229518.04754781723[17, 10500] loss: 0.2794229540.729616165161[17, 11000] loss: 0.3315149563.406533241272[17, 11500] loss: 0.3663409585.91639995575[17, 12000] loss: 0.3291059608.502917528152[17, 12500] loss: 0.3077029630.996292352676Epoch [17] loss: 928166.386405[18,   500] loss: 0.2566579653.766775131226[18,  1000] loss: 0.2507829676.299525260925[18,  1500] loss: 0.2312279698.741688966751[18,  2000] loss: 0.2448129721.28795003891[18,  2500] loss: 0.2318369744.02438545227[18,  3000] loss: 0.2010199766.533569335938[18,  3500] loss: 0.2337459789.153128385544[18,  4000] loss: 0.2392159811.914859294891[18,  4500] loss: 0.2660619834.521200418472[18,  5000] loss: 0.2497479857.145733118057[18,  5500] loss: 0.2500559880.072817325592[18,  6000] loss: 0.3093149902.825620889664[18,  6500] loss: 0.2800439925.539582967758[18,  7000] loss: 0.2758919948.111685276031[18,  7500] loss: 0.2687769970.824148654938[18,  8000] loss: 0.2767239993.486221551895[18,  8500] loss: 0.25649710015.918305158615[18,  9000] loss: 0.27445810038.48482465744[18,  9500] loss: 0.28888410061.081337213516[18, 10000] loss: 0.28442710083.981688976288[18, 10500] loss: 0.28406310106.844988822937[18, 11000] loss: 0.33144610129.54730796814[18, 11500] loss: 0.27041510152.198766469955[18, 12000] loss: 0.25631710174.91376209259[18, 12500] loss: 0.27426110197.545411348343Epoch [18] loss: 824514.076245[19,   500] loss: 0.18874210220.526101350784[19,  1000] loss: 0.21472510243.433713674545[19,  1500] loss: 0.22200110266.21726346016[19,  2000] loss: 0.23687710288.767955303192[19,  2500] loss: 0.23907610311.622373342514[19,  3000] loss: 0.21120510334.281323671341[19,  3500] loss: 0.21178210356.892676353455[19,  4000] loss: 0.24856710379.363488197327[19,  4500] loss: 0.22700910402.071313142776[19,  5000] loss: 0.25037110424.591220617294[19,  5500] loss: 0.26359010447.131222963333[19,  6000] loss: 0.22345610469.937822580338[19,  6500] loss: 0.28778610492.622110128403[19,  7000] loss: 0.25767810515.151414871216[19,  7500] loss: 0.23893610537.65397143364[19,  8000] loss: 0.20027010560.132708311081[19,  8500] loss: 0.26560810582.621988773346[19,  9000] loss: 0.23902410605.35458278656[19,  9500] loss: 0.23645110627.873842716217[19, 10000] loss: 0.25687010650.329425811768[19, 10500] loss: 0.26746510673.147076368332[19, 11000] loss: 0.23196810695.682271718979[19, 11500] loss: 0.26311210718.226870298386[19, 12000] loss: 0.25319210741.07383775711[19, 12500] loss: 0.27737310763.719053983688Epoch [19] loss: 764439.250815[20,   500] loss: 0.23599610786.453660726547[20,  1000] loss: 0.24013310808.785233974457[20,  1500] loss: 0.28577710831.414349079132[20,  2000] loss: 0.21282110854.05872964859[20,  2500] loss: 0.21788610876.565974473953[20,  3000] loss: 0.25270510899.029497623444[20,  3500] loss: 0.19543210921.458001613617[20,  4000] loss: 0.23599010943.943826913834[20,  4500] loss: 0.22460710966.406038284302[20,  5000] loss: 0.20084910988.799670696259[20,  5500] loss: 0.22090011011.294651269913[20,  6000] loss: 0.22482911033.987253427505[20,  6500] loss: 0.23346111056.383397579193[20,  7000] loss: 0.21997811078.966337680817[20,  7500] loss: 0.24807511101.668555259705[20,  8000] loss: 0.26665111124.258160352707[20,  8500] loss: 0.24414011146.852136850357[20,  9000] loss: 0.23291111169.434042215347[20,  9500] loss: 0.24300911191.675781011581[20, 10000] loss: 0.26413411214.11050105095[20, 10500] loss: 0.21792411236.597955703735[20, 11000] loss: 0.23343411259.306109905243[20, 11500] loss: 0.23616211281.868612527847[20, 12000] loss: 0.23825511304.834299325943[20, 12500] loss: 0.24263911327.622391939163Epoch [20] loss: 742829.035130Finished TrainingSaving model to /data/s4091221/trained-models/wide_resnet101_22020-02-25 03:22:22.053526GroundTruth:    cat  ship  ship planeSending data to GPUSending model to GPUtensor([[-9.7474e-01, -5.4383e+00,  3.8907e+00,  1.5247e+01,  3.1519e+00,          4.5024e+00, -1.1574e-01, -4.3731e+00, -5.1838e+00, -8.6647e+00],        [ 1.3277e+00,  2.0439e+00, -8.8375e-01,  2.1057e-01, -7.0211e-02,         -1.0531e+00, -2.8468e+00, -1.5385e+00,  3.7278e+00,  6.6763e-01],        [ 1.6192e+00, -9.3763e-01, -8.9608e-01, -2.0958e-02, -3.3838e-03,         -4.9923e-01, -2.0235e+00, -8.8902e-01,  6.0407e+00, -1.1685e+00],        [ 2.9571e+00,  1.0710e+00,  1.1159e+00,  5.5756e-01,  1.9590e-01,         -1.1575e+00, -3.1813e+00, -1.7358e-01,  1.3489e+00, -1.6241e+00]],       device='cuda:0', grad_fn=<AddmmBackward>)Predicted:    cat  ship  ship planeAccuracy of the network on the 4000.0 test images: 76 %###############################################################################Peregrine ClusterJob 9730758 for user 's4091221'Finished at: Tue Feb 25 03:23:24 CET 2020Job details:============Name                : wide_resnet101.shUser                : s4091221Partition           : gpuNodes               : pg-gpu40Cores               : 12State               : COMPLETEDSubmit              : 2020-02-24T18:00:42Start               : 2020-02-24T21:01:03End                 : 2020-02-25T03:23:23Reserved walltime   : 15:00:00Used walltime       : 06:22:20Used CPU time       : 06:33:35 (efficiency:  8.58%)% User (Computation): 97.86%% System (I/O)      :  2.14%Mem reserved        : 12000M/nodeMax Mem used        : 3.24G (pg-gpu40)Max Disk Write      : 661.92M (pg-gpu40)Max Disk Read       : 1.06G (pg-gpu40)Acknowledgements:=================Please see this page for information about acknowledging Peregrine in your publications:https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output################################################################################