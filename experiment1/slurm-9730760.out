Requirement already satisfied: torchvision in ./.local/lib/python3.7/site-packages (0.5.0)Requirement already satisfied: pillow>=4.1.1 in ./.local/lib/python3.7/site-packages (from torchvision) (7.0.0)Requirement already satisfied: six in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from torchvision) (1.12.0)Requirement already satisfied: numpy in ./.local/lib/python3.7/site-packages (from torchvision) (1.18.1)Requirement already satisfied: torch==1.4.0 in ./.local/lib/python3.7/site-packages (from torchvision) (1.4.0)###########################################################################################################{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=34, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)Files already downloaded and verifiedFiles already downloaded and verifiedcuda:012500 bird  frog horse   car['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']Model wide_resnet50_2 LoadedResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): Bottleneck(      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer2): Sequential(    (0): Bottleneck(      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer3): Sequential(    (0): Bottleneck(      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (4): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (5): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer4): Sequential(    (0): Bottleneck(      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=2048, out_features=10, bias=True))Model wide_resnet50_2 ReshapedSending model to GPULearning Rate: 0.001, Weight Decay: 0, Momentum: 0Defined <class 'torch.optim.sgd.SGD'> OptimizerStarting Training at 1582574943.3645508[1,   500] loss: 2.63201014.306904315948486[1,  1000] loss: 2.61814326.664833068847656[1,  1500] loss: 2.55194839.06929874420166[1,  2000] loss: 2.55807951.60734963417053[1,  2500] loss: 2.50501963.95058345794678[1,  3000] loss: 2.51035976.42018413543701[1,  3500] loss: 2.47264289.00597381591797[1,  4000] loss: 2.462445102.25552606582642[1,  4500] loss: 2.430845114.70554780960083[1,  5000] loss: 2.428726127.22034025192261[1,  5500] loss: 2.419243139.84798073768616[1,  6000] loss: 2.379072152.42083430290222[1,  6500] loss: 2.386930164.85644054412842[1,  7000] loss: 2.361029177.42636966705322[1,  7500] loss: 2.353888190.06053471565247[1,  8000] loss: 2.337022202.7044506072998[1,  8500] loss: 2.341033215.1955692768097[1,  9000] loss: 2.321027227.74051642417908[1,  9500] loss: 2.336435240.31557512283325[1, 10000] loss: 2.420013252.87680220603943[1, 10500] loss: 2.384769265.3952121734619[1, 11000] loss: 2.388298277.897013425827[1, 11500] loss: 2.371674290.31877541542053[1, 12000] loss: 2.370020302.8268599510193[1, 12500] loss: 2.343423315.4463846683502Epoch [1] loss: 7596090.317179[2,   500] loss: 2.314682328.3470368385315[2,  1000] loss: 2.331526341.13027715682983[2,  1500] loss: 2.248681353.61812710762024[2,  2000] loss: 2.262640366.0952339172363[2,  2500] loss: 2.254761378.79782342910767[2,  3000] loss: 2.237572391.36550664901733[2,  3500] loss: 2.239461403.8512279987335[2,  4000] loss: 2.181102416.30045318603516[2,  4500] loss: 2.228346428.7637662887573[2,  5000] loss: 2.217424441.4396698474884[2,  5500] loss: 2.190834453.9122083187103[2,  6000] loss: 2.146477466.5838027000427[2,  6500] loss: 2.195484479.23393845558167[2,  7000] loss: 2.152943491.7758946418762[2,  7500] loss: 2.153760504.4715986251831[2,  8000] loss: 2.145781516.9138464927673[2,  8500] loss: 2.152626529.3180358409882[2,  9000] loss: 2.118002541.7538096904755[2,  9500] loss: 2.127732554.2201204299927[2, 10000] loss: 2.088994566.7145836353302[2, 10500] loss: 2.083912579.2104148864746[2, 11000] loss: 2.063135591.7901077270508[2, 11500] loss: 2.072800604.2699093818665[2, 12000] loss: 2.046823616.6215572357178[2, 12500] loss: 2.081038629.0840272903442Epoch [2] loss: 6809652.355023[3,   500] loss: 2.060466641.9181087017059[3,  1000] loss: 2.090194654.4123883247375[3,  1500] loss: 2.033284666.9866330623627[3,  2000] loss: 2.025657679.8395919799805[3,  2500] loss: 2.037112692.4713995456696[3,  3000] loss: 2.022739704.8912281990051[3,  3500] loss: 1.999102717.4214978218079[3,  4000] loss: 2.004320730.0640969276428[3,  4500] loss: 2.002765742.6688165664673[3,  5000] loss: 1.995932755.1904797554016[3,  5500] loss: 2.038769767.7597358226776[3,  6000] loss: 1.941167780.3203427791595[3,  6500] loss: 1.962626793.1554899215698[3,  7000] loss: 1.965357805.6183273792267[3,  7500] loss: 1.916954818.5786471366882[3,  8000] loss: 1.874341831.0162742137909[3,  8500] loss: 1.959915843.6305072307587[3,  9000] loss: 1.911913856.3028936386108[3,  9500] loss: 1.942162868.9223024845123[3, 10000] loss: 1.981607881.5094766616821[3, 10500] loss: 1.917740894.1128170490265[3, 11000] loss: 1.910740906.9088070392609[3, 11500] loss: 1.929252919.4517996311188[3, 12000] loss: 1.921188931.9842140674591[3, 12500] loss: 1.876839944.6050109863281Epoch [3] loss: 6191678.673570[4,   500] loss: 1.880852957.2316319942474[4,  1000] loss: 1.890027969.7911441326141[4,  1500] loss: 1.834633982.3297491073608[4,  2000] loss: 1.909212994.8466219902039[4,  2500] loss: 1.8439141007.2372395992279[4,  3000] loss: 1.8372601019.752370595932[4,  3500] loss: 1.8448691032.4761583805084[4,  4000] loss: 1.8729291045.3675079345703[4,  4500] loss: 1.8311911058.007539510727[4,  5000] loss: 1.8610521070.509276151657[4,  5500] loss: 1.8959961083.082691669464[4,  6000] loss: 1.8212351095.576708316803[4,  6500] loss: 1.7844621108.026550769806[4,  7000] loss: 1.8503461120.6280987262726[4,  7500] loss: 1.8423841133.1582238674164[4,  8000] loss: 1.8594501145.6859560012817[4,  8500] loss: 1.8134021158.2775294780731[4,  9000] loss: 1.8065201170.7572841644287[4,  9500] loss: 1.7821121183.1090440750122[4, 10000] loss: 1.7963341195.6892552375793[4, 10500] loss: 1.7826091208.2775480747223[4, 11000] loss: 1.8311731220.9833471775055[4, 11500] loss: 1.7614981233.5629832744598[4, 12000] loss: 1.8096161246.3672354221344[4, 12500] loss: 1.7823241259.2173345088959Epoch [4] loss: 5750664.748133[5,   500] loss: 1.7680741272.0519876480103[5,  1000] loss: 1.7063051284.6496744155884[5,  1500] loss: 1.7597011297.1600410938263[5,  2000] loss: 1.7110241309.68071103096[5,  2500] loss: 1.7528281322.2378644943237[5,  3000] loss: 1.7850731334.786409854889[5,  3500] loss: 1.6985971347.274665594101[5,  4000] loss: 1.7339501359.7871770858765[5,  4500] loss: 1.7224191372.2940926551819[5,  5000] loss: 1.7336241384.991839647293[5,  5500] loss: 1.7741551397.645595550537[5,  6000] loss: 1.6850251410.188140630722[5,  6500] loss: 1.7526381422.8620297908783[5,  7000] loss: 1.7305541435.6111979484558[5,  7500] loss: 1.6935191448.1803514957428[5,  8000] loss: 1.7380251460.6785070896149[5,  8500] loss: 1.7236021473.0654389858246[5,  9000] loss: 1.6970411485.5107350349426[5,  9500] loss: 1.7026981498.0450892448425[5, 10000] loss: 1.6827811510.4709055423737[5, 10500] loss: 1.6873981523.0789804458618[5, 11000] loss: 1.6790361535.61824965477[5, 11500] loss: 1.6784361548.0624477863312[5, 12000] loss: 1.6816191560.8014779090881[5, 12500] loss: 1.6791281573.2514498233795Epoch [5] loss: 5382362.965435[6,   500] loss: 1.6615781585.7456357479095[6,  1000] loss: 1.5952321598.2723977565765[6,  1500] loss: 1.6599951610.6957449913025[6,  2000] loss: 1.6662781623.2293558120728[6,  2500] loss: 1.6275741635.6706087589264[6,  3000] loss: 1.6660131648.1199622154236[6,  3500] loss: 1.6559901660.5817172527313[6,  4000] loss: 1.6611911673.0393559932709[6,  4500] loss: 1.5828151685.5919003486633[6,  5000] loss: 1.6305811698.0190024375916[6,  5500] loss: 1.6129331710.429337978363[6,  6000] loss: 1.6184721723.1088981628418[6,  6500] loss: 1.5753471735.985119819641[6,  7000] loss: 1.6046151748.4942276477814[6,  7500] loss: 1.6356961760.9523813724518[6,  8000] loss: 1.6248681773.4536488056183[6,  8500] loss: 1.6134651785.9903740882874[6,  9000] loss: 1.6018021798.5435109138489[6,  9500] loss: 1.6278051811.0499067306519[6, 10000] loss: 1.5974991823.448531627655[6, 10500] loss: 1.5733441835.8005061149597[6, 11000] loss: 1.6329811848.1830742359161[6, 11500] loss: 1.6294551860.7417423725128[6, 12000] loss: 1.5838731873.292837381363[6, 12500] loss: 1.6128701885.8064558506012Epoch [6] loss: 5077663.591371[7,   500] loss: 1.5673361898.503966808319[7,  1000] loss: 1.6070391911.249850988388[7,  1500] loss: 1.5969161923.7792282104492[7,  2000] loss: 1.5144271936.3428626060486[7,  2500] loss: 1.5428721948.773095369339[7,  3000] loss: 1.5528711961.1741111278534[7,  3500] loss: 1.5567211973.6246950626373[7,  4000] loss: 1.5297881986.054293870926[7,  4500] loss: 1.5414721998.5721299648285[7,  5000] loss: 1.4859852011.073443889618[7,  5500] loss: 1.5395802023.3500480651855[7,  6000] loss: 1.5221222035.7390961647034[7,  6500] loss: 1.5373712048.2151069641113[7,  7000] loss: 1.4946492060.55073928833[7,  7500] loss: 1.5256292072.92191696167[7,  8000] loss: 1.5866682085.2973136901855[7,  8500] loss: 1.5192762097.6497180461884[7,  9000] loss: 1.5349872109.9876461029053[7,  9500] loss: 1.5398102122.452803373337[7, 10000] loss: 1.5265042134.9863846302032[7, 10500] loss: 1.4845632147.4024274349213[7, 11000] loss: 1.4948372159.8153915405273[7, 11500] loss: 1.4920882172.3305575847626[7, 12000] loss: 1.5103712184.762909412384[7, 12500] loss: 1.5122662197.1313779354095Epoch [7] loss: 4797178.365256[8,   500] loss: 1.4718212209.788002729416[8,  1000] loss: 1.4606922222.1306385993958[8,  1500] loss: 1.4376152234.4321949481964[8,  2000] loss: 1.4863962246.6911499500275[8,  2500] loss: 1.4624112259.024313926697[8,  3000] loss: 1.4409462271.381919145584[8,  3500] loss: 1.4778272283.909359693527[8,  4000] loss: 1.4329742296.4055755138397[8,  4500] loss: 1.4700052308.7441782951355[8,  5000] loss: 1.4677342321.2850008010864[8,  5500] loss: 1.4574632333.73197722435[8,  6000] loss: 1.4721992346.978773832321[8,  6500] loss: 1.4501212359.3827011585236[8,  7000] loss: 1.4382322371.8319396972656[8,  7500] loss: 1.4368532384.6359407901764[8,  8000] loss: 1.4500972397.2198781967163[8,  8500] loss: 1.4404672409.683627843857[8,  9000] loss: 1.4776892422.1472771167755[8,  9500] loss: 1.4367762434.7108545303345[8, 10000] loss: 1.4565062447.214741230011[8, 10500] loss: 1.4590072459.6843786239624[8, 11000] loss: 1.4296842472.2165660858154[8, 11500] loss: 1.4844732484.7391064167023[8, 12000] loss: 1.4522962497.0812101364136[8, 12500] loss: 1.4087262509.6478338241577Epoch [8] loss: 4559753.737137[9,   500] loss: 1.4069502522.336088657379[9,  1000] loss: 1.3784552534.9430301189423[9,  1500] loss: 1.4037182547.479490041733[9,  2000] loss: 1.3789942560.020297765732[9,  2500] loss: 1.3964552572.5040826797485[9,  3000] loss: 1.4026022585.096970796585[9,  3500] loss: 1.4175952597.5887565612793[9,  4000] loss: 1.4317862610.088386297226[9,  4500] loss: 1.3680892622.597633600235[9,  5000] loss: 1.4378562635.2453587055206[9,  5500] loss: 1.4180222647.7453207969666[9,  6000] loss: 1.3565862660.322463274002[9,  6500] loss: 1.4221722672.8796820640564[9,  7000] loss: 1.3777712685.6153037548065[9,  7500] loss: 1.4012142698.212068796158[9,  8000] loss: 1.3670122710.7842829227448[9,  8500] loss: 1.4147252723.235922574997[9,  9000] loss: 1.3792452735.80784535408[9,  9500] loss: 1.3403262748.486648082733[9, 10000] loss: 1.3898952761.0739002227783[9, 10500] loss: 1.3551592773.677678346634[9, 11000] loss: 1.4171782786.1586344242096[9, 11500] loss: 1.3713332798.625624895096[9, 12000] loss: 1.3653552811.134389400482[9, 12500] loss: 1.3757852823.749181985855Epoch [9] loss: 4344750.726922[10,   500] loss: 1.3622192836.2444908618927[10,  1000] loss: 1.3794302848.6822497844696[10,  1500] loss: 1.3768002861.0809252262115[10,  2000] loss: 1.3318822873.6233196258545[10,  2500] loss: 1.3372402885.978969335556[10,  3000] loss: 1.3021822898.3869619369507[10,  3500] loss: 1.3968102910.8628816604614[10,  4000] loss: 1.3571682923.3407459259033[10,  4500] loss: 1.2822742936.1186983585358[10,  5000] loss: 1.3321542948.729241371155[10,  5500] loss: 1.3118222961.084853887558[10,  6000] loss: 1.3145402973.539113998413[10,  6500] loss: 1.3452352986.0480449199677[10,  7000] loss: 1.3489742998.5461728572845[10,  7500] loss: 1.3182543011.07529091835[10,  8000] loss: 1.3425463023.483682155609[10,  8500] loss: 1.3224543035.997528076172[10,  9000] loss: 1.3713583048.443863391876[10,  9500] loss: 1.3261643061.0688276290894[10, 10000] loss: 1.2995753073.4258046150208[10, 10500] loss: 1.3014353085.7493484020233[10, 11000] loss: 1.3104293098.225022792816[10, 11500] loss: 1.2896903110.687888622284[10, 12000] loss: 1.2749523123.293755531311[10, 12500] loss: 1.3178033135.694245815277Epoch [10] loss: 4165334.008520[11,   500] loss: 1.3132293148.3928031921387[11,  1000] loss: 1.2814783160.8943893909454[11,  1500] loss: 1.2194943173.514596939087[11,  2000] loss: 1.2582163186.1576948165894[11,  2500] loss: 1.2563603198.6996870040894[11,  3000] loss: 1.2744023211.0697495937347[11,  3500] loss: 1.2725523223.6533482074738[11,  4000] loss: 1.2786123236.1783814430237[11,  4500] loss: 1.2636403248.722489595413[11,  5000] loss: 1.2753563261.2532935142517[11,  5500] loss: 1.2706013273.7137472629547[11,  6000] loss: 1.2807283286.2210714817047[11,  6500] loss: 1.2710553298.7399401664734[11,  7000] loss: 1.3101113311.301626443863[11,  7500] loss: 1.3017403323.858101129532[11,  8000] loss: 1.2708493336.576349258423[11,  8500] loss: 1.2845863349.1709489822388[11,  9000] loss: 1.2741123361.8817908763885[11,  9500] loss: 1.2691193374.3493666648865[11, 10000] loss: 1.2700083387.0596220493317[11, 10500] loss: 1.2990133399.663093805313[11, 11000] loss: 1.2746633412.1513707637787[11, 11500] loss: 1.2354693424.6405413150787[11, 12000] loss: 1.2802733437.1475150585175[11, 12500] loss: 1.3403423449.5831921100616Epoch [11] loss: 4001879.059855[12,   500] loss: 1.2465803462.2364933490753[12,  1000] loss: 1.1494613474.771395921707[12,  1500] loss: 1.2013643487.3094000816345[12,  2000] loss: 1.2374503499.8551716804504[12,  2500] loss: 1.2050703512.490736246109[12,  3000] loss: 1.2176643524.9954268932343[12,  3500] loss: 1.2293423537.652559041977[12,  4000] loss: 1.1318193550.179156780243[12,  4500] loss: 1.2077563562.6991353034973[12,  5000] loss: 1.2288373575.2519245147705[12,  5500] loss: 1.1713463587.904164791107[12,  6000] loss: 1.2383943600.4333856105804[12,  6500] loss: 1.2248423612.9991080760956[12,  7000] loss: 1.2513643625.50745844841[12,  7500] loss: 1.2481123638.1490516662598[12,  8000] loss: 1.1994383650.6292135715485[12,  8500] loss: 1.2127013663.2102382183075[12,  9000] loss: 1.2004853675.6475105285645[12,  9500] loss: 1.2305743688.1807730197906[12, 10000] loss: 1.2171583700.6599979400635[12, 10500] loss: 1.1950493713.116182565689[12, 11000] loss: 1.1986893725.5643525123596[12, 11500] loss: 1.2113713737.9979798793793[12, 12000] loss: 1.2330773750.5631835460663[12, 12500] loss: 1.2293883763.236046075821Epoch [12] loss: 3805718.641132[13,   500] loss: 1.1904433775.8676130771637[13,  1000] loss: 1.1610863788.395576477051[13,  1500] loss: 1.1536093800.8965606689453[13,  2000] loss: 1.1656053813.3845670223236[13,  2500] loss: 1.1334553826.000492334366[13,  3000] loss: 1.1330133838.322598218918[13,  3500] loss: 1.2273973851.053168773651[13,  4000] loss: 1.1581133863.4504873752594[13,  4500] loss: 1.1611343875.991798877716[13,  5000] loss: 1.1731793888.3657042980194[13,  5500] loss: 1.2034523900.7547268867493[13,  6000] loss: 1.1681943914.842740058899[13,  6500] loss: 1.1968793927.2785601615906[13,  7000] loss: 1.2311893939.6294133663177[13,  7500] loss: 1.1722953952.0897917747498[13,  8000] loss: 1.1761073964.6259717941284[13,  8500] loss: 1.1951413977.2106926441193[13,  9000] loss: 1.1973153989.619122028351[13,  9500] loss: 1.1801724002.1013345718384[13, 10000] loss: 1.2034464014.4894466400146[13, 10500] loss: 1.1325934026.9081485271454[13, 11000] loss: 1.1718054039.415389060974[13, 11500] loss: 1.1268204051.9380090236664[13, 12000] loss: 1.1649304065.1385242938995[13, 12500] loss: 1.1503854077.645473718643Epoch [13] loss: 3678380.412575[14,   500] loss: 1.0726534090.722477912903[14,  1000] loss: 1.1272404103.087168455124[14,  1500] loss: 1.1282284115.399580001831[14,  2000] loss: 1.0762914127.866713762283[14,  2500] loss: 1.1492524140.345557928085[14,  3000] loss: 1.1249464152.952914714813[14,  3500] loss: 1.1053144165.463265419006[14,  4000] loss: 1.1493904177.976864814758[14,  4500] loss: 1.1621934190.5157771110535[14,  5000] loss: 1.1268894203.175402402878[14,  5500] loss: 1.0994204215.891880989075[14,  6000] loss: 1.1548334229.893216848373[14,  6500] loss: 1.1287294242.415972232819[14,  7000] loss: 1.1276534254.868410348892[14,  7500] loss: 1.1137454267.51541352272[14,  8000] loss: 1.1314994279.91786813736[14,  8500] loss: 1.1305654292.4925236701965[14,  9000] loss: 1.1548584304.993990421295[14,  9500] loss: 1.1645314317.388871669769[14, 10000] loss: 1.1191624329.825075864792[14, 10500] loss: 1.1403194342.361384868622[14, 11000] loss: 1.1122434354.900490999222[14, 11500] loss: 1.1246704367.2971971035[14, 12000] loss: 1.1284374379.67965221405[14, 12500] loss: 1.1220784392.1202092170715Epoch [14] loss: 3529604.709229[15,   500] loss: 1.0240864404.867943048477[15,  1000] loss: 1.0644084417.235389709473[15,  1500] loss: 1.0139634429.682257652283[15,  2000] loss: 1.0220234442.235682964325[15,  2500] loss: 1.0965114454.876413822174[15,  3000] loss: 1.0617864467.487841844559[15,  3500] loss: 1.0581144480.064082145691[15,  4000] loss: 1.0881454492.633392095566[15,  4500] loss: 1.0953504505.360179901123[15,  5000] loss: 1.0556504518.019182682037[15,  5500] loss: 1.1054644530.5589282512665[15,  6000] loss: 1.0904894542.989255428314[15,  6500] loss: 1.0912604555.282634973526[15,  7000] loss: 1.0942454567.7231295108795[15,  7500] loss: 1.0693704580.223997116089[15,  8000] loss: 1.0235514592.66659617424[15,  8500] loss: 1.0991294605.065706729889[15,  9000] loss: 1.1241994617.585777521133[15,  9500] loss: 1.0606894630.017764091492[15, 10000] loss: 1.1186084642.516992330551[15, 10500] loss: 1.0569184655.123815774918[15, 11000] loss: 1.0722924667.62899184227[15, 11500] loss: 1.1196914680.179525375366[15, 12000] loss: 1.0668664692.773602724075[15, 12500] loss: 1.1214084705.269499540329Epoch [15] loss: 3372301.413720[16,   500] loss: 1.0406924717.979860305786[16,  1000] loss: 0.9643074730.508371591568[16,  1500] loss: 1.0437644742.986946582794[16,  2000] loss: 1.0016614755.486067533493[16,  2500] loss: 1.0278184767.905898094177[16,  3000] loss: 1.0560284780.459935903549[16,  3500] loss: 1.0289604792.959100723267[16,  4000] loss: 1.0039774805.388038158417[16,  4500] loss: 1.0271694817.797222137451[16,  5000] loss: 1.0381464830.1482582092285[16,  5500] loss: 1.0438034842.548359155655[16,  6000] loss: 1.0479824854.894086122513[16,  6500] loss: 1.0238374867.187419652939[16,  7000] loss: 1.0355774879.676954269409[16,  7500] loss: 1.0359554891.988510847092[16,  8000] loss: 0.9914474904.350051403046[16,  8500] loss: 1.0625324916.744047641754[16,  9000] loss: 1.0237654929.192498445511[16,  9500] loss: 1.0161924941.6122579574585[16, 10000] loss: 1.0256174953.967369794846[16, 10500] loss: 1.0811724966.3375852108[16, 11000] loss: 1.0103614978.709533929825[16, 11500] loss: 1.0438574991.140751361847[16, 12000] loss: 0.9918335003.636889457703[16, 12500] loss: 1.0642725016.388180017471Epoch [16] loss: 3219184.366796[17,   500] loss: 0.9086325029.1894063949585[17,  1000] loss: 0.9439945041.692103624344[17,  1500] loss: 0.9798505054.207106113434[17,  2000] loss: 0.9516215066.573637008667[17,  2500] loss: 0.9763165079.24427652359[17,  3000] loss: 0.9781825091.629257202148[17,  3500] loss: 0.9185345104.1024351119995[17,  4000] loss: 0.9684955116.490049123764[17,  4500] loss: 0.9972105128.843257665634[17,  5000] loss: 1.0043765141.220052719116[17,  5500] loss: 0.9990815153.570099115372[17,  6000] loss: 0.9972595166.222323656082[17,  6500] loss: 1.0028945178.677423715591[17,  7000] loss: 0.9753945191.381598949432[17,  7500] loss: 0.9847165203.858825206757[17,  8000] loss: 0.9587175216.327141046524[17,  8500] loss: 0.9796995228.979407072067[17,  9000] loss: 1.0419905241.4427037239075[17,  9500] loss: 0.9897615254.079441308975[17, 10000] loss: 0.9530825266.691066265106[17, 10500] loss: 0.9641515279.217157363892[17, 11000] loss: 0.9960515291.898875236511[17, 11500] loss: 0.9944095304.611876964569[17, 12000] loss: 1.0491955317.171295881271[17, 12500] loss: 1.0083685329.6937165260315Epoch [17] loss: 3058603.805367[18,   500] loss: 0.9071105342.3927936553955[18,  1000] loss: 0.9451645354.811519861221[18,  1500] loss: 0.8860275367.233616352081[18,  2000] loss: 0.9469575379.687456846237[18,  2500] loss: 0.9111765392.061671257019[18,  3000] loss: 0.9069985404.628693819046[18,  3500] loss: 1.0498255417.093465328217[18,  4000] loss: 1.0218635429.499166727066[18,  4500] loss: 0.9649855441.96244263649[18,  5000] loss: 0.9645515454.529598236084[18,  5500] loss: 0.9677335467.105608463287[18,  6000] loss: 0.9300765479.482834815979[18,  6500] loss: 0.9882295492.009565353394[18,  7000] loss: 0.9044925504.501403331757[18,  7500] loss: 0.9463325517.098587274551[18,  8000] loss: 0.9772325529.734784126282[18,  8500] loss: 0.9295515542.172996520996[18,  9000] loss: 0.9414885554.770644903183[18,  9500] loss: 0.9727525567.293035030365[18, 10000] loss: 0.9409875579.804199457169[18, 10500] loss: 0.9437675592.338586091995[18, 11000] loss: 0.9559855604.916298627853[18, 11500] loss: 0.9572615617.429883718491[18, 12000] loss: 0.9189005629.848534345627[18, 12500] loss: 0.9458545642.460909128189Epoch [18] loss: 2965157.851262[19,   500] loss: 0.8717075655.0720911026[19,  1000] loss: 0.8154385667.6257038116455[19,  1500] loss: 0.8558705680.123046159744[19,  2000] loss: 0.9074075692.620275020599[19,  2500] loss: 0.9075025705.227912902832[19,  3000] loss: 0.8843715717.799074172974[19,  3500] loss: 0.9247175730.259898662567[19,  4000] loss: 0.8836335742.854773521423[19,  4500] loss: 0.8501615755.621887922287[19,  5000] loss: 0.8909765768.296995639801[19,  5500] loss: 0.8513665780.8671996593475[19,  6000] loss: 0.9434325793.326215982437[19,  6500] loss: 0.8776815805.731315851212[19,  7000] loss: 0.9257315818.383810043335[19,  7500] loss: 0.8909805830.927246570587[19,  8000] loss: 0.8570755843.449436187744[19,  8500] loss: 0.9198505855.9239153862[19,  9000] loss: 0.8791695868.4275159835815[19,  9500] loss: 0.9137105880.872357606888[19, 10000] loss: 0.8991025893.5750279426575[19, 10500] loss: 0.9028375906.299508810043[19, 11000] loss: 0.8860385918.740782737732[19, 11500] loss: 0.8826625931.273916959763[19, 12000] loss: 0.9432215943.842097520828[19, 12500] loss: 0.9512815956.292984247208Epoch [19] loss: 2798060.026144[20,   500] loss: 0.8331855968.970889806747[20,  1000] loss: 0.8116735981.631561994553[20,  1500] loss: 0.8336975994.22313451767[20,  2000] loss: 0.8708946006.83974814415[20,  2500] loss: 0.8224016019.3438222408295[20,  3000] loss: 0.8499676031.900447845459[20,  3500] loss: 0.8604036044.419991493225[20,  4000] loss: 0.8825156056.998572349548[20,  4500] loss: 0.8121406069.4515376091[20,  5000] loss: 0.8226256081.969437360764[20,  5500] loss: 0.8638616094.745107889175[20,  6000] loss: 0.8232356107.456777095795[20,  6500] loss: 0.8718586120.045123577118[20,  7000] loss: 0.8737186132.612503290176[20,  7500] loss: 0.8273056145.184738874435[20,  8000] loss: 0.8383536157.742792606354[20,  8500] loss: 0.8533436170.49244260788[20,  9000] loss: 0.8429096183.005213737488[20,  9500] loss: 0.8666466195.471788883209[20, 10000] loss: 0.8744946208.106018304825[20, 10500] loss: 0.8464626220.552500486374[20, 11000] loss: 0.8744676233.165962696075[20, 11500] loss: 0.8679856245.730822086334[20, 12000] loss: 0.8533476258.177028179169[20, 12500] loss: 0.8944246270.725574016571Epoch [20] loss: 2658886.122735Finished TrainingSaving model to /data/s4091221/trained-models/wide_resnet50_22020-02-24 22:53:34.141444GroundTruth:    cat  ship  ship planeSending data to GPUSending model to GPUtensor([[-2.3271,  0.7453, -2.7263,  5.4485, -1.0552,  1.9348, -1.4281, -2.1350,          2.0899, -1.8453],        [ 2.3612,  4.9758, -3.9805, -1.9219, -2.4858, -3.0328, -4.7120, -3.0164,          7.2603,  3.8492],        [ 2.7649,  0.8419, -3.6590, -1.2039, -0.3229, -2.0820, -4.1731, -0.5798,          5.3162,  2.5409],        [ 3.4687,  1.0487, -1.0294, -0.7250,  1.6726, -2.1500, -4.4205, -0.6958,          1.7416, -0.2244]], device='cuda:0', grad_fn=<AddmmBackward>)Predicted:    cat  ship  ship planeAccuracy of the network on the 4000.0 test images: 63 %###########################################################################################################{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': True, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=34, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=True, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)Files already downloaded and verifiedFiles already downloaded and verifiedcuda:0Downloading: "https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth" to /home/s4091221/.cache/torch/checkpoints/wide_resnet50_2-95faca4d.pth12500  dog  bird plane  deer['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']Model wide_resnet50_2 LoadedResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): Bottleneck(      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer2): Sequential(    (0): Bottleneck(      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer3): Sequential(    (0): Bottleneck(      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (3): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (4): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (5): Bottleneck(      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (layer4): Sequential(    (0): Bottleneck(      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (downsample): Sequential(        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )    (2): Bottleneck(      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=2048, out_features=10, bias=True))Model wide_resnet50_2 ReshapedSending model to GPULearning Rate: 0.001, Weight Decay: 0, Momentum: 0Defined <class 'torch.optim.sgd.SGD'> OptimizerStarting Training at 1582581291.4345942[1,   500] loss: 2.27494915.39644718170166[1,  1000] loss: 2.24045528.41538405418396[1,  1500] loss: 2.13924341.050172567367554[1,  2000] loss: 2.09352353.566638469696045[1,  2500] loss: 2.07252366.19929051399231[1,  3000] loss: 2.06632878.75258994102478[1,  3500] loss: 2.00620291.23337936401367[1,  4000] loss: 2.004275103.70827221870422[1,  4500] loss: 1.969720116.5324056148529[1,  5000] loss: 1.929182129.28939628601074[1,  5500] loss: 1.890062141.95293807983398[1,  6000] loss: 1.922507154.51276683807373[1,  6500] loss: 1.871627166.9102418422699[1,  7000] loss: 1.861643179.4255621433258[1,  7500] loss: 1.843762191.92026925086975[1,  8000] loss: 1.841783204.53502106666565[1,  8500] loss: 1.799258217.28284096717834[1,  9000] loss: 1.799582230.10160756111145[1,  9500] loss: 1.809454242.6206920146942[1, 10000] loss: 1.794683255.0627555847168[1, 10500] loss: 1.707457267.5821695327759[1, 11000] loss: 1.748615279.9899482727051[1, 11500] loss: 1.670860292.62575817108154[1, 12000] loss: 1.650167305.1035454273224[1, 12500] loss: 1.670563317.72618770599365Epoch [1] loss: 5981958.121254[2,   500] loss: 1.655887330.6442275047302[2,  1000] loss: 1.646813343.19601821899414[2,  1500] loss: 1.610184355.77798891067505[2,  2000] loss: 1.603346368.28771686553955[2,  2500] loss: 1.544439380.8330307006836[2,  3000] loss: 1.568520393.65769505500793[2,  3500] loss: 1.543853406.21712708473206[2,  4000] loss: 1.523914418.8495829105377[2,  4500] loss: 1.510909431.57291984558105[2,  5000] loss: 1.491210444.13169050216675[2,  5500] loss: 1.477190456.57259821891785[2,  6000] loss: 1.486739469.1998426914215[2,  6500] loss: 1.482194481.79722571372986[2,  7000] loss: 1.456550494.53293085098267[2,  7500] loss: 1.424466507.45637559890747[2,  8000] loss: 1.421835520.229998588562[2,  8500] loss: 1.473130532.9464635848999[2,  9000] loss: 1.354861545.4011225700378[2,  9500] loss: 1.387133557.9304573535919[2, 10000] loss: 1.415221570.4466788768768[2, 10500] loss: 1.396690583.1154084205627[2, 11000] loss: 1.358718595.7341623306274[2, 11500] loss: 1.338058608.725759267807[2, 12000] loss: 1.372806621.28049492836[2, 12500] loss: 1.387767633.9707081317902Epoch [2] loss: 4620567.715444[3,   500] loss: 1.329886647.1199572086334[3,  1000] loss: 1.306929660.0035796165466[3,  1500] loss: 1.314596672.747962474823[3,  2000] loss: 1.318340685.2626812458038[3,  2500] loss: 1.288547697.7893929481506[3,  3000] loss: 1.273919710.2717070579529[3,  3500] loss: 1.241285722.9837200641632[3,  4000] loss: 1.226053735.6189024448395[3,  4500] loss: 1.261172748.2442615032196[3,  5000] loss: 1.293562760.7093138694763[3,  5500] loss: 1.282322773.160249710083[3,  6000] loss: 1.251612785.6332862377167[3,  6500] loss: 1.246504798.3443496227264[3,  7000] loss: 1.235337810.9090709686279[3,  7500] loss: 1.169326823.6109321117401[3,  8000] loss: 1.226565836.0230643749237[3,  8500] loss: 1.158263849.0632305145264[3,  9000] loss: 1.192228861.4613728523254[3,  9500] loss: 1.188862874.0018475055695[3, 10000] loss: 1.196258886.7636601924896[3, 10500] loss: 1.179164899.2530896663666[3, 11000] loss: 1.120392911.8092772960663[3, 11500] loss: 1.133018924.5999159812927[3, 12000] loss: 1.186994937.1663944721222[3, 12500] loss: 1.145778949.9621877670288Epoch [3] loss: 3857649.068897[4,   500] loss: 1.082202962.7824091911316[4,  1000] loss: 1.105672975.4839804172516[4,  1500] loss: 1.083632987.9680547714233[4,  2000] loss: 1.0591611000.5307664871216[4,  2500] loss: 1.1035501013.141637802124[4,  3000] loss: 1.0643001025.658935546875[4,  3500] loss: 1.0908881038.349487543106[4,  4000] loss: 1.0515511050.841145992279[4,  4500] loss: 1.0562981063.365892648697[4,  5000] loss: 1.0282481075.7688627243042[4,  5500] loss: 1.0820561088.2456941604614[4,  6000] loss: 1.0883291100.8260746002197[4,  6500] loss: 1.0468311113.3729493618011[4,  7000] loss: 1.0644351125.8307752609253[4,  7500] loss: 1.0552011138.2979094982147[4,  8000] loss: 1.0588851150.8672504425049[4,  8500] loss: 1.0008311163.5070188045502[4,  9000] loss: 1.0166871176.1190350055695[4,  9500] loss: 0.9996371188.8383226394653[4, 10000] loss: 1.0362051201.1247143745422[4, 10500] loss: 1.0105411213.5086147785187[4, 11000] loss: 1.0094051225.7826869487762[4, 11500] loss: 1.0255571238.0801603794098[4, 12000] loss: 0.9904461250.4850010871887[4, 12500] loss: 1.0050481262.9493033885956Epoch [4] loss: 3274974.044676[5,   500] loss: 1.0154271275.4798679351807[5,  1000] loss: 0.9405931287.8338296413422[5,  1500] loss: 0.9505061300.3749372959137[5,  2000] loss: 0.9974841312.8509945869446[5,  2500] loss: 0.9222531325.3318769931793[5,  3000] loss: 0.9976611337.6394069194794[5,  3500] loss: 0.9744381350.0146837234497[5,  4000] loss: 0.9505971362.3363432884216[5,  4500] loss: 0.9183081374.6563456058502[5,  5000] loss: 0.9234771386.9754257202148[5,  5500] loss: 0.9466121399.459513425827[5,  6000] loss: 0.9388121411.768708229065[5,  6500] loss: 0.9337801424.0569727420807[5,  7000] loss: 0.9793381436.3439662456512[5,  7500] loss: 0.9386371448.6930258274078[5,  8000] loss: 0.9405211461.01256275177[5,  8500] loss: 0.9723481473.3485372066498[5,  9000] loss: 0.8603581485.9154393672943[5,  9500] loss: 0.8960481498.2443783283234[5, 10000] loss: 0.9154031510.6891996860504[5, 10500] loss: 0.9025041523.0973935127258[5, 11000] loss: 0.9435341535.4142138957977[5, 11500] loss: 0.8991061548.0103511810303[5, 12000] loss: 0.8801051560.4483199119568[5, 12500] loss: 0.9039751572.8734681606293Epoch [5] loss: 2958376.421676[6,   500] loss: 0.8657911585.3869788646698[6,  1000] loss: 0.8638821597.9918148517609[6,  1500] loss: 0.8567161610.5790898799896[6,  2000] loss: 0.9271391623.1455526351929[6,  2500] loss: 0.8385561635.8234384059906[6,  3000] loss: 0.8764181648.3582065105438[6,  3500] loss: 0.8906531660.8047959804535[6,  4000] loss: 0.8476791673.2699782848358[6,  4500] loss: 0.8843891685.9068989753723[6,  5000] loss: 0.7954561698.3758261203766[6,  5500] loss: 0.8180851710.912348985672[6,  6000] loss: 0.8328921723.510314464569[6,  6500] loss: 0.8473821736.2779424190521[6,  7000] loss: 0.8141851748.7942023277283[6,  7500] loss: 0.8571211761.3340632915497[6,  8000] loss: 0.8898731773.8619012832642[6,  8500] loss: 0.8452561786.3268177509308[6,  9000] loss: 0.8186421799.1319558620453[6,  9500] loss: 0.8344211811.82701587677[6, 10000] loss: 0.8132181824.423802614212[6, 10500] loss: 0.8106651836.9966804981232[6, 11000] loss: 0.7514011849.3608264923096[6, 11500] loss: 0.8120161861.685664176941[6, 12000] loss: 0.7772051874.2379496097565[6, 12500] loss: 0.8262101886.7937581539154Epoch [6] loss: 2664216.788976[7,   500] loss: 0.7597301899.7463347911835[7,  1000] loss: 0.7943161912.2143216133118[7,  1500] loss: 0.7512991924.586493730545[7,  2000] loss: 0.7935241937.2249510288239[7,  2500] loss: 0.7556241949.853317975998[7,  3000] loss: 0.7511751962.351987361908[7,  3500] loss: 0.7197331974.7705726623535[7,  4000] loss: 0.7793201987.345511674881[7,  4500] loss: 0.7305401999.8817665576935[7,  5000] loss: 0.7599712012.732185125351[7,  5500] loss: 0.7419312025.4165997505188[7,  6000] loss: 0.7963882037.9346585273743[7,  6500] loss: 0.7342452050.600024461746[7,  7000] loss: 0.7360562063.0711591243744[7,  7500] loss: 0.7277862075.5567195415497[7,  8000] loss: 0.7525492088.1287503242493[7,  8500] loss: 0.7651192100.7150399684906[7,  9000] loss: 0.7602512113.228790283203[7,  9500] loss: 0.7652892125.7857546806335[7, 10000] loss: 0.7651982138.7897963523865[7, 10500] loss: 0.7110672151.628352880478[7, 11000] loss: 0.7398462164.5602838993073[7, 11500] loss: 0.7049172177.108251810074[7, 12000] loss: 0.7261202189.7970221042633[7, 12500] loss: 0.7417962202.178448677063Epoch [7] loss: 2369362.777351[8,   500] loss: 0.6900762214.848338365555[8,  1000] loss: 0.6833832227.322183609009[8,  1500] loss: 0.6397762239.8956842422485[8,  2000] loss: 0.6706152252.266189813614[8,  2500] loss: 0.6445182264.818968296051[8,  3000] loss: 0.7024342277.347340106964[8,  3500] loss: 0.6496572290.0812463760376[8,  4000] loss: 0.6747342303.027858734131[8,  4500] loss: 0.6878882315.9543092250824[8,  5000] loss: 0.6408402328.7126727104187[8,  5500] loss: 0.6563552341.405957221985[8,  6000] loss: 0.6562722354.157285928726[8,  6500] loss: 0.6818752366.9738008975983[8,  7000] loss: 0.6195992380.2888271808624[8,  7500] loss: 0.6475722394.156877040863[8,  8000] loss: 0.6733522406.7158119678497[8,  8500] loss: 0.7128172419.90473818779[8,  9000] loss: 0.6786132432.4632654190063[8,  9500] loss: 0.7225492445.259570121765[8, 10000] loss: 0.6533312458.102192878723[8, 10500] loss: 0.6101022470.93386554718[8, 11000] loss: 0.6475852483.6762294769287[8, 11500] loss: 0.7050552496.4015171527863[8, 12000] loss: 0.6537822509.226389169693[8, 12500] loss: 0.6825002522.322765350342Epoch [8] loss: 2073871.639471[9,   500] loss: 0.6524282535.280851840973[9,  1000] loss: 0.5730672547.9333066940308[9,  1500] loss: 0.6053432560.962061405182[9,  2000] loss: 0.5847982573.8306081295013[9,  2500] loss: 0.5941512586.9529435634613[9,  3000] loss: 0.5765652600.3560087680817[9,  3500] loss: 0.6374422613.7585983276367[9,  4000] loss: 0.5596932627.1232550144196[9,  4500] loss: 0.6666322640.5986845493317[9,  5000] loss: 0.5981132654.1955251693726[9,  5500] loss: 0.5869962667.7001373767853[9,  6000] loss: 0.6195232680.6996920108795[9,  6500] loss: 0.6527642693.8987991809845[9,  7000] loss: 0.6363852706.854931116104[9,  7500] loss: 0.6509552719.4949111938477[9,  8000] loss: 0.6565862732.059233188629[9,  8500] loss: 0.5950812744.440196275711[9,  9000] loss: 0.6660952756.8036320209503[9,  9500] loss: 0.5892522769.806513786316[9, 10000] loss: 0.5897202782.6677951812744[9, 10500] loss: 0.6314632795.4347891807556[9, 11000] loss: 0.6216342807.9168043136597[9, 11500] loss: 0.5738142820.431797027588[9, 12000] loss: 0.6164812833.0658366680145[9, 12500] loss: 0.6438602845.5397782325745Epoch [9] loss: 1910311.861839[10,   500] loss: 0.5309622858.311792373657[10,  1000] loss: 0.6211412870.7949707508087[10,  1500] loss: 0.5971242883.250983476639[10,  2000] loss: 0.5771882895.773305416107[10,  2500] loss: 0.5963552908.334439754486[10,  3000] loss: 0.5368222920.86501789093[10,  3500] loss: 0.5619492933.386511325836[10,  4000] loss: 0.5350052945.835339784622[10,  4500] loss: 0.5539162958.296010494232[10,  5000] loss: 0.5498512970.7549664974213[10,  5500] loss: 0.5447742983.2242431640625[10,  6000] loss: 0.5346942995.688993215561[10,  6500] loss: 0.5264813008.2782728672028[10,  7000] loss: 0.5539733020.7367029190063[10,  7500] loss: 0.5795903033.2411811351776[10,  8000] loss: 0.5499843045.9475696086884[10,  8500] loss: 0.5645493058.47709107399[10,  9000] loss: 0.5554663071.4150307178497[10,  9500] loss: 0.5918453083.9714615345[10, 10000] loss: 0.6027473096.6826367378235[10, 10500] loss: 0.6295083109.279292345047[10, 11000] loss: 0.5686583122.211386203766[10, 11500] loss: 0.5910093134.6189522743225[10, 12000] loss: 0.5597923147.089661836624[10, 12500] loss: 0.5097483159.6398854255676Epoch [10] loss: 1773743.880755[11,   500] loss: 0.5032493172.2673337459564[11,  1000] loss: 0.4987563184.660989046097[11,  1500] loss: 0.5135593197.2276906967163[11,  2000] loss: 0.5108403209.8117713928223[11,  2500] loss: 0.5022623222.509393453598[11,  3000] loss: 0.4812033235.119441986084[11,  3500] loss: 0.5115393247.6265568733215[11,  4000] loss: 0.4806973260.2135832309723[11,  4500] loss: 0.5567453272.961349964142[11,  5000] loss: 0.4884703286.0478222370148[11,  5500] loss: 0.4772443299.0638630390167[11,  6000] loss: 0.5294863312.171390771866[11,  6500] loss: 0.4984913324.6505842208862[11,  7000] loss: 0.4758883337.1049423217773[11,  7500] loss: 0.4969883349.5630564689636[11,  8000] loss: 0.5056043361.8974261283875[11,  8500] loss: 0.4959263374.399149656296[11,  9000] loss: 0.5090183386.8016319274902[11,  9500] loss: 0.5097883399.184323787689[11, 10000] loss: 0.5039843411.575928211212[11, 10500] loss: 0.5249553423.8596017360687[11, 11000] loss: 0.5421913436.386314868927[11, 11500] loss: 0.4855423448.7578032016754[11, 12000] loss: 0.4943883461.109511613846[11, 12500] loss: 0.5377373473.5834662914276Epoch [11] loss: 1567368.557914[12,   500] loss: 0.4564443486.1529524326324[12,  1000] loss: 0.4435313498.7448127269745[12,  1500] loss: 0.4363043511.2897181510925[12,  2000] loss: 0.4079763523.7298724651337[12,  2500] loss: 0.4641453536.3546092510223[12,  3000] loss: 0.4160073549.0677461624146[12,  3500] loss: 0.4731483561.6509392261505[12,  4000] loss: 0.4670553573.9682166576385[12,  4500] loss: 0.4638583586.746418952942[12,  5000] loss: 0.4837773599.1981432437897[12,  5500] loss: 0.4964133611.5828087329865[12,  6000] loss: 0.4583033623.881804704666[12,  6500] loss: 0.4571893636.42679977417[12,  7000] loss: 0.4771673649.068778038025[12,  7500] loss: 0.4139443661.504314184189[12,  8000] loss: 0.4786773674.101729631424[12,  8500] loss: 0.4337073686.4739520549774[12,  9000] loss: 0.4881933698.867720603943[12,  9500] loss: 0.5317183711.646512746811[12, 10000] loss: 0.4567123724.123027563095[12, 10500] loss: 0.4675233737.3888108730316[12, 11000] loss: 0.4618543749.903846025467[12, 11500] loss: 0.4784993762.466922044754[12, 12000] loss: 0.4439503774.9109196662903[12, 12500] loss: 0.4471163787.297366142273Epoch [12] loss: 1444848.375264[13,   500] loss: 0.3769153799.9248321056366[13,  1000] loss: 0.3493003812.314027786255[13,  1500] loss: 0.3828813825.501012325287[13,  2000] loss: 0.3737423837.9095985889435[13,  2500] loss: 0.4379433850.4725699424744[13,  3000] loss: 0.4498083863.942113876343[13,  3500] loss: 0.3929173891.1621334552765[13,  4000] loss: 0.3920573904.906519174576[13,  4500] loss: 0.4415363917.5237827301025[13,  5000] loss: 0.3869443930.00816988945[13,  5500] loss: 0.4108423942.389120578766[13,  6000] loss: 0.4396753954.9102082252502[13,  6500] loss: 0.3851733967.5913710594177[13,  7000] loss: 0.3883023980.4952867031097[13,  7500] loss: 0.3750733992.9306819438934[13,  8000] loss: 0.3932844005.8926594257355[13,  8500] loss: 0.4130594018.5611629486084[13,  9000] loss: 0.4382224031.322649240494[13,  9500] loss: 0.4035204043.98210477829[13, 10000] loss: 0.4152194056.718078136444[13, 10500] loss: 0.4483774069.184317588806[13, 11000] loss: 0.4349764081.8509600162506[13, 11500] loss: 0.4979264094.542535305023[13, 12000] loss: 0.4296564107.079015016556[13, 12500] loss: 0.4555034119.73534321785Epoch [13] loss: 1291735.918350[14,   500] loss: 0.3324984134.291393518448[14,  1000] loss: 0.3380324146.985105752945[14,  1500] loss: 0.3344874159.696142911911[14,  2000] loss: 0.3671304172.155971050262[14,  2500] loss: 0.3831794184.735641956329[14,  3000] loss: 0.3392374197.292338132858[14,  3500] loss: 0.3899084209.8273849487305[14,  4000] loss: 0.3757424222.307894945145[14,  4500] loss: 0.3805884234.881804943085[14,  5000] loss: 0.3855344247.438624620438[14,  5500] loss: 0.3241704259.81192111969[14,  6000] loss: 0.3520094272.282189369202[14,  6500] loss: 0.3703564284.70879650116[14,  7000] loss: 0.3884504297.291787385941[14,  7500] loss: 0.3604774309.834355354309[14,  8000] loss: 0.4196384322.227069616318[14,  8500] loss: 0.3694964334.702982902527[14,  9000] loss: 0.3758444347.106511116028[14,  9500] loss: 0.3732524359.6162321567535[14, 10000] loss: 0.3980114372.186988115311[14, 10500] loss: 0.3913844384.6649260520935[14, 11000] loss: 0.4078524397.451583623886[14, 11500] loss: 0.4266334410.167633295059[14, 12000] loss: 0.3748274423.048738956451[14, 12500] loss: 0.4274554435.788289785385Epoch [14] loss: 1173917.154873[15,   500] loss: 0.3521074448.752544403076[15,  1000] loss: 0.3107714461.401283979416[15,  1500] loss: 0.3508784474.096784114838[15,  2000] loss: 0.2903734486.794206619263[15,  2500] loss: 0.3687644499.589939594269[15,  3000] loss: 0.3367974512.391986846924[15,  3500] loss: 0.3132504524.909478902817[15,  4000] loss: 0.3053824537.5362412929535[15,  4500] loss: 0.3351374550.296581745148[15,  5000] loss: 0.3973124563.038723945618[15,  5500] loss: 0.3161334575.665570497513[15,  6000] loss: 0.3242744588.641082048416[15,  6500] loss: 0.3131414601.326877593994[15,  7000] loss: 0.3396854614.01914525032[15,  7500] loss: 0.3954384626.788290739059[15,  8000] loss: 0.3831774639.635489225388[15,  8500] loss: 0.3823254652.31867313385[15,  9000] loss: 0.3817074664.6876056194305[15,  9500] loss: 0.3338264677.64083647728[15, 10000] loss: 0.3465414690.46112036705[15, 10500] loss: 0.3633204703.224801540375[15, 11000] loss: 0.3693114715.671589374542[15, 11500] loss: 0.3935164728.317858457565[15, 12000] loss: 0.3418414740.84798002243[15, 12500] loss: 0.3697094753.376964569092Epoch [15] loss: 1086851.910069[16,   500] loss: 0.2587124766.097062110901[16,  1000] loss: 0.2878414778.67579293251[16,  1500] loss: 0.2536934791.419860363007[16,  2000] loss: 0.3259924804.209983587265[16,  2500] loss: 0.3049464816.968195676804[16,  3000] loss: 0.2750674829.278405427933[16,  3500] loss: 0.2811014841.723084688187[16,  4000] loss: 0.2648634854.134469747543[16,  4500] loss: 0.3011704866.6174194812775[16,  5000] loss: 0.2819094879.149275302887[16,  5500] loss: 0.3128354891.814346313477[16,  6000] loss: 0.3347184904.704942941666[16,  6500] loss: 0.3045424917.388124227524[16,  7000] loss: 0.3343604930.330537557602[16,  7500] loss: 0.3151844943.340002775192[16,  8000] loss: 0.3391624956.327854633331[16,  8500] loss: 0.3106174969.417783975601[16,  9000] loss: 0.2718984982.49702334404[16,  9500] loss: 0.3187754995.635529279709[16, 10000] loss: 0.3115905008.725402832031[16, 10500] loss: 0.2823065021.85075211525[16, 11000] loss: 0.2768775035.402277469635[16, 11500] loss: 0.3562765048.666571378708[16, 12000] loss: 0.3659265062.122591972351[16, 12500] loss: 0.3202335075.384432792664Epoch [16] loss: 948918.226936[17,   500] loss: 0.2961065088.450833559036[17,  1000] loss: 0.2928225101.666797637939[17,  1500] loss: 0.2829025114.696937084198[17,  2000] loss: 0.2439415127.551694154739[17,  2500] loss: 0.2926875140.612481832504[17,  3000] loss: 0.3200425153.720325708389[17,  3500] loss: 0.2796915166.418760299683[17,  4000] loss: 0.2592795179.2575805187225[17,  4500] loss: 0.2696905192.404355049133[17,  5000] loss: 0.2501885205.354030847549[17,  5500] loss: 0.2907165218.496971607208[17,  6000] loss: 0.2983895232.021593093872[17,  6500] loss: 0.2995735245.765065670013[17,  7000] loss: 0.2754395259.286817073822[17,  7500] loss: 0.2728125272.711032152176[17,  8000] loss: 0.3396305285.875935792923[17,  8500] loss: 0.2550445299.233667373657[17,  9000] loss: 0.2777035312.70419216156[17,  9500] loss: 0.3131085326.015132427216[17, 10000] loss: 0.2906305340.323965787888[17, 10500] loss: 0.3469165353.521278858185[17, 11000] loss: 0.2796035366.615474224091[17, 11500] loss: 0.3172145379.962978601456[17, 12000] loss: 0.2693255393.353959798813[17, 12500] loss: 0.3145215406.58737039566Epoch [17] loss: 902654.100164[18,   500] loss: 0.2712675420.152381658554[18,  1000] loss: 0.2489065433.402588844299[18,  1500] loss: 0.2254425447.016442060471[18,  2000] loss: 0.2583155460.279866695404[18,  2500] loss: 0.2953465473.041458845139[18,  3000] loss: 0.2632495485.736753463745[18,  3500] loss: 0.2760875498.431475400925[18,  4000] loss: 0.2299255511.935250759125[18,  4500] loss: 0.2660845525.176432609558[18,  5000] loss: 0.2573485538.487706184387[18,  5500] loss: 0.2653025551.535981893539[18,  6000] loss: 0.2518565564.514166593552[18,  6500] loss: 0.2735695577.595376729965[18,  7000] loss: 0.2675555591.001711845398[18,  7500] loss: 0.2626165604.4123067855835[18,  8000] loss: 0.2907475617.483761787415[18,  8500] loss: 0.3388555630.646730422974[18,  9000] loss: 0.2757055643.54322886467[18,  9500] loss: 0.2673845657.1483290195465[18, 10000] loss: 0.2449005670.988806247711[18, 10500] loss: 0.2652145684.549884557724[18, 11000] loss: 0.2462585698.127720594406[18, 11500] loss: 0.2899165711.667749166489[18, 12000] loss: 0.3275215724.6041741371155[18, 12500] loss: 0.2940255737.436963558197Epoch [18] loss: 836066.101700[19,   500] loss: 0.2038175750.465784788132[19,  1000] loss: 0.2010215763.130273580551[19,  1500] loss: 0.1911765775.886811494827[19,  2000] loss: 0.2024745788.659563302994[19,  2500] loss: 0.2252055801.333149433136[19,  3000] loss: 0.2109335814.780102014542[19,  3500] loss: 0.2533215827.6136865615845[19,  4000] loss: 0.2167665840.472111940384[19,  4500] loss: 0.2262415853.168671131134[19,  5000] loss: 0.2565895865.866081476212[19,  5500] loss: 0.2114565878.625334978104[19,  6000] loss: 0.2113545891.436779975891[19,  6500] loss: 0.2404495904.332948684692[19,  7000] loss: 0.2704195917.026649713516[19,  7500] loss: 0.2411095929.625186443329[19,  8000] loss: 0.2776405942.458882570267[19,  8500] loss: 0.2695235955.269655227661[19,  9000] loss: 0.2475775968.319126844406[19,  9500] loss: 0.2824995981.225419044495[19, 10000] loss: 0.2523425993.973672628403[19, 10500] loss: 0.2608926006.54151725769[19, 11000] loss: 0.2799306019.464883804321[19, 11500] loss: 0.2685056032.864200592041[19, 12000] loss: 0.2775196045.441250801086[19, 12500] loss: 0.2715126057.93755364418Epoch [19] loss: 757634.977217[20,   500] loss: 0.2196596070.547635316849[20,  1000] loss: 0.2199006083.086601018906[20,  1500] loss: 0.1833476095.669717550278[20,  2000] loss: 0.1970566108.130441904068[20,  2500] loss: 0.1885076120.684714078903[20,  3000] loss: 0.1709486133.266049623489[20,  3500] loss: 0.2055296145.786177396774[20,  4000] loss: 0.2315066158.240549564362[20,  4500] loss: 0.2334596170.7461705207825[20,  5000] loss: 0.2278376183.628883838654[20,  5500] loss: 0.2480786196.378153324127[20,  6000] loss: 0.2174866208.9837329387665[20,  6500] loss: 0.2075756221.575855970383[20,  7000] loss: 0.2587456234.121645450592[20,  7500] loss: 0.2631976246.57945227623[20,  8000] loss: 0.2267896259.111184835434[20,  8500] loss: 0.2034526271.574547767639[20,  9000] loss: 0.2494996284.131222248077[20,  9500] loss: 0.2100326296.535153627396[20, 10000] loss: 0.1899126309.1373908519745[20, 10500] loss: 0.2269116321.45804977417[20, 11000] loss: 0.2625496333.944760799408[20, 11500] loss: 0.2412926346.43473815918[20, 12000] loss: 0.2269596358.982520103455[20, 12500] loss: 0.2269136371.448928594589Epoch [20] loss: 695595.363967Finished TrainingSaving model to /data/s4091221/trained-models/wide_resnet50_22020-02-25 00:41:02.929232GroundTruth:    cat  ship  ship planeSending data to GPUSending model to GPUtensor([[ -5.6965,  -1.7709,   5.7751,  30.9272,   1.0179,  17.3789,   3.1467,         -22.7788,  -4.5050, -16.3964],        [  1.9316,   0.5926,   0.2378,   0.2711,  -0.5240,  -0.5961,  -2.2996,           0.2695,   1.5142,  -0.7191],        [  3.5264,   2.6915,  -0.1756,  -2.1555,   0.0813,  -2.5298,  -2.3740,           0.0829,   4.0456,  -0.5096],        [  3.4081,  -2.1626,   0.3564,   0.7808,   1.3316,   0.1887,  -1.7726,          -0.6158,   0.3225,  -1.1889]], device='cuda:0',       grad_fn=<AddmmBackward>)Predicted:    cat plane  ship planeAccuracy of the network on the 4000.0 test images: 66 %###############################################################################Peregrine ClusterJob 9730760 for user 's4091221'Finished at: Tue Feb 25 00:41:46 CET 2020Job details:============Name                : wide_resnet50.shUser                : s4091221Partition           : gpuNodes               : pg-gpu36Cores               : 12State               : COMPLETEDSubmit              : 2020-02-24T18:00:45Start               : 2020-02-24T21:07:32End                 : 2020-02-25T00:41:46Reserved walltime   : 15:00:00Used walltime       : 03:34:14Used CPU time       : 03:45:55 (efficiency:  8.79%)% User (Computation): 95.96%% System (I/O)      :  4.04%Mem reserved        : 12000M/nodeMax Mem used        : 3.00G (pg-gpu36)Max Disk Write      : 440.33M (pg-gpu36)Max Disk Read       : 1.06G (pg-gpu36)Acknowledgements:=================Please see this page for information about acknowledging Peregrine in your publications:https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output################################################################################