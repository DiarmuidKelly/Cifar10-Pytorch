Requirement already satisfied: torchvision in ./.local/lib/python3.7/site-packages (0.5.0)
Requirement already satisfied: pillow>=4.1.1 in ./.local/lib/python3.7/site-packages (from torchvision) (7.0.0)
Requirement already satisfied: numpy in ./.local/lib/python3.7/site-packages (from torchvision) (1.18.1)
Requirement already satisfied: six in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from torchvision) (1.12.0)
Requirement already satisfied: torch==1.4.0 in ./.local/lib/python3.7/site-packages (from torchvision) (1.4.0)
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'batch_size': 4, 'workers': 2, 'model_archi': 14, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=14, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
horse  frog  frog   cat
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet18 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
Model resnet18 Reshaped
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582586230.3984945
[1,   500] loss: 2.272791
6.811220169067383
[1,  1000] loss: 2.183943
11.644244194030762
[1,  1500] loss: 2.142109
16.42706823348999
[1,  2000] loss: 2.081610
21.272330284118652
[1,  2500] loss: 2.034201
26.129472494125366
[1,  3000] loss: 2.015210
30.916146993637085
[1,  3500] loss: 1.996351
35.7059543132782
[1,  4000] loss: 1.982607
40.50137710571289
[1,  4500] loss: 1.924104
45.35082268714905
[1,  5000] loss: 1.951119
50.1354718208313
[1,  5500] loss: 1.914679
54.90459370613098
[1,  6000] loss: 1.906932
59.70677733421326
[1,  6500] loss: 1.873507
64.50706481933594
[1,  7000] loss: 1.892782
69.27164959907532
[1,  7500] loss: 1.874682
74.04889154434204
[1,  8000] loss: 1.816461
78.76703810691833
[1,  8500] loss: 1.833454
83.540687084198
[1,  9000] loss: 1.838440
88.33214259147644
[1,  9500] loss: 1.832246
93.14944219589233
[1, 10000] loss: 1.780831
97.98780679702759
[1, 10500] loss: 1.764003
102.81472754478455
[1, 11000] loss: 1.786544
107.54910254478455
[1, 11500] loss: 1.779835
112.39144682884216
[1, 12000] loss: 1.811136
117.26284050941467
[1, 12500] loss: 1.743452
121.9946928024292
Epoch [1] loss: 6025767.059344
[2,   500] loss: 1.768081
126.87779760360718
[2,  1000] loss: 1.770354
131.68079829216003
[2,  1500] loss: 1.738567
136.47212481498718
[2,  2000] loss: 1.745530
141.26235675811768
[2,  2500] loss: 1.733065
146.16699266433716
[2,  3000] loss: 1.746109
151.07337856292725
[2,  3500] loss: 1.711198
155.86032223701477
[2,  4000] loss: 1.686776
160.73149991035461
[2,  4500] loss: 1.640230
165.6383044719696
[2,  5000] loss: 1.718696
170.53551745414734
[2,  5500] loss: 1.683420
175.38227677345276
[2,  6000] loss: 1.678306
180.3206911087036
[2,  6500] loss: 1.667952
185.21486854553223
[2,  7000] loss: 1.673385
190.1332266330719
[2,  7500] loss: 1.664051
194.91876435279846
[2,  8000] loss: 1.615969
199.6752598285675
[2,  8500] loss: 1.605125
204.47589755058289
[2,  9000] loss: 1.619780
209.27709817886353
[2,  9500] loss: 1.593279
214.07835149765015
[2, 10000] loss: 1.624113
218.9040629863739
[2, 10500] loss: 1.612781
223.74362087249756
[2, 11000] loss: 1.592965
228.54473233222961
[2, 11500] loss: 1.582287
233.34776329994202
[2, 12000] loss: 1.592283
238.13713788986206
[2, 12500] loss: 1.593420
242.88872289657593
Epoch [2] loss: 5218508.653757
[3,   500] loss: 1.542866
247.82204747200012
[3,  1000] loss: 1.590679
252.55858325958252
[3,  1500] loss: 1.601733
257.4263689517975
[3,  2000] loss: 1.515902
262.2032980918884
[3,  2500] loss: 1.571516
266.96987199783325
[3,  3000] loss: 1.555307
271.766921043396
[3,  3500] loss: 1.501900
276.54671001434326
[3,  4000] loss: 1.535677
281.3796458244324
[3,  4500] loss: 1.549854
286.18262243270874
[3,  5000] loss: 1.528313
291.01609802246094
[3,  5500] loss: 1.516364
295.88743138313293
[3,  6000] loss: 1.486454
300.7166931629181
[3,  6500] loss: 1.536363
305.4826090335846
[3,  7000] loss: 1.540348
310.2206299304962
[3,  7500] loss: 1.547771
314.94202041625977
[3,  8000] loss: 1.500432
319.74870133399963
[3,  8500] loss: 1.496144
324.5459957122803
[3,  9000] loss: 1.553498
329.4059135913849
[3,  9500] loss: 1.450603
334.19295716285706
[3, 10000] loss: 1.463908
338.9764187335968
[3, 10500] loss: 1.481853
343.7970836162567
[3, 11000] loss: 1.487887
348.5993072986603
[3, 11500] loss: 1.487235
353.4116246700287
[3, 12000] loss: 1.461395
358.19879364967346
[3, 12500] loss: 1.473149
363.00416350364685
Epoch [3] loss: 4772405.898934
[4,   500] loss: 1.392542
367.9237937927246
[4,  1000] loss: 1.473682
372.7722680568695
[4,  1500] loss: 1.459785
377.6204252243042
[4,  2000] loss: 1.445745
382.508709192276
[4,  2500] loss: 1.398153
387.3539891242981
[4,  3000] loss: 1.368247
392.25531673431396
[4,  3500] loss: 1.383503
397.1211521625519
[4,  4000] loss: 1.397837
401.922810792923
[4,  4500] loss: 1.388347
406.6670169830322
[4,  5000] loss: 1.409614
411.52669048309326
[4,  5500] loss: 1.378815
416.3913559913635
[4,  6000] loss: 1.378652
421.1567666530609
[4,  6500] loss: 1.393865
426.01213669776917
[4,  7000] loss: 1.398972
430.796906709671
[4,  7500] loss: 1.421907
435.6008131504059
[4,  8000] loss: 1.414792
440.45426988601685
[4,  8500] loss: 1.413555
445.2154185771942
[4,  9000] loss: 1.350656
449.98657178878784
[4,  9500] loss: 1.393983
454.74099802970886
[4, 10000] loss: 1.347547
459.54295086860657
[4, 10500] loss: 1.331279
464.2824249267578
[4, 11000] loss: 1.375736
469.1061990261078
[4, 11500] loss: 1.374679
473.8524179458618
[4, 12000] loss: 1.338415
478.6476876735687
[4, 12500] loss: 1.357855
483.4374051094055
Epoch [4] loss: 4358762.987589
[5,   500] loss: 1.331316
488.3370535373688
[5,  1000] loss: 1.326926
493.0690665245056
[5,  1500] loss: 1.293133
497.79617834091187
[5,  2000] loss: 1.302327
502.5572726726532
[5,  2500] loss: 1.341909
507.39809703826904
[5,  3000] loss: 1.351305
512.2936236858368
[5,  3500] loss: 1.314872
517.1552004814148
[5,  4000] loss: 1.275741
521.9496574401855
[5,  4500] loss: 1.312207
526.7791743278503
[5,  5000] loss: 1.275523
531.6668808460236
[5,  5500] loss: 1.295817
536.5141844749451
[5,  6000] loss: 1.284006
541.3421545028687
[5,  6500] loss: 1.299452
546.1789395809174
[5,  7000] loss: 1.259522
550.9558916091919
[5,  7500] loss: 1.288497
555.7530148029327
[5,  8000] loss: 1.294082
560.524510383606
[5,  8500] loss: 1.272886
565.2561259269714
[5,  9000] loss: 1.318280
570.0832834243774
[5,  9500] loss: 1.265598
575.0151975154877
[5, 10000] loss: 1.295503
579.8783509731293
[5, 10500] loss: 1.276143
584.7260982990265
[5, 11000] loss: 1.275313
589.5971803665161
[5, 11500] loss: 1.297943
594.4615616798401
[5, 12000] loss: 1.239987
599.3296468257904
[5, 12500] loss: 1.252567
604.1485455036163
Epoch [5] loss: 4066291.063141
[6,   500] loss: 1.260786
609.040548324585
[6,  1000] loss: 1.221236
613.8543255329132
[6,  1500] loss: 1.210558
618.6520311832428
[6,  2000] loss: 1.198353
623.5343689918518
[6,  2500] loss: 1.241794
628.3633804321289
[6,  3000] loss: 1.185428
633.1734864711761
[6,  3500] loss: 1.203356
637.9988956451416
[6,  4000] loss: 1.226924
642.7728033065796
[6,  4500] loss: 1.167859
647.4959499835968
[6,  5000] loss: 1.195717
652.2574751377106
[6,  5500] loss: 1.269492
657.011953830719
[6,  6000] loss: 1.226838
661.8224110603333
[6,  6500] loss: 1.241154
666.596328496933
[6,  7000] loss: 1.168071
671.3639883995056
[6,  7500] loss: 1.192826
676.1995487213135
[6,  8000] loss: 1.198428
680.9723241329193
[6,  8500] loss: 1.243396
685.8162572383881
[6,  9000] loss: 1.205489
690.6314480304718
[6,  9500] loss: 1.203927
695.4424695968628
[6, 10000] loss: 1.213135
700.3164415359497
[6, 10500] loss: 1.246191
705.1401929855347
[6, 11000] loss: 1.187025
710.0082058906555
[6, 11500] loss: 1.177493
714.8613245487213
[6, 12000] loss: 1.167600
719.6658120155334
[6, 12500] loss: 1.162515
724.5400061607361
Epoch [6] loss: 3794522.990027
[7,   500] loss: 1.108515
729.4948496818542
[7,  1000] loss: 1.164604
734.3431615829468
[7,  1500] loss: 1.178059
739.2463023662567
[7,  2000] loss: 1.164713
744.0842020511627
[7,  2500] loss: 1.130606
748.8925352096558
[7,  3000] loss: 1.134308
753.7813112735748
[7,  3500] loss: 1.181295
758.582581281662
[7,  4000] loss: 1.166081
763.4222116470337
[7,  4500] loss: 1.141209
768.2117834091187
[7,  5000] loss: 1.182952
773.0788052082062
[7,  5500] loss: 1.181932
777.9312660694122
[7,  6000] loss: 1.117964
782.8651247024536
[7,  6500] loss: 1.103255
787.7230660915375
[7,  7000] loss: 1.112488
792.6498467922211
[7,  7500] loss: 1.110297
797.4584062099457
[7,  8000] loss: 1.104733
802.3131632804871
[7,  8500] loss: 1.091983
807.1390788555145
[7,  9000] loss: 1.162326
811.9489848613739
[7,  9500] loss: 1.153462
816.7829711437225
[7, 10000] loss: 1.170815
821.590413570404
[7, 10500] loss: 1.068158
826.3717896938324
[7, 11000] loss: 1.135283
831.2444067001343
[7, 11500] loss: 1.140821
836.1494989395142
[7, 12000] loss: 1.145803
840.9624271392822
[7, 12500] loss: 1.121374
845.8271629810333
Epoch [7] loss: 3566047.457784
[8,   500] loss: 1.078548
850.7843582630157
[8,  1000] loss: 1.076428
855.5977199077606
[8,  1500] loss: 1.093064
860.4892919063568
[8,  2000] loss: 1.095171
865.4027581214905
[8,  2500] loss: 1.106870
870.2475504875183
[8,  3000] loss: 1.112594
875.1425428390503
[8,  3500] loss: 1.088808
879.960390329361
[8,  4000] loss: 1.103906
884.8100073337555
[8,  4500] loss: 1.075867
889.5984544754028
[8,  5000] loss: 1.058462
894.417311668396
[8,  5500] loss: 1.083780
899.2888391017914
[8,  6000] loss: 1.055771
904.0676517486572
[8,  6500] loss: 1.055112
908.8962302207947
[8,  7000] loss: 1.069988
913.7565221786499
[8,  7500] loss: 1.071049
918.5770635604858
[8,  8000] loss: 1.069492
923.3848679065704
[8,  8500] loss: 1.075154
928.1726574897766
[8,  9000] loss: 1.052713
933.0277755260468
[8,  9500] loss: 1.106906
937.8320415019989
[8, 10000] loss: 1.048279
942.6568379402161
[8, 10500] loss: 1.052912
947.4961047172546
[8, 11000] loss: 1.075501
952.3443841934204
[8, 11500] loss: 1.089482
957.393797159195
[8, 12000] loss: 1.057278
962.2892010211945
[8, 12500] loss: 1.012570
967.1048080921173
Epoch [8] loss: 3365489.106044
[9,   500] loss: 1.045412
972.0851011276245
[9,  1000] loss: 1.007562
976.8808009624481
[9,  1500] loss: 1.025934
981.7485482692719
[9,  2000] loss: 1.009312
986.5844807624817
[9,  2500] loss: 1.026265
991.4604685306549
[9,  3000] loss: 0.994443
996.2532680034637
[9,  3500] loss: 0.997445
1001.0661079883575
[9,  4000] loss: 1.023752
1005.8703999519348
[9,  4500] loss: 1.023358
1010.7099769115448
[9,  5000] loss: 1.061018
1015.5989582538605
[9,  5500] loss: 1.020633
1020.4205610752106
[9,  6000] loss: 1.023498
1025.3084464073181
[9,  6500] loss: 1.015636
1030.149252653122
[9,  7000] loss: 1.029292
1034.941812992096
[9,  7500] loss: 1.082148
1039.771045923233
[9,  8000] loss: 1.034114
1044.5773100852966
[9,  8500] loss: 1.018778
1049.458862066269
[9,  9000] loss: 1.009649
1054.2982437610626
[9,  9500] loss: 0.967419
1059.1957716941833
[9, 10000] loss: 1.008817
1063.9954001903534
[9, 10500] loss: 1.020492
1068.834858417511
[9, 11000] loss: 1.006310
1073.737288236618
[9, 11500] loss: 1.020381
1078.5776994228363
[9, 12000] loss: 1.043788
1083.4388449192047
[9, 12500] loss: 0.969380
1088.326740026474
Epoch [9] loss: 3197320.132144
[10,   500] loss: 0.961304
1093.2576892375946
[10,  1000] loss: 0.995233
1098.071623802185
[10,  1500] loss: 0.959200
1102.905333518982
[10,  2000] loss: 0.966721
1107.7615430355072
[10,  2500] loss: 0.968003
1112.5993010997772
[10,  3000] loss: 0.948350
1117.465129852295
[10,  3500] loss: 0.994009
1122.2889151573181
[10,  4000] loss: 0.922611
1127.1503212451935
[10,  4500] loss: 0.956838
1131.9693396091461
[10,  5000] loss: 0.956435
1136.7948560714722
[10,  5500] loss: 1.015543
1141.6297914981842
[10,  6000] loss: 0.959874
1146.4813208580017
[10,  6500] loss: 0.969574
1151.3300607204437
[10,  7000] loss: 0.956886
1156.198316335678
[10,  7500] loss: 0.968470
1161.0385727882385
[10,  8000] loss: 0.972935
1165.7830002307892
[10,  8500] loss: 0.995523
1170.5348715782166
[10,  9000] loss: 0.985100
1175.3300149440765
[10,  9500] loss: 0.933675
1180.1102788448334
[10, 10000] loss: 0.942615
1184.8893072605133
[10, 10500] loss: 0.971345
1189.6991391181946
[10, 11000] loss: 0.956670
1194.596557378769
[10, 11500] loss: 0.951110
1199.4254837036133
[10, 12000] loss: 0.959601
1204.2216420173645
[10, 12500] loss: 0.962336
1209.0008759498596
Epoch [10] loss: 3003953.674962
[11,   500] loss: 0.892899
1213.90687251091
[11,  1000] loss: 0.965694
1218.7821943759918
[11,  1500] loss: 0.899405
1223.6257395744324
[11,  2000] loss: 0.911261
1228.5398614406586
[11,  2500] loss: 0.971073
1233.4929563999176
[11,  3000] loss: 0.929504
1238.2542293071747
[11,  3500] loss: 0.920880
1243.0595450401306
[11,  4000] loss: 0.942773
1247.954106092453
[11,  4500] loss: 0.923723
1252.8585486412048
[11,  5000] loss: 0.876321
1257.6789696216583
[11,  5500] loss: 0.906628
1262.4865508079529
[11,  6000] loss: 0.962661
1267.3255155086517
[11,  6500] loss: 0.880132
1272.132202386856
[11,  7000] loss: 0.916230
1277.0195546150208
[11,  7500] loss: 0.888657
1281.9215881824493
[11,  8000] loss: 0.878635
1286.7172844409943
[11,  8500] loss: 0.959301
1291.466880083084
[11,  9000] loss: 0.926410
1296.2051355838776
[11,  9500] loss: 0.919973
1300.997866153717
[11, 10000] loss: 0.963064
1305.8244833946228
[11, 10500] loss: 0.885753
1310.6662561893463
[11, 11000] loss: 0.892465
1315.416084766388
[11, 11500] loss: 0.935266
1320.2104074954987
[11, 12000] loss: 0.957460
1325.0617594718933
[11, 12500] loss: 0.920198
1329.8969237804413
Epoch [11] loss: 2884113.576120
[12,   500] loss: 0.836231
1334.88583445549
[12,  1000] loss: 0.861464
1339.763132095337
[12,  1500] loss: 0.876730
1344.6012768745422
[12,  2000] loss: 0.864925
1349.394782781601
[12,  2500] loss: 0.888599
1354.1953904628754
[12,  3000] loss: 0.864386
1359.0189945697784
[12,  3500] loss: 0.862176
1363.8091654777527
[12,  4000] loss: 0.864938
1368.6025817394257
[12,  4500] loss: 0.819674
1373.4148921966553
[12,  5000] loss: 0.912219
1378.1878004074097
[12,  5500] loss: 0.867804
1383.0015316009521
[12,  6000] loss: 0.889946
1387.807944059372
[12,  6500] loss: 0.866530
1392.6698606014252
[12,  7000] loss: 0.901971
1397.521550655365
[12,  7500] loss: 0.856294
1402.3915390968323
[12,  8000] loss: 0.859114
1407.2592613697052
[12,  8500] loss: 0.895465
1412.086436033249
[12,  9000] loss: 0.923742
1416.8755593299866
[12,  9500] loss: 0.885933
1421.6826298236847
[12, 10000] loss: 0.845621
1426.4563360214233
[12, 10500] loss: 0.920506
1431.2994601726532
[12, 11000] loss: 0.927547
1436.1198494434357
[12, 11500] loss: 0.919092
1440.9343028068542
[12, 12000] loss: 0.888782
1445.754637002945
[12, 12500] loss: 0.860513
1450.592232465744
Epoch [12] loss: 2749804.515735
[13,   500] loss: 0.834279
1455.464607000351
[13,  1000] loss: 0.840017
1460.2578556537628
[13,  1500] loss: 0.840666
1465.0464282035828
[13,  2000] loss: 0.890408
1469.9611012935638
[13,  2500] loss: 0.880433
1474.8338024616241
[13,  3000] loss: 0.827551
1479.6361854076385
[13,  3500] loss: 0.825439
1484.4293010234833
[13,  4000] loss: 0.848228
1489.2100059986115
[13,  4500] loss: 0.776006
1493.9819836616516
[13,  5000] loss: 0.786710
1498.814860343933
[13,  5500] loss: 0.844313
1503.6554753780365
[13,  6000] loss: 0.805530
1508.5676753520966
[13,  6500] loss: 0.835494
1513.4485471248627
[13,  7000] loss: 0.841241
1518.2356843948364
[13,  7500] loss: 0.858316
1523.1531431674957
[13,  8000] loss: 0.850523
1528.045122385025
[13,  8500] loss: 0.859948
1532.939837217331
[13,  9000] loss: 0.838693
1537.8397843837738
[13,  9500] loss: 0.846199
1542.7308123111725
[13, 10000] loss: 0.831986
1547.5882167816162
[13, 10500] loss: 0.838149
1552.4835622310638
[13, 11000] loss: 0.834184
1557.3209097385406
[13, 11500] loss: 0.810895
1562.1429510116577
[13, 12000] loss: 0.809031
1566.945924282074
[13, 12500] loss: 0.846905
1571.680311203003
Epoch [13] loss: 2602805.451836
[14,   500] loss: 0.821173
1576.5275497436523
[14,  1000] loss: 0.809096
1581.2735104560852
[14,  1500] loss: 0.768833
1586.0564541816711
[14,  2000] loss: 0.813846
1590.885678768158
[14,  2500] loss: 0.822641
1595.693104982376
[14,  3000] loss: 0.788042
1600.4486813545227
[14,  3500] loss: 0.779546
1605.2519028186798
[14,  4000] loss: 0.740421
1610.0812628269196
[14,  4500] loss: 0.779142
1614.8750355243683
[14,  5000] loss: 0.850514
1619.6717641353607
[14,  5500] loss: 0.785614
1624.496698141098
[14,  6000] loss: 0.825398
1629.3108758926392
[14,  6500] loss: 0.809061
1634.1410417556763
[14,  7000] loss: 0.810528
1638.9570758342743
[14,  7500] loss: 0.822928
1643.782618522644
[14,  8000] loss: 0.814863
1648.6173400878906
[14,  8500] loss: 0.832814
1653.3783423900604
[14,  9000] loss: 0.767407
1658.1577439308167
[14,  9500] loss: 0.848217
1662.9745903015137
[14, 10000] loss: 0.820585
1667.7560732364655
[14, 10500] loss: 0.836916
1672.6192021369934
[14, 11000] loss: 0.806224
1677.3997581005096
[14, 11500] loss: 0.833206
1682.2531805038452
[14, 12000] loss: 0.757655
1687.0959980487823
[14, 12500] loss: 0.796785
1691.9420607089996
Epoch [14] loss: 2520153.630045
[15,   500] loss: 0.763430
1696.8803803920746
[15,  1000] loss: 0.724843
1701.7506303787231
[15,  1500] loss: 0.760424
1706.515226840973
[15,  2000] loss: 0.731472
1711.332236289978
[15,  2500] loss: 0.783379
1716.1465108394623
[15,  3000] loss: 0.725469
1720.9064304828644
[15,  3500] loss: 0.731208
1725.686163187027
[15,  4000] loss: 0.774748
1730.5149919986725
[15,  4500] loss: 0.764439
1735.3119254112244
[15,  5000] loss: 0.744992
1740.111366033554
[15,  5500] loss: 0.796021
1744.9530928134918
[15,  6000] loss: 0.765915
1749.7288780212402
[15,  6500] loss: 0.809754
1754.47141289711
[15,  7000] loss: 0.814656
1759.2076289653778
[15,  7500] loss: 0.771630
1763.9370512962341
[15,  8000] loss: 0.800083
1768.6977214813232
[15,  8500] loss: 0.760190
1773.4714126586914
[15,  9000] loss: 0.754075
1778.278608083725
[15,  9500] loss: 0.804483
1783.081224679947
[15, 10000] loss: 0.778049
1787.7938895225525
[15, 10500] loss: 0.757204
1792.5404481887817
[15, 11000] loss: 0.770893
1797.3421158790588
[15, 11500] loss: 0.795237
1802.146347284317
[15, 12000] loss: 0.750502
1806.946783065796
[15, 12500] loss: 0.787203
1811.7421798706055
Epoch [15] loss: 2420696.191945
[16,   500] loss: 0.696307
1816.9599244594574
[16,  1000] loss: 0.714474
1821.7687411308289
[16,  1500] loss: 0.771376
1826.616374015808
[16,  2000] loss: 0.745500
1831.3644425868988
[16,  2500] loss: 0.719133
1836.0724127292633
[16,  3000] loss: 0.729692
1840.797343492508
[16,  3500] loss: 0.741630
1845.5402390956879
[16,  4000] loss: 0.745454
1850.3221683502197
[16,  4500] loss: 0.702949
1855.1466405391693
[16,  5000] loss: 0.726294
1860.003428697586
[16,  5500] loss: 0.760747
1864.7839426994324
[16,  6000] loss: 0.736859
1869.58247423172
[16,  6500] loss: 0.727963
1874.393082857132
[16,  7000] loss: 0.723347
1879.1805334091187
[16,  7500] loss: 0.713708
1883.9971866607666
[16,  8000] loss: 0.737765
1888.7954347133636
[16,  8500] loss: 0.742211
1893.6113097667694
[16,  9000] loss: 0.723109
1898.4190421104431
[16,  9500] loss: 0.793793
1903.2457249164581
[16, 10000] loss: 0.670724
1908.0594625473022
[16, 10500] loss: 0.780395
1912.8345754146576
[16, 11000] loss: 0.764541
1917.6800045967102
[16, 11500] loss: 0.762730
1922.517025232315
[16, 12000] loss: 0.753140
1927.3097038269043
[16, 12500] loss: 0.727872
1932.0858988761902
Epoch [16] loss: 2306157.916135
[17,   500] loss: 0.661371
1937.049195766449
[17,  1000] loss: 0.669402
1941.824925661087
[17,  1500] loss: 0.692697
1946.5858664512634
[17,  2000] loss: 0.649614
1951.465014219284
[17,  2500] loss: 0.742031
1956.2756900787354
[17,  3000] loss: 0.702243
1961.0247626304626
[17,  3500] loss: 0.694251
1965.768615245819
[17,  4000] loss: 0.638543
1970.5553359985352
[17,  4500] loss: 0.703395
1975.4678003787994
[17,  5000] loss: 0.726573
1980.4276785850525
[17,  5500] loss: 0.715547
1985.4343621730804
[17,  6000] loss: 0.692854
1990.386449098587
[17,  6500] loss: 0.708156
1995.345065355301
[17,  7000] loss: 0.674489
2000.2510232925415
[17,  7500] loss: 0.662809
2005.1381232738495
[17,  8000] loss: 0.691737
2009.9454019069672
[17,  8500] loss: 0.711622
2014.7813096046448
[17,  9000] loss: 0.641199
2019.5509996414185
[17,  9500] loss: 0.726327
2024.369679927826
[17, 10000] loss: 0.704092
2029.2159621715546
[17, 10500] loss: 0.700146
2034.0089299678802
[17, 11000] loss: 0.716188
2038.835708618164
[17, 11500] loss: 0.737472
2043.70614528656
[17, 12000] loss: 0.677358
2048.52996301651
[17, 12500] loss: 0.729011
2053.3455584049225
Epoch [17] loss: 2182103.752013
[18,   500] loss: 0.648972
2058.294250011444
[18,  1000] loss: 0.663110
2063.1631755828857
[18,  1500] loss: 0.660698
2068.1535573005676
[18,  2000] loss: 0.669795
2072.9749717712402
[18,  2500] loss: 0.634284
2077.823523044586
[18,  3000] loss: 0.627744
2082.695772409439
[18,  3500] loss: 0.675781
2087.5465655326843
[18,  4000] loss: 0.621016
2092.3893070220947
[18,  4500] loss: 0.645078
2097.148113965988
[18,  5000] loss: 0.620857
2101.9909443855286
[18,  5500] loss: 0.638399
2106.8284754753113
[18,  6000] loss: 0.668049
2111.644885778427
[18,  6500] loss: 0.662131
2116.413437128067
[18,  7000] loss: 0.663932
2121.2429115772247
[18,  7500] loss: 0.654871
2126.0597887039185
[18,  8000] loss: 0.703248
2130.873652458191
[18,  8500] loss: 0.656458
2135.6544234752655
[18,  9000] loss: 0.674515
2140.425087928772
[18,  9500] loss: 0.654912
2145.1791088581085
[18, 10000] loss: 0.683015
2150.01092338562
[18, 10500] loss: 0.661878
2154.871792793274
[18, 11000] loss: 0.689574
2159.701947927475
[18, 11500] loss: 0.668659
2164.5129041671753
[18, 12000] loss: 0.633216
2169.2945148944855
[18, 12500] loss: 0.707760
2174.1003968715668
Epoch [18] loss: 2049608.875756
[19,   500] loss: 0.602377
2179.0995342731476
[19,  1000] loss: 0.590988
2183.948143005371
[19,  1500] loss: 0.617822
2188.760354757309
[19,  2000] loss: 0.616099
2193.542930841446
[19,  2500] loss: 0.632822
2198.3459193706512
[19,  3000] loss: 0.644991
2203.1169188022614
[19,  3500] loss: 0.593463
2207.9072053432465
[19,  4000] loss: 0.629494
2212.8026955127716
[19,  4500] loss: 0.625664
2217.678654909134
[19,  5000] loss: 0.658196
2222.476140022278
[19,  5500] loss: 0.673033
2227.224931716919
[19,  6000] loss: 0.618731
2232.0050377845764
[19,  6500] loss: 0.660224
2236.7824833393097
[19,  7000] loss: 0.645483
2241.6178708076477
[19,  7500] loss: 0.598236
2246.424192428589
[19,  8000] loss: 0.672780
2251.2395510673523
[19,  8500] loss: 0.614265
2256.0677058696747
[19,  9000] loss: 0.640365
2260.855262041092
[19,  9500] loss: 0.688042
2265.6361989974976
[19, 10000] loss: 0.639934
2270.4255406856537
[19, 10500] loss: 0.664195
2275.2774727344513
[19, 11000] loss: 0.646471
2280.1158952713013
[19, 11500] loss: 0.671497
2284.954850435257
[19, 12000] loss: 0.615681
2289.736791610718
[19, 12500] loss: 0.656867
2294.528143644333
Epoch [19] loss: 1996718.704533
[20,   500] loss: 0.592611
2299.4793558120728
[20,  1000] loss: 0.616188
2304.278724193573
[20,  1500] loss: 0.604286
2309.0571477413177
[20,  2000] loss: 0.568640
2313.895400285721
[20,  2500] loss: 0.603273
2318.649547815323
[20,  3000] loss: 0.607035
2323.3963911533356
[20,  3500] loss: 0.568719
2328.148249387741
[20,  4000] loss: 0.594126
2332.9612119197845
[20,  4500] loss: 0.582707
2337.756050825119
[20,  5000] loss: 0.606398
2342.5851912498474
[20,  5500] loss: 0.587985
2347.3655700683594
[20,  6000] loss: 0.605608
2352.137089252472
[20,  6500] loss: 0.565766
2356.9244289398193
[20,  7000] loss: 0.623621
2361.729915380478
[20,  7500] loss: 0.601642
2366.4746222496033
[20,  8000] loss: 0.633856
2371.2203392982483
[20,  8500] loss: 0.651564
2375.970371246338
[20,  9000] loss: 0.606591
2380.7831830978394
[20,  9500] loss: 0.657799
2385.6046855449677
[20, 10000] loss: 0.617540
2390.3621802330017
[20, 10500] loss: 0.646577
2395.1659030914307
[20, 11000] loss: 0.645878
2400.0107958316803
[20, 11500] loss: 0.567358
2404.8175110816956
[20, 12000] loss: 0.596421
2409.5975511074066
[20, 12500] loss: 0.592667
2414.3566410541534
Epoch [20] loss: 1900713.313851
Finished Training
Saving model to /data/s4091221/trained-models/resnet182020-02-25 00:57:24.816639
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-1.8718, -1.3169, -0.9354,  3.4069, -0.0834,  1.5195,  1.0975, -2.5733,
          0.4999, -0.2827],
        [ 3.5698,  5.2386, -2.8651, -1.6679, -2.6431, -4.1049, -3.6071, -4.9973,
          8.8642,  1.7504],
        [ 5.0218,  1.4606, -2.3738, -0.8579,  0.0282, -3.0591, -3.0999, -2.9036,
          4.4639,  1.2359],
        [ 4.5940, -1.4518, -1.2690, -0.8403, -0.3138, -4.0679, -2.1686, -2.4012,
          5.6998,  0.7420]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat  ship plane  ship
Accuracy of the network on the 4000.0 test images: 70 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': True, 'batch_size': 4, 'workers': 2, 'model_archi': 14, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=14, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=True, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 frog  deer plane   cat
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet18 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)
Model resnet18 Reshaped
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582588666.1762938
[1,   500] loss: 2.122810
4.939640045166016
[1,  1000] loss: 1.971023
9.803983211517334
[1,  1500] loss: 1.955257
14.702081680297852
[1,  2000] loss: 1.900506
19.608574867248535
[1,  2500] loss: 1.797132
24.508100748062134
[1,  3000] loss: 1.819252
29.398625135421753
[1,  3500] loss: 1.738314
34.24547863006592
[1,  4000] loss: 1.726037
39.1890230178833
[1,  4500] loss: 1.666044
44.18295621871948
[1,  5000] loss: 1.604410
49.09716606140137
[1,  5500] loss: 1.605509
54.07627606391907
[1,  6000] loss: 1.630485
58.93955612182617
[1,  6500] loss: 1.597146
63.83768582344055
[1,  7000] loss: 1.604573
68.83516669273376
[1,  7500] loss: 1.566690
73.77025103569031
[1,  8000] loss: 1.629047
78.67534017562866
[1,  8500] loss: 1.532270
83.5818862915039
[1,  9000] loss: 1.543613
88.49549221992493
[1,  9500] loss: 1.507424
93.33666014671326
[1, 10000] loss: 1.498593
98.1781177520752
[1, 10500] loss: 1.496802
103.03150725364685
[1, 11000] loss: 1.493487
107.86893320083618
[1, 11500] loss: 1.471235
112.753737449646
[1, 12000] loss: 1.399681
117.60305118560791
[1, 12500] loss: 1.427039
122.47343349456787
Epoch [1] loss: 5199000.360352
[2,   500] loss: 1.385940
127.45970940589905
[2,  1000] loss: 1.367250
132.3073832988739
[2,  1500] loss: 1.380022
137.26153469085693
[2,  2000] loss: 1.376438
142.2211673259735
[2,  2500] loss: 1.372602
147.13514518737793
[2,  3000] loss: 1.316368
152.01156663894653
[2,  3500] loss: 1.350297
156.86192178726196
[2,  4000] loss: 1.339291
161.71362328529358
[2,  4500] loss: 1.334519
166.55695724487305
[2,  5000] loss: 1.345360
171.46342849731445
[2,  5500] loss: 1.304809
176.4524621963501
[2,  6000] loss: 1.324819
181.41916584968567
[2,  6500] loss: 1.331054
186.35892271995544
[2,  7000] loss: 1.329595
191.24641227722168
[2,  7500] loss: 1.315130
196.23135685920715
[2,  8000] loss: 1.263774
201.10248517990112
[2,  8500] loss: 1.272061
205.986341714859
[2,  9000] loss: 1.262802
210.8769657611847
[2,  9500] loss: 1.283958
215.75112533569336
[2, 10000] loss: 1.298033
220.6443727016449
[2, 10500] loss: 1.260046
225.51078295707703
[2, 11000] loss: 1.265812
230.3806493282318
[2, 11500] loss: 1.283161
235.26262950897217
[2, 12000] loss: 1.217645
240.09422159194946
[2, 12500] loss: 1.207767
244.96605134010315
Epoch [2] loss: 4094431.111955
[3,   500] loss: 1.207827
249.96341753005981
[3,  1000] loss: 1.204532
254.8398208618164
[3,  1500] loss: 1.147284
259.71320819854736
[3,  2000] loss: 1.203508
264.59378027915955
[3,  2500] loss: 1.239173
269.4035086631775
[3,  3000] loss: 1.215299
274.210161447525
[3,  3500] loss: 1.167968
279.0361096858978
[3,  4000] loss: 1.167249
283.9425048828125
[3,  4500] loss: 1.131068
288.8058636188507
[3,  5000] loss: 1.150820
293.69093322753906
[3,  5500] loss: 1.141922
298.612078666687
[3,  6000] loss: 1.185380
303.48419308662415
[3,  6500] loss: 1.154681
308.32253980636597
[3,  7000] loss: 1.170283
313.2009446620941
[3,  7500] loss: 1.147856
318.1557927131653
[3,  8000] loss: 1.182445
323.04857993125916
[3,  8500] loss: 1.162461
327.94609928131104
[3,  9000] loss: 1.121798
332.811336517334
[3,  9500] loss: 1.116272
337.6931428909302
[3, 10000] loss: 1.199171
342.5416557788849
[3, 10500] loss: 1.137283
347.39683508872986
[3, 11000] loss: 1.118960
352.2526168823242
[3, 11500] loss: 1.087432
357.11702609062195
[3, 12000] loss: 1.177671
362.01656556129456
[3, 12500] loss: 1.119830
366.88983035087585
Epoch [3] loss: 3637942.702637
[4,   500] loss: 1.108380
371.94503259658813
[4,  1000] loss: 1.040842
376.86059308052063
[4,  1500] loss: 1.046102
381.80562114715576
[4,  2000] loss: 1.095419
386.72806572914124
[4,  2500] loss: 1.064334
391.59628987312317
[4,  3000] loss: 1.082909
396.5008144378662
[4,  3500] loss: 1.054905
401.42009377479553
[4,  4000] loss: 1.078102
406.5925600528717
[4,  4500] loss: 1.080893
411.5295219421387
[4,  5000] loss: 1.053382
416.4854784011841
[4,  5500] loss: 1.022924
421.4562087059021
[4,  6000] loss: 1.048414
426.47297525405884
[4,  6500] loss: 1.035989
431.3764934539795
[4,  7000] loss: 1.051762
436.4133038520813
[4,  7500] loss: 1.068696
441.42105746269226
[4,  8000] loss: 1.115187
446.4583032131195
[4,  8500] loss: 1.085194
451.53977513313293
[4,  9000] loss: 1.049909
456.43999695777893
[4,  9500] loss: 1.020364
461.3952856063843
[4, 10000] loss: 1.040200
466.37870383262634
[4, 10500] loss: 1.030576
471.4186279773712
[4, 11000] loss: 1.046923
476.444212436676
[4, 11500] loss: 1.093042
481.4408473968506
[4, 12000] loss: 1.011640
486.371378660202
[4, 12500] loss: 1.039823
491.26315569877625
Epoch [4] loss: 3314127.224656
[5,   500] loss: 0.948351
496.3568756580353
[5,  1000] loss: 0.949241
501.2939558029175
[5,  1500] loss: 0.970378
506.3582797050476
[5,  2000] loss: 0.989470
511.3348710536957
[5,  2500] loss: 0.929076
516.3515892028809
[5,  3000] loss: 0.975938
521.3129813671112
[5,  3500] loss: 0.948628
526.173220872879
[5,  4000] loss: 0.981789
531.2055342197418
[5,  4500] loss: 1.050220
536.2256228923798
[5,  5000] loss: 1.043726
541.2438588142395
[5,  5500] loss: 0.980299
546.1102232933044
[5,  6000] loss: 0.930204
551.0097522735596
[5,  6500] loss: 0.978628
555.8970446586609
[5,  7000] loss: 1.009892
560.8521039485931
[5,  7500] loss: 0.994528
565.7981338500977
[5,  8000] loss: 0.970964
570.7515172958374
[5,  8500] loss: 1.000627
575.6827778816223
[5,  9000] loss: 0.982009
580.5614123344421
[5,  9500] loss: 1.001822
585.4344239234924
[5, 10000] loss: 0.963723
590.3627741336823
[5, 10500] loss: 1.038038
595.288311958313
[5, 11000] loss: 0.950232
600.20552110672
[5, 11500] loss: 0.999194
605.1225888729095
[5, 12000] loss: 0.978292
610.0419807434082
[5, 12500] loss: 1.002361
615.0516350269318
Epoch [5] loss: 3086187.182335
[6,   500] loss: 0.913504
620.0381593704224
[6,  1000] loss: 0.917619
624.8824999332428
[6,  1500] loss: 0.923472
629.8432502746582
[6,  2000] loss: 0.970765
634.7932538986206
[6,  2500] loss: 0.947153
639.8059682846069
[6,  3000] loss: 0.929936
644.7204895019531
[6,  3500] loss: 0.923387
649.6074826717377
[6,  4000] loss: 0.935355
654.5324335098267
[6,  4500] loss: 0.927426
659.4759442806244
[6,  5000] loss: 0.969308
664.454708814621
[6,  5500] loss: 0.979275
669.4410636425018
[6,  6000] loss: 0.990238
674.3952176570892
[6,  6500] loss: 0.935185
679.3399164676666
[6,  7000] loss: 0.928610
684.2855279445648
[6,  7500] loss: 0.893039
689.2596509456635
[6,  8000] loss: 0.894515
694.2002680301666
[6,  8500] loss: 0.927665
699.154764175415
[6,  9000] loss: 0.911038
704.1274592876434
[6,  9500] loss: 0.880062
709.1496706008911
[6, 10000] loss: 0.900174
714.1712870597839
[6, 10500] loss: 0.848131
719.2178966999054
[6, 11000] loss: 0.880752
724.1831452846527
[6, 11500] loss: 0.858021
729.0896301269531
[6, 12000] loss: 0.948827
734.1074833869934
[6, 12500] loss: 0.865094
739.1985061168671
Epoch [6] loss: 2884876.556879
[7,   500] loss: 0.896367
744.4850056171417
[7,  1000] loss: 0.882598
749.6467282772064
[7,  1500] loss: 0.830898
754.8716857433319
[7,  2000] loss: 0.872303
759.9865336418152
[7,  2500] loss: 0.854380
765.0362348556519
[7,  3000] loss: 0.837989
770.2576010227203
[7,  3500] loss: 0.866745
775.4017200469971
[7,  4000] loss: 0.863714
780.3961889743805
[7,  4500] loss: 0.830284
785.4601933956146
[7,  5000] loss: 0.865184
790.5153760910034
[7,  5500] loss: 0.850338
795.4802613258362
[7,  6000] loss: 0.860337
800.4323482513428
[7,  6500] loss: 0.837891
805.39741563797
[7,  7000] loss: 0.916416
810.3549737930298
[7,  7500] loss: 0.878614
815.2863945960999
[7,  8000] loss: 0.853047
820.2165505886078
[7,  8500] loss: 0.882592
825.1500933170319
[7,  9000] loss: 0.881068
830.1059358119965
[7,  9500] loss: 0.885577
835.0483860969543
[7, 10000] loss: 0.929182
839.98876786232
[7, 10500] loss: 0.860164
844.9399280548096
[7, 11000] loss: 0.862820
849.82470536232
[7, 11500] loss: 0.881119
854.7115573883057
[7, 12000] loss: 0.882058
859.6271500587463
[7, 12500] loss: 0.870254
864.5085334777832
Epoch [7] loss: 2711468.072058
[8,   500] loss: 0.809064
869.4720633029938
[8,  1000] loss: 0.815920
874.2967722415924
[8,  1500] loss: 0.810481
879.1589097976685
[8,  2000] loss: 0.800745
884.0356795787811
[8,  2500] loss: 0.782434
889.0451233386993
[8,  3000] loss: 0.833372
894.0539817810059
[8,  3500] loss: 0.815031
899.0874691009521
[8,  4000] loss: 0.843097
904.048023223877
[8,  4500] loss: 0.798858
909.0405552387238
[8,  5000] loss: 0.818884
914.0696265697479
[8,  5500] loss: 0.795460
919.1655490398407
[8,  6000] loss: 0.824214
924.1468749046326
[8,  6500] loss: 0.794881
929.2284259796143
[8,  7000] loss: 0.820423
934.2370920181274
[8,  7500] loss: 0.809517
939.2851321697235
[8,  8000] loss: 0.803242
944.4098744392395
[8,  8500] loss: 0.850635
949.4358024597168
[8,  9000] loss: 0.804091
954.4473369121552
[8,  9500] loss: 0.786966
959.3877682685852
[8, 10000] loss: 0.829259
964.3358972072601
[8, 10500] loss: 0.801954
969.3906173706055
[8, 11000] loss: 0.802391
974.4667673110962
[8, 11500] loss: 0.830438
979.387861251831
[8, 12000] loss: 0.742038
984.3404676914215
[8, 12500] loss: 0.763652
989.4189596176147
Epoch [8] loss: 2533812.119723
[9,   500] loss: 0.775739
994.5779213905334
[9,  1000] loss: 0.740646
999.5300898551941
[9,  1500] loss: 0.743284
1004.4019582271576
[9,  2000] loss: 0.764556
1009.2979021072388
[9,  2500] loss: 0.771747
1014.3764107227325
[9,  3000] loss: 0.733035
1019.4386825561523
[9,  3500] loss: 0.761516
1024.387941122055
[9,  4000] loss: 0.762752
1029.4152665138245
[9,  4500] loss: 0.785334
1034.5145919322968
[9,  5000] loss: 0.721408
1039.666214466095
[9,  5500] loss: 0.825428
1044.7745060920715
[9,  6000] loss: 0.785603
1049.9163041114807
[9,  6500] loss: 0.753444
1054.8753187656403
[9,  7000] loss: 0.813189
1059.8029091358185
[9,  7500] loss: 0.770975
1064.80753159523
[9,  8000] loss: 0.788305
1069.9508731365204
[9,  8500] loss: 0.788055
1075.167141675949
[9,  9000] loss: 0.764334
1080.2910068035126
[9,  9500] loss: 0.758659
1085.3932967185974
[9, 10000] loss: 0.821544
1090.3933503627777
[9, 10500] loss: 0.760667
1095.4091203212738
[9, 11000] loss: 0.756150
1100.4454934597015
[9, 11500] loss: 0.774947
1105.5126276016235
[9, 12000] loss: 0.795484
1110.575163602829
[9, 12500] loss: 0.793838
1115.5052678585052
Epoch [9] loss: 2428393.008343
[10,   500] loss: 0.703369
1120.635234117508
[10,  1000] loss: 0.712575
1125.5863146781921
[10,  1500] loss: 0.703934
1130.530213356018
[10,  2000] loss: 0.686943
1135.4447486400604
[10,  2500] loss: 0.721050
1140.4529914855957
[10,  3000] loss: 0.723374
1145.5182585716248
[10,  3500] loss: 0.727697
1150.54021525383
[10,  4000] loss: 0.736501
1155.6110663414001
[10,  4500] loss: 0.757946
1160.562230348587
[10,  5000] loss: 0.769872
1165.4971661567688
[10,  5500] loss: 0.722143
1170.4945657253265
[10,  6000] loss: 0.705213
1175.5177955627441
[10,  6500] loss: 0.698374
1180.5208160877228
[10,  7000] loss: 0.779310
1185.5140027999878
[10,  7500] loss: 0.722299
1190.5681438446045
[10,  8000] loss: 0.696882
1195.7036488056183
[10,  8500] loss: 0.664104
1200.8238213062286
[10,  9000] loss: 0.740469
1205.7823460102081
[10,  9500] loss: 0.762361
1210.7918000221252
[10, 10000] loss: 0.739060
1215.844527721405
[10, 10500] loss: 0.733150
1220.848822593689
[10, 11000] loss: 0.709580
1225.7851581573486
[10, 11500] loss: 0.745851
1230.7579221725464
[10, 12000] loss: 0.719200
1235.754086971283
[10, 12500] loss: 0.695934
1240.791934967041
Epoch [10] loss: 2255936.200859
[11,   500] loss: 0.713042
1245.8909640312195
[11,  1000] loss: 0.677193
1250.8967008590698
[11,  1500] loss: 0.725703
1255.8956594467163
[11,  2000] loss: 0.725569
1260.8812923431396
[11,  2500] loss: 0.687664
1265.8387763500214
[11,  3000] loss: 0.688091
1270.7890782356262
[11,  3500] loss: 0.705918
1275.7191002368927
[11,  4000] loss: 0.694124
1280.644838809967
[11,  4500] loss: 0.803288
1285.7075169086456
[11,  5000] loss: 0.675210
1290.6753435134888
[11,  5500] loss: 0.679724
1295.5677580833435
[11,  6000] loss: 0.693499
1300.610507965088
[11,  6500] loss: 0.701432
1305.6140251159668
[11,  7000] loss: 0.702840
1310.564145565033
[11,  7500] loss: 0.676736
1315.5136148929596
[11,  8000] loss: 0.749122
1320.4961731433868
[11,  8500] loss: 0.702813
1325.5798559188843
[11,  9000] loss: 0.704046
1330.535893201828
[11,  9500] loss: 0.719120
1335.6110990047455
[11, 10000] loss: 0.661024
1340.7027626037598
[11, 10500] loss: 0.716055
1345.8170166015625
[11, 11000] loss: 0.728390
1351.0493729114532
[11, 11500] loss: 0.710561
1356.1834440231323
[11, 12000] loss: 0.656429
1361.3828966617584
[11, 12500] loss: 0.691523
1366.5344388484955
Epoch [11] loss: 2218990.145437
[12,   500] loss: 0.640453
1371.9038865566254
[12,  1000] loss: 0.603489
1376.8969905376434
[12,  1500] loss: 0.694736
1381.8500392436981
[12,  2000] loss: 0.615820
1386.8996274471283
[12,  2500] loss: 0.652144
1391.887151002884
[12,  3000] loss: 0.655334
1396.9241573810577
[12,  3500] loss: 0.629254
1401.7906157970428
[12,  4000] loss: 0.594554
1406.6971514225006
[12,  4500] loss: 0.643735
1411.5626513957977
[12,  5000] loss: 0.617367
1416.4841310977936
[12,  5500] loss: 0.703368
1421.400354385376
[12,  6000] loss: 0.637837
1426.36975979805
[12,  6500] loss: 0.671762
1431.3210110664368
[12,  7000] loss: 0.658312
1436.268152475357
[12,  7500] loss: 0.695572
1441.0996162891388
[12,  8000] loss: 0.666299
1445.9304523468018
[12,  8500] loss: 0.657691
1450.7692716121674
[12,  9000] loss: 0.671600
1455.5973801612854
[12,  9500] loss: 0.674551
1460.5428314208984
[12, 10000] loss: 0.671109
1465.5167577266693
[12, 10500] loss: 0.644285
1470.4495332241058
[12, 11000] loss: 0.715657
1475.3986806869507
[12, 11500] loss: 0.685477
1480.415147781372
[12, 12000] loss: 0.639140
1485.4285814762115
[12, 12500] loss: 0.634417
1490.3945052623749
Epoch [12] loss: 2057053.270663
[13,   500] loss: 0.671151
1495.4033463001251
[13,  1000] loss: 0.572177
1500.3508651256561
[13,  1500] loss: 0.620820
1505.3161227703094
[13,  2000] loss: 0.618991
1510.2299437522888
[13,  2500] loss: 0.613902
1515.1658384799957
[13,  3000] loss: 0.631031
1520.0762510299683
[13,  3500] loss: 0.597614
1525.0247399806976
[13,  4000] loss: 0.657212
1529.9336519241333
[13,  4500] loss: 0.611680
1534.9012098312378
[13,  5000] loss: 0.665159
1539.7910833358765
[13,  5500] loss: 0.589704
1544.6705305576324
[13,  6000] loss: 0.640613
1549.5641129016876
[13,  6500] loss: 0.620549
1554.5119035243988
[13,  7000] loss: 0.600295
1559.4672136306763
[13,  7500] loss: 0.622529
1564.4395866394043
[13,  8000] loss: 0.638815
1569.444227695465
[13,  8500] loss: 0.569170
1574.3861184120178
[13,  9000] loss: 0.617838
1579.526119709015
[13,  9500] loss: 0.688442
1584.4521584510803
[13, 10000] loss: 0.672016
1589.394990682602
[13, 10500] loss: 0.628703
1594.4519436359406
[13, 11000] loss: 0.646872
1599.464283466339
[13, 11500] loss: 0.612889
1604.48015999794
[13, 12000] loss: 0.613823
1609.4697477817535
[13, 12500] loss: 0.599698
1614.538408279419
Epoch [13] loss: 1949659.527359
[14,   500] loss: 0.569683
1619.6612074375153
[14,  1000] loss: 0.552186
1624.70360994339
[14,  1500] loss: 0.594852
1629.7019445896149
[14,  2000] loss: 0.598855
1634.7711215019226
[14,  2500] loss: 0.595889
1639.8462352752686
[14,  3000] loss: 0.607604
1644.8546199798584
[14,  3500] loss: 0.591562
1649.8135142326355
[14,  4000] loss: 0.565005
1654.7359457015991
[14,  4500] loss: 0.570900
1659.7012856006622
[14,  5000] loss: 0.646315
1664.672512769699
[14,  5500] loss: 0.596493
1669.6879448890686
[14,  6000] loss: 0.624517
1674.8840110301971
[14,  6500] loss: 0.594758
1679.948272228241
[14,  7000] loss: 0.603245
1684.9584012031555
[14,  7500] loss: 0.559904
1690.0224921703339
[14,  8000] loss: 0.633875
1694.9970536231995
[14,  8500] loss: 0.608663
1699.9690492153168
[14,  9000] loss: 0.595128
1705.009346961975
[14,  9500] loss: 0.613646
1710.0517182350159
[14, 10000] loss: 0.584586
1715.0979602336884
[14, 10500] loss: 0.603895
1720.115085363388
[14, 11000] loss: 0.571990
1725.1077408790588
[14, 11500] loss: 0.632545
1730.1157829761505
[14, 12000] loss: 0.615282
1735.1519334316254
[14, 12500] loss: 0.608132
1740.160667181015
Epoch [14] loss: 1875998.867312
[15,   500] loss: 0.551963
1745.1892037391663
[15,  1000] loss: 0.517731
1750.1150062084198
[15,  1500] loss: 0.550873
1755.2037239074707
[15,  2000] loss: 0.566418
1760.1783938407898
[15,  2500] loss: 0.609336
1765.1509552001953
[15,  3000] loss: 0.545436
1770.041009902954
[15,  3500] loss: 0.566603
1774.9809710979462
[15,  4000] loss: 0.546008
1779.9091522693634
[15,  4500] loss: 0.539639
1784.8113932609558
[15,  5000] loss: 0.611704
1789.5964200496674
[15,  5500] loss: 0.603548
1794.430200099945
[15,  6000] loss: 0.556890
1799.4477865695953
[15,  6500] loss: 0.559792
1804.5064990520477
[15,  7000] loss: 0.581720
1809.562330007553
[15,  7500] loss: 0.578569
1814.5483434200287
[15,  8000] loss: 0.537657
1819.617927789688
[15,  8500] loss: 0.539894
1824.532889842987
[15,  9000] loss: 0.539308
1829.4565279483795
[15,  9500] loss: 0.578668
1834.3417596817017
[15, 10000] loss: 0.594393
1839.1994059085846
[15, 10500] loss: 0.563632
1844.0958757400513
[15, 11000] loss: 0.607167
1848.9553468227386
[15, 11500] loss: 0.527966
1853.9658932685852
[15, 12000] loss: 0.583544
1858.8705458641052
[15, 12500] loss: 0.564954
1863.7860503196716
Epoch [15] loss: 1765037.478750
[16,   500] loss: 0.547079
1868.7912335395813
[16,  1000] loss: 0.528872
1873.658761024475
[16,  1500] loss: 0.519995
1878.6706154346466
[16,  2000] loss: 0.567391
1883.6546137332916
[16,  2500] loss: 0.532047
1888.6662504673004
[16,  3000] loss: 0.551450
1893.6622865200043
[16,  3500] loss: 0.518082
1898.711220741272
[16,  4000] loss: 0.576551
1903.669097661972
[16,  4500] loss: 0.548148
1908.5771493911743
[16,  5000] loss: 0.537293
1913.627037525177
[16,  5500] loss: 0.550323
1918.6110372543335
[16,  6000] loss: 0.545749
1923.5823593139648
[16,  6500] loss: 0.555421
1928.5088412761688
[16,  7000] loss: 0.538414
1933.4066336154938
[16,  7500] loss: 0.601204
1938.5357842445374
[16,  8000] loss: 0.541906
1943.588211774826
[16,  8500] loss: 0.503342
1948.5792210102081
[16,  9000] loss: 0.554545
1953.5624656677246
[16,  9500] loss: 0.528115
1958.6443564891815
[16, 10000] loss: 0.489302
1963.7886564731598
[16, 10500] loss: 0.480668
1968.882387638092
[16, 11000] loss: 0.556924
1973.883048772812
[16, 11500] loss: 0.528701
1978.8406269550323
[16, 12000] loss: 0.548630
1983.7981967926025
[16, 12500] loss: 0.510591
1988.696796655655
Epoch [16] loss: 1688957.469752
[17,   500] loss: 0.504238
1993.882574081421
[17,  1000] loss: 0.525959
1998.9612200260162
[17,  1500] loss: 0.473395
2003.9416110515594
[17,  2000] loss: 0.476614
2008.9517042636871
[17,  2500] loss: 0.475566
2013.9936316013336
[17,  3000] loss: 0.534763
2019.0017807483673
[17,  3500] loss: 0.485893
2023.9749689102173
[17,  4000] loss: 0.479630
2028.9258122444153
[17,  4500] loss: 0.550508
2033.9076006412506
[17,  5000] loss: 0.501220
2038.992103099823
[17,  5500] loss: 0.473767
2044.0128223896027
[17,  6000] loss: 0.511333
2049.055104970932
[17,  6500] loss: 0.487540
2054.0775871276855
[17,  7000] loss: 0.507110
2059.094429254532
[17,  7500] loss: 0.513357
2064.0628893375397
[17,  8000] loss: 0.511525
2069.1145763397217
[17,  8500] loss: 0.529662
2074.0655739307404
[17,  9000] loss: 0.552828
2079.075328350067
[17,  9500] loss: 0.547495
2084.0171151161194
[17, 10000] loss: 0.510793
2089.0207891464233
[17, 10500] loss: 0.492220
2093.9163331985474
[17, 11000] loss: 0.494593
2098.8119263648987
[17, 11500] loss: 0.530305
2103.6846573352814
[17, 12000] loss: 0.517089
2108.518200159073
[17, 12500] loss: 0.502028
2113.577762365341
Epoch [17] loss: 1585795.925255
[18,   500] loss: 0.449392
2118.8476724624634
[18,  1000] loss: 0.463767
2123.8848056793213
[18,  1500] loss: 0.485461
2128.9182319641113
[18,  2000] loss: 0.461540
2134.02205991745
[18,  2500] loss: 0.464036
2138.9800658226013
[18,  3000] loss: 0.511497
2143.863303899765
[18,  3500] loss: 0.456012
2148.9216134548187
[18,  4000] loss: 0.482408
2154.155526638031
[18,  4500] loss: 0.513075
2159.1474270820618
[18,  5000] loss: 0.501517
2164.13152217865
[18,  5500] loss: 0.514606
2169.044495344162
[18,  6000] loss: 0.472888
2173.9842307567596
[18,  6500] loss: 0.470791
2178.8961091041565
[18,  7000] loss: 0.463562
2183.8727469444275
[18,  7500] loss: 0.500612
2188.8097825050354
[18,  8000] loss: 0.446040
2193.693952560425
[18,  8500] loss: 0.524454
2198.6602866649628
[18,  9000] loss: 0.468707
2203.6836371421814
[18,  9500] loss: 0.513716
2208.6417531967163
[18, 10000] loss: 0.488431
2213.522928953171
[18, 10500] loss: 0.468307
2218.4537484645844
[18, 11000] loss: 0.460730
2223.4010767936707
[18, 11500] loss: 0.538950
2228.250201702118
[18, 12000] loss: 0.501109
2233.076295375824
[18, 12500] loss: 0.510939
2237.9577345848083
Epoch [18] loss: 1526105.027332
[19,   500] loss: 0.441675
2243.1020736694336
[19,  1000] loss: 0.382189
2248.0962476730347
[19,  1500] loss: 0.430225
2252.992646932602
[19,  2000] loss: 0.459729
2257.9776599407196
[19,  2500] loss: 0.477690
2262.9899864196777
[19,  3000] loss: 0.445825
2267.9433631896973
[19,  3500] loss: 0.450575
2272.887528896332
[19,  4000] loss: 0.443604
2277.8709423542023
[19,  4500] loss: 0.495650
2282.915786743164
[19,  5000] loss: 0.476077
2287.901145219803
[19,  5500] loss: 0.454294
2292.803477525711
[19,  6000] loss: 0.493744
2297.698385953903
[19,  6500] loss: 0.416517
2302.571359872818
[19,  7000] loss: 0.433534
2307.5435802936554
[19,  7500] loss: 0.484145
2312.5969359874725
[19,  8000] loss: 0.441839
2317.6859731674194
[19,  8500] loss: 0.434649
2322.76455950737
[19,  9000] loss: 0.459137
2327.833856344223
[19,  9500] loss: 0.451733
2332.9279265403748
[19, 10000] loss: 0.409137
2337.9471011161804
[19, 10500] loss: 0.471165
2343.0070295333862
[19, 11000] loss: 0.495473
2348.1626455783844
[19, 11500] loss: 0.509384
2353.3069598674774
[19, 12000] loss: 0.459926
2358.3884267807007
[19, 12500] loss: 0.514336
2363.4553096294403
Epoch [19] loss: 1445092.245934
[20,   500] loss: 0.428593
2368.700781106949
[20,  1000] loss: 0.411443
2373.7206337451935
[20,  1500] loss: 0.403403
2378.714954853058
[20,  2000] loss: 0.425793
2383.660894870758
[20,  2500] loss: 0.441056
2388.665606021881
[20,  3000] loss: 0.422178
2393.719174146652
[20,  3500] loss: 0.441574
2398.7547776699066
[20,  4000] loss: 0.429053
2403.802259683609
[20,  4500] loss: 0.421155
2408.8244898319244
[20,  5000] loss: 0.396437
2413.842363357544
[20,  5500] loss: 0.431844
2418.8157103061676
[20,  6000] loss: 0.413851
2423.8149194717407
[20,  6500] loss: 0.436238
2428.8461394309998
[20,  7000] loss: 0.437978
2433.9425563812256
[20,  7500] loss: 0.433715
2438.8847081661224
[20,  8000] loss: 0.464269
2443.830378293991
[20,  8500] loss: 0.426859
2448.7898428440094
[20,  9000] loss: 0.474282
2453.7472348213196
[20,  9500] loss: 0.409941
2458.6231219768524
[20, 10000] loss: 0.462948
2463.4790213108063
[20, 10500] loss: 0.470383
2468.4234008789062
[20, 11000] loss: 0.474478
2473.396744251251
[20, 11500] loss: 0.492467
2478.3475975990295
[20, 12000] loss: 0.447264
2483.2905836105347
[20, 12500] loss: 0.441831
2488.236173391342
Epoch [20] loss: 1363193.246991
Finished Training
Saving model to /data/s4091221/trained-models/resnet182020-02-25 01:39:14.449792
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-0.2456,  0.0722,  1.8894,  9.6669, -0.6443,  5.4181, -3.1187, -3.1228,
         -3.9657, -4.5932],
        [ 3.7037,  6.6862, -5.4319,  0.1381, -4.1204,  1.1415, -8.6070, -2.6180,
          7.5777, -0.3946],
        [ 0.4525,  0.5457, -0.1400, -0.5810, -1.2099, -0.7430, -1.2182,  0.0218,
          2.5562, -1.1844],
        [ 2.0582, -2.3481,  1.0367,  0.5923,  0.2026, -1.2474, -1.4204, -0.8246,
          1.4494, -0.4545]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat  ship  ship plane
Accuracy of the network on the 4000.0 test images: 75 %


###############################################################################
Peregrine Cluster
Job 9734792 for user 's4091221'
Finished at: Tue Feb 25 01:39:25 CET 2020

Job details:
============

Name                : resnet18.sh
User                : s4091221
Partition           : gpu
Nodes               : pg-gpu34
Cores               : 12
State               : COMPLETED
Submit              : 2020-02-24T23:58:14
Start               : 2020-02-25T00:16:01
End                 : 2020-02-25T01:39:25
Reserved walltime   : 15:00:00
Used walltime       : 01:23:24
Used CPU time       : 01:34:38 (efficiency:  9.46%)
% User (Computation): 93.91%
% System (I/O)      :  6.09%
Mem reserved        : 12000M/node
Max Mem used        : 2.91G (pg-gpu34)
Max Disk Write      : 227.69M (pg-gpu34)
Max Disk Read       : 1.06G (pg-gpu34)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
