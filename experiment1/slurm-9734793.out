Requirement already satisfied: torchvision in ./.local/lib/python3.7/site-packages (0.5.0)
Requirement already satisfied: six in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from torchvision) (1.12.0)
Requirement already satisfied: torch==1.4.0 in ./.local/lib/python3.7/site-packages (from torchvision) (1.4.0)
Requirement already satisfied: pillow>=4.1.1 in ./.local/lib/python3.7/site-packages (from torchvision) (7.0.0)
Requirement already satisfied: numpy in ./.local/lib/python3.7/site-packages (from torchvision) (1.18.1)
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=16, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
  cat   dog   dog   dog
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582587180.744881
[1,   500] loss: 2.590165
13.037400960922241
[1,  1000] loss: 2.573904
24.59389615058899
[1,  1500] loss: 2.552585
36.04075646400452
[1,  2000] loss: 2.554722
47.57965421676636
[1,  2500] loss: 2.529762
59.00436854362488
[1,  3000] loss: 2.482138
70.43437671661377
[1,  3500] loss: 2.478299
81.92013311386108
[1,  4000] loss: 2.447617
93.35230112075806
[1,  4500] loss: 2.426558
104.71595287322998
[1,  5000] loss: 2.412539
116.22184896469116
[1,  5500] loss: 2.416706
127.78488278388977
[1,  6000] loss: 2.435720
139.40167760849
[1,  6500] loss: 2.373401
150.8921356201172
[1,  7000] loss: 2.370368
162.46429085731506
[1,  7500] loss: 2.377714
173.90256810188293
[1,  8000] loss: 2.341870
185.4926357269287
[1,  8500] loss: 2.320585
197.0566165447235
[1,  9000] loss: 2.313370
208.55463361740112
[1,  9500] loss: 2.345127
220.09122920036316
[1, 10000] loss: 2.310504
231.58170413970947
[1, 10500] loss: 2.338001
243.0358603000641
[1, 11000] loss: 2.268475
254.46045517921448
[1, 11500] loss: 2.275881
265.93453645706177
[1, 12000] loss: 2.263361
277.5294210910797
[1, 12500] loss: 2.250432
289.09057903289795
Epoch [1] loss: 7526338.838048
[2,   500] loss: 2.288790
300.93592047691345
[2,  1000] loss: 2.302012
312.4949688911438
[2,  1500] loss: 2.254120
323.946391582489
[2,  2000] loss: 2.215959
335.5476961135864
[2,  2500] loss: 2.261193
347.05008029937744
[2,  3000] loss: 2.232539
358.69593715667725
[2,  3500] loss: 2.192313
370.25837898254395
[2,  4000] loss: 2.184470
381.70701813697815
[2,  4500] loss: 2.214444
393.3502445220947
[2,  5000] loss: 2.149961
404.77962946891785
[2,  5500] loss: 2.186751
416.3801329135895
[2,  6000] loss: 2.148398
428.0936689376831
[2,  6500] loss: 2.199288
439.7770104408264
[2,  7000] loss: 2.154550
451.39566349983215
[2,  7500] loss: 2.179770
462.97395396232605
[2,  8000] loss: 2.172635
474.6684067249298
[2,  8500] loss: 2.228301
486.34024453163147
[2,  9000] loss: 2.203789
498.0750892162323
[2,  9500] loss: 2.152010
509.78219842910767
[2, 10000] loss: 2.106003
521.3033511638641
[2, 10500] loss: 2.139822
532.9572615623474
[2, 11000] loss: 2.153943
544.5841801166534
[2, 11500] loss: 2.136410
556.2471671104431
[2, 12000] loss: 2.128907
567.7132384777069
[2, 12500] loss: 2.114651
579.3653249740601
Epoch [2] loss: 6843748.033080
[3,   500] loss: 2.107280
591.0148837566376
[3,  1000] loss: 2.055864
602.5213508605957
[3,  1500] loss: 2.064148
614.1009986400604
[3,  2000] loss: 2.052103
625.6756479740143
[3,  2500] loss: 2.046365
637.1547644138336
[3,  3000] loss: 2.026416
648.7799603939056
[3,  3500] loss: 2.013501
660.4205162525177
[3,  4000] loss: 2.040812
672.0407772064209
[3,  4500] loss: 2.007317
683.6184215545654
[3,  5000] loss: 2.038949
695.2061958312988
[3,  5500] loss: 2.069307
706.625848531723
[3,  6000] loss: 1.996978
718.1053056716919
[3,  6500] loss: 2.031161
729.6069478988647
[3,  7000] loss: 1.997456
741.1446712017059
[3,  7500] loss: 1.964624
752.7546103000641
[3,  8000] loss: 2.003357
764.2995457649231
[3,  8500] loss: 2.010344
775.8659083843231
[3,  9000] loss: 2.012384
787.4522795677185
[3,  9500] loss: 1.996550
798.9778807163239
[3, 10000] loss: 1.989840
810.5403816699982
[3, 10500] loss: 1.972525
822.1020736694336
[3, 11000] loss: 1.907903
833.6550452709198
[3, 11500] loss: 1.928055
845.1717612743378
[3, 12000] loss: 1.921367
856.7199547290802
[3, 12500] loss: 1.925860
868.310956954956
Epoch [3] loss: 6280231.138344
[4,   500] loss: 1.933702
880.1218965053558
[4,  1000] loss: 1.883680
891.7667043209076
[4,  1500] loss: 1.952772
903.2342391014099
[4,  2000] loss: 1.915826
914.8201446533203
[4,  2500] loss: 1.877604
926.5377826690674
[4,  3000] loss: 1.910090
938.2004547119141
[4,  3500] loss: 1.920877
949.8050668239594
[4,  4000] loss: 1.896865
961.203869342804
[4,  4500] loss: 1.895841
972.5962092876434
[4,  5000] loss: 1.913853
984.1423981189728
[4,  5500] loss: 1.877475
995.7115449905396
[4,  6000] loss: 1.899669
1007.4727876186371
[4,  6500] loss: 1.843735
1019.1579504013062
[4,  7000] loss: 1.860780
1030.771645307541
[4,  7500] loss: 1.853945
1042.307344198227
[4,  8000] loss: 1.870533
1053.7389907836914
[4,  8500] loss: 1.831935
1065.293921470642
[4,  9000] loss: 1.844558
1076.9736671447754
[4,  9500] loss: 1.830450
1088.6188118457794
[4, 10000] loss: 1.804593
1100.2622742652893
[4, 10500] loss: 1.877427
1111.895105600357
[4, 11000] loss: 1.782227
1123.5475871562958
[4, 11500] loss: 1.817449
1135.1382098197937
[4, 12000] loss: 1.812418
1146.789852142334
[4, 12500] loss: 1.814349
1158.4423291683197
Epoch [4] loss: 5833034.475906
[5,   500] loss: 1.813628
1170.183121919632
[5,  1000] loss: 1.801183
1181.7567536830902
[5,  1500] loss: 1.812594
1193.246083021164
[5,  2000] loss: 1.823945
1204.8767340183258
[5,  2500] loss: 1.755738
1216.4097590446472
[5,  3000] loss: 1.805176
1227.9834327697754
[5,  3500] loss: 1.776120
1239.5507900714874
[5,  4000] loss: 1.800695
1251.1123135089874
[5,  4500] loss: 1.765707
1262.5588648319244
[5,  5000] loss: 1.794163
1274.0569097995758
[5,  5500] loss: 1.742989
1285.6398034095764
[5,  6000] loss: 1.796867
1297.2574226856232
[5,  6500] loss: 1.729612
1308.8249435424805
[5,  7000] loss: 1.786534
1320.4162299633026
[5,  7500] loss: 1.793014
1332.0319979190826
[5,  8000] loss: 1.762826
1343.6806693077087
[5,  8500] loss: 1.747799
1355.3158564567566
[5,  9000] loss: 1.717019
1366.7985458374023
[5,  9500] loss: 1.724109
1378.2665750980377
[5, 10000] loss: 1.765292
1389.9379041194916
[5, 10500] loss: 1.745281
1401.4769122600555
[5, 11000] loss: 1.750253
1413.084909439087
[5, 11500] loss: 1.717045
1424.6338994503021
[5, 12000] loss: 1.748710
1436.0773816108704
[5, 12500] loss: 1.744387
1447.555408000946
Epoch [5] loss: 5522695.107897
[6,   500] loss: 1.726702
1459.1427924633026
[6,  1000] loss: 1.673093
1470.5346851348877
[6,  1500] loss: 1.700698
1482.0789632797241
[6,  2000] loss: 1.714309
1493.5964229106903
[6,  2500] loss: 1.766554
1505.1310210227966
[6,  3000] loss: 1.687036
1516.5398950576782
[6,  3500] loss: 1.683174
1528.0613672733307
[6,  4000] loss: 1.687086
1539.7057101726532
[6,  4500] loss: 1.745189
1551.3225502967834
[6,  5000] loss: 1.716696
1563.2252099514008
[6,  5500] loss: 1.673671
1575.2496964931488
[6,  6000] loss: 1.680817
1586.9928941726685
[6,  6500] loss: 1.678379
1598.5920104980469
[6,  7000] loss: 1.681505
1610.1174821853638
[6,  7500] loss: 1.704146
1621.663630247116
[6,  8000] loss: 1.662271
1633.3171401023865
[6,  8500] loss: 1.668197
1645.07382106781
[6,  9000] loss: 1.658614
1656.4569664001465
[6,  9500] loss: 1.684567
1667.911073923111
[6, 10000] loss: 1.688390
1679.4948992729187
[6, 10500] loss: 1.647943
1691.048139333725
[6, 11000] loss: 1.667687
1702.5345888137817
[6, 11500] loss: 1.652957
1714.1879541873932
[6, 12000] loss: 1.669471
1725.7331809997559
[6, 12500] loss: 1.670157
1737.2665376663208
Epoch [6] loss: 5270503.135484
[7,   500] loss: 1.592825
1748.9324321746826
[7,  1000] loss: 1.628237
1760.5878126621246
[7,  1500] loss: 1.640691
1772.2117552757263
[7,  2000] loss: 1.593050
1783.8528640270233
[7,  2500] loss: 1.665567
1795.4710993766785
[7,  3000] loss: 1.661761
1807.1673414707184
[7,  3500] loss: 1.647001
1818.92045378685
[7,  4000] loss: 1.651497
1830.6841146945953
[7,  4500] loss: 1.636619
1842.413373708725
[7,  5000] loss: 1.613297
1854.018384695053
[7,  5500] loss: 1.614500
1865.5152990818024
[7,  6000] loss: 1.629498
1877.0325157642365
[7,  6500] loss: 1.633669
1888.5076577663422
[7,  7000] loss: 1.591750
1900.0173783302307
[7,  7500] loss: 1.622843
1911.4469277858734
[7,  8000] loss: 1.615710
1922.9284734725952
[7,  8500] loss: 1.635632
1934.421404838562
[7,  9000] loss: 1.549540
1945.95006108284
[7,  9500] loss: 1.636347
1957.6670939922333
[7, 10000] loss: 1.572146
1969.2797393798828
[7, 10500] loss: 1.573547
1980.9043629169464
[7, 11000] loss: 1.604594
1992.5840566158295
[7, 11500] loss: 1.606750
2004.209946155548
[7, 12000] loss: 1.636943
2015.9228360652924
[7, 12500] loss: 1.593409
2027.338612794876
Epoch [7] loss: 5064228.749845
[8,   500] loss: 1.568968
2038.9532306194305
[8,  1000] loss: 1.586089
2050.4804077148438
[8,  1500] loss: 1.574627
2061.8517570495605
[8,  2000] loss: 1.623609
2073.3037707805634
[8,  2500] loss: 1.557360
2084.8329787254333
[8,  3000] loss: 1.580748
2096.188408613205
[8,  3500] loss: 1.537314
2107.809165239334
[8,  4000] loss: 1.558999
2119.290777206421
[8,  4500] loss: 1.528497
2130.9321250915527
[8,  5000] loss: 1.527063
2142.4152884483337
[8,  5500] loss: 1.552831
2153.8988773822784
[8,  6000] loss: 1.602697
2165.3858993053436
[8,  6500] loss: 1.487801
2177.1292176246643
[8,  7000] loss: 1.521821
2188.6794447898865
[8,  7500] loss: 1.583294
2200.142626285553
[8,  8000] loss: 1.536859
2211.6185619831085
[8,  8500] loss: 1.556912
2223.106076478958
[8,  9000] loss: 1.535902
2234.6069774627686
[8,  9500] loss: 1.558748
2246.088390350342
[8, 10000] loss: 1.542778
2257.495438337326
[8, 10500] loss: 1.515802
2269.062812805176
[8, 11000] loss: 1.513687
2280.5391442775726
[8, 11500] loss: 1.532847
2291.9678959846497
[8, 12000] loss: 1.487136
2303.4271824359894
[8, 12500] loss: 1.517889
2315.0388288497925
Epoch [8] loss: 4855667.912537
[9,   500] loss: 1.423999
2326.7253308296204
[9,  1000] loss: 1.517496
2338.2052557468414
[9,  1500] loss: 1.536464
2349.922831773758
[9,  2000] loss: 1.482067
2361.5183424949646
[9,  2500] loss: 1.545985
2373.1162281036377
[9,  3000] loss: 1.459556
2384.6759464740753
[9,  3500] loss: 1.492009
2396.154397726059
[9,  4000] loss: 1.494075
2407.644323825836
[9,  4500] loss: 1.499215
2419.0986802577972
[9,  5000] loss: 1.487940
2430.6554210186005
[9,  5500] loss: 1.515869
2442.2071754932404
[9,  6000] loss: 1.462179
2453.7210915088654
[9,  6500] loss: 1.494950
2465.232852935791
[9,  7000] loss: 1.496547
2476.6805169582367
[9,  7500] loss: 1.466669
2488.2105791568756
[9,  8000] loss: 1.460720
2499.7145767211914
[9,  8500] loss: 1.492574
2511.224778652191
[9,  9000] loss: 1.511925
2522.9082655906677
[9,  9500] loss: 1.535285
2534.4120626449585
[9, 10000] loss: 1.497471
2545.885505914688
[9, 10500] loss: 1.453923
2557.3629388809204
[9, 11000] loss: 1.468527
2568.844550848007
[9, 11500] loss: 1.483108
2580.336499929428
[9, 12000] loss: 1.448712
2591.8784062862396
[9, 12500] loss: 1.457440
2603.379599571228
Epoch [9] loss: 4660508.314986
[10,   500] loss: 1.459922
2614.9975628852844
[10,  1000] loss: 1.446416
2626.4217326641083
[10,  1500] loss: 1.446977
2637.947587251663
[10,  2000] loss: 1.485370
2649.41711974144
[10,  2500] loss: 1.487202
2660.9002091884613
[10,  3000] loss: 1.410492
2672.4793107509613
[10,  3500] loss: 1.419853
2683.951430797577
[10,  4000] loss: 1.419163
2695.4264607429504
[10,  4500] loss: 1.424400
2707.0688054561615
[10,  5000] loss: 1.401464
2718.534848213196
[10,  5500] loss: 1.427900
2730.0274584293365
[10,  6000] loss: 1.463757
2741.6201791763306
[10,  6500] loss: 1.420931
2753.103130340576
[10,  7000] loss: 1.418017
2764.5203955173492
[10,  7500] loss: 1.436669
2776.009575843811
[10,  8000] loss: 1.433494
2787.580759525299
[10,  8500] loss: 1.433823
2799.230532169342
[10,  9000] loss: 1.426063
2810.6891894340515
[10,  9500] loss: 1.424896
2822.2342245578766
[10, 10000] loss: 1.424974
2833.766112089157
[10, 10500] loss: 1.423790
2845.372122526169
[10, 11000] loss: 1.455137
2856.891254425049
[10, 11500] loss: 1.434133
2868.4710750579834
[10, 12000] loss: 1.412207
2879.992607355118
[10, 12500] loss: 1.398124
2891.4834654331207
Epoch [10] loss: 4473270.430551
[11,   500] loss: 1.371623
2903.052815437317
[11,  1000] loss: 1.408547
2914.5411264896393
[11,  1500] loss: 1.353770
2925.9796566963196
[11,  2000] loss: 1.357152
2937.287971019745
[11,  2500] loss: 1.398156
2948.7171099185944
[11,  3000] loss: 1.429090
2960.242444753647
[11,  3500] loss: 1.399186
2971.781638145447
[11,  4000] loss: 1.410932
2983.32452249527
[11,  4500] loss: 1.386472
2995.105455160141
[11,  5000] loss: 1.388359
3006.6108124256134
[11,  5500] loss: 1.406724
3018.1379709243774
[11,  6000] loss: 1.404739
3029.6052837371826
[11,  6500] loss: 1.385689
3041.1415734291077
[11,  7000] loss: 1.410658
3052.6296446323395
[11,  7500] loss: 1.376281
3064.085414171219
[11,  8000] loss: 1.395159
3075.5931046009064
[11,  8500] loss: 1.397628
3087.084841966629
[11,  9000] loss: 1.376834
3098.493863105774
[11,  9500] loss: 1.383732
3110.0084998607635
[11, 10000] loss: 1.335208
3121.4791264533997
[11, 10500] loss: 1.362326
3132.9619977474213
[11, 11000] loss: 1.370124
3144.435056447983
[11, 11500] loss: 1.347409
3155.9572534561157
[11, 12000] loss: 1.378061
3167.4452850818634
[11, 12500] loss: 1.340189
3178.9738166332245
Epoch [11] loss: 4335664.621757
[12,   500] loss: 1.340518
3190.5810174942017
[12,  1000] loss: 1.320493
3201.9618718624115
[12,  1500] loss: 1.337822
3213.3265023231506
[12,  2000] loss: 1.332349
3224.8053872585297
[12,  2500] loss: 1.311069
3236.3565542697906
[12,  3000] loss: 1.320988
3247.650571346283
[12,  3500] loss: 1.367626
3259.111219406128
[12,  4000] loss: 1.380723
3270.5487089157104
[12,  4500] loss: 1.356097
3281.9815742969513
[12,  5000] loss: 1.288061
3293.457413673401
[12,  5500] loss: 1.312287
3304.8937883377075
[12,  6000] loss: 1.305475
3316.35857796669
[12,  6500] loss: 1.349359
3327.8501403331757
[12,  7000] loss: 1.341933
3339.276302099228
[12,  7500] loss: 1.286108
3350.6713075637817
[12,  8000] loss: 1.338716
3362.0764453411102
[12,  8500] loss: 1.310744
3373.4565773010254
[12,  9000] loss: 1.345708
3384.9616005420685
[12,  9500] loss: 1.297739
3396.663017511368
[12, 10000] loss: 1.325144
3408.277701854706
[12, 10500] loss: 1.361198
3419.788545370102
[12, 11000] loss: 1.320116
3431.3423941135406
[12, 11500] loss: 1.369242
3442.928215742111
[12, 12000] loss: 1.320602
3454.5246291160583
[12, 12500] loss: 1.346017
3465.964689731598
Epoch [12] loss: 4175913.819740
[13,   500] loss: 1.278540
3477.56116437912
[13,  1000] loss: 1.278333
3489.0793402194977
[13,  1500] loss: 1.253009
3500.6642701625824
[13,  2000] loss: 1.370054
3512.3042538166046
[13,  2500] loss: 1.276276
3523.8779907226562
[13,  3000] loss: 1.296734
3535.392033100128
[13,  3500] loss: 1.288104
3546.960381746292
[13,  4000] loss: 1.300497
3558.462415456772
[13,  4500] loss: 1.275476
3570.0067048072815
[13,  5000] loss: 1.309357
3581.5454733371735
[13,  5500] loss: 1.285938
3593.0286588668823
[13,  6000] loss: 1.290516
3604.426480770111
[13,  6500] loss: 1.293850
3615.877028942108
[13,  7000] loss: 1.298473
3627.286900997162
[13,  7500] loss: 1.316393
3638.745002746582
[13,  8000] loss: 1.290546
3650.136335134506
[13,  8500] loss: 1.355639
3661.631931066513
[13,  9000] loss: 1.315863
3673.126536130905
[13,  9500] loss: 1.317776
3684.6597757339478
[13, 10000] loss: 1.282014
3696.0652680397034
[13, 10500] loss: 1.262847
3707.3349289894104
[13, 11000] loss: 1.295414
3718.5989701747894
[13, 11500] loss: 1.339613
3730.0081346035004
[13, 12000] loss: 1.220743
3741.4781975746155
[13, 12500] loss: 1.289528
3754.6767992973328
Epoch [13] loss: 4073098.002512
[14,   500] loss: 1.293734
3767.7024042606354
[14,  1000] loss: 1.257442
3779.2121720314026
[14,  1500] loss: 1.248455
3790.6514954566956
[14,  2000] loss: 1.300881
3802.0793833732605
[14,  2500] loss: 1.247600
3813.5649943351746
[14,  3000] loss: 1.248105
3825.1413514614105
[14,  3500] loss: 1.281419
3836.6409361362457
[14,  4000] loss: 1.304746
3848.1767239570618
[14,  4500] loss: 1.242577
3859.626240491867
[14,  5000] loss: 1.243352
3871.133165359497
[14,  5500] loss: 1.270436
3882.8990767002106
[14,  6000] loss: 1.245607
3894.264174222946
[14,  6500] loss: 1.284818
3905.741413831711
[14,  7000] loss: 1.230673
3917.3735988140106
[14,  7500] loss: 1.232602
3928.871104001999
[14,  8000] loss: 1.256452
3940.173019886017
[14,  8500] loss: 1.265043
3951.5373816490173
[14,  9000] loss: 1.250923
3963.1113097667694
[14,  9500] loss: 1.200181
3974.6981778144836
[14, 10000] loss: 1.265875
3986.1545662879944
[14, 10500] loss: 1.229089
3997.7178931236267
[14, 11000] loss: 1.207454
4009.1735846996307
[14, 11500] loss: 1.272775
4020.4969515800476
[14, 12000] loss: 1.229833
4031.9794075489044
[14, 12500] loss: 1.265307
4043.277049303055
Epoch [14] loss: 3918155.230065
[15,   500] loss: 1.201045
4054.8187606334686
[15,  1000] loss: 1.214346
4066.234727859497
[15,  1500] loss: 1.265790
4077.693902015686
[15,  2000] loss: 1.185382
4089.271791934967
[15,  2500] loss: 1.191459
4100.785094738007
[15,  3000] loss: 1.229348
4112.269179821014
[15,  3500] loss: 1.206644
4123.696651935577
[15,  4000] loss: 1.220023
4135.277647018433
[15,  4500] loss: 1.218836
4146.708652496338
[15,  5000] loss: 1.158007
4158.174862861633
[15,  5500] loss: 1.237341
4169.678640365601
[15,  6000] loss: 1.230472
4181.154106616974
[15,  6500] loss: 1.225130
4192.700926542282
[15,  7000] loss: 1.215597
4204.330100774765
[15,  7500] loss: 1.250482
4215.884598016739
[15,  8000] loss: 1.211088
4227.379478693008
[15,  8500] loss: 1.230076
4238.951884031296
[15,  9000] loss: 1.215929
4250.46043586731
[15,  9500] loss: 1.177275
4261.92128610611
[15, 10000] loss: 1.203053
4273.399092197418
[15, 10500] loss: 1.222701
4284.894175291061
[15, 11000] loss: 1.198758
4296.408789634705
[15, 11500] loss: 1.198275
4307.860459804535
[15, 12000] loss: 1.152892
4319.244797706604
[15, 12500] loss: 1.232096
4330.639689922333
Epoch [15] loss: 3787597.208165
[16,   500] loss: 1.171714
4343.8603951931
[16,  1000] loss: 1.111039
4355.390444278717
[16,  1500] loss: 1.137955
4366.816322565079
[16,  2000] loss: 1.174933
4378.2797746658325
[16,  2500] loss: 1.193748
4389.790567398071
[16,  3000] loss: 1.183796
4401.211105108261
[16,  3500] loss: 1.192414
4412.589954614639
[16,  4000] loss: 1.188023
4424.126358985901
[16,  4500] loss: 1.188761
4435.6078679561615
[16,  5000] loss: 1.159918
4447.106747865677
[16,  5500] loss: 1.154384
4458.698922634125
[16,  6000] loss: 1.174293
4470.212270498276
[16,  6500] loss: 1.196654
4481.644597530365
[16,  7000] loss: 1.163987
4493.039209604263
[16,  7500] loss: 1.178089
4504.44935464859
[16,  8000] loss: 1.177598
4515.928146123886
[16,  8500] loss: 1.189118
4527.463317632675
[16,  9000] loss: 1.154979
4538.9470999240875
[16,  9500] loss: 1.162231
4550.463921070099
[16, 10000] loss: 1.200342
4562.05305814743
[16, 10500] loss: 1.160807
4573.565902471542
[16, 11000] loss: 1.168964
4585.142736911774
[16, 11500] loss: 1.180415
4596.690864562988
[16, 12000] loss: 1.164922
4608.021893978119
[16, 12500] loss: 1.198716
4619.433028936386
Epoch [16] loss: 3673035.776044
[17,   500] loss: 1.105837
4631.040137290955
[17,  1000] loss: 1.184905
4642.521327495575
[17,  1500] loss: 1.094981
4653.9353103637695
[17,  2000] loss: 1.116835
4665.404014825821
[17,  2500] loss: 1.117066
4676.8937792778015
[17,  3000] loss: 1.144342
4688.399375915527
[17,  3500] loss: 1.171576
4699.818797826767
[17,  4000] loss: 1.112467
4711.163934707642
[17,  4500] loss: 1.170679
4722.615335702896
[17,  5000] loss: 1.153232
4734.084211349487
[17,  5500] loss: 1.138512
4745.534171581268
[17,  6000] loss: 1.131985
4757.072297811508
[17,  6500] loss: 1.153672
4768.651655912399
[17,  7000] loss: 1.116510
4780.179682970047
[17,  7500] loss: 1.172367
4791.610062122345
[17,  8000] loss: 1.159268
4802.953467130661
[17,  8500] loss: 1.153300
4814.376335382462
[17,  9000] loss: 1.142642
4825.840177059174
[17,  9500] loss: 1.182832
4837.262127161026
[17, 10000] loss: 1.089930
4848.668548822403
[17, 10500] loss: 1.154751
4860.124851703644
[17, 11000] loss: 1.168395
4871.590350627899
[17, 11500] loss: 1.123687
4883.04340171814
[17, 12000] loss: 1.101197
4894.443188667297
[17, 12500] loss: 1.106624
4905.919629573822
Epoch [17] loss: 3560801.389029
[18,   500] loss: 1.070392
4917.434157371521
[18,  1000] loss: 1.046378
4928.906033277512
[18,  1500] loss: 1.128099
4940.422423124313
[18,  2000] loss: 1.101511
4951.958796739578
[18,  2500] loss: 1.092977
4963.460587024689
[18,  3000] loss: 1.062070
4974.885503292084
[18,  3500] loss: 1.095292
4986.435947418213
[18,  4000] loss: 1.088514
4997.932171344757
[18,  4500] loss: 1.086950
5009.391238689423
[18,  5000] loss: 1.137209
5020.954329490662
[18,  5500] loss: 1.104911
5032.423596382141
[18,  6000] loss: 1.099768
5043.875578165054
[18,  6500] loss: 1.130037
5055.309651851654
[18,  7000] loss: 1.101142
5066.879012823105
[18,  7500] loss: 1.125440
5078.324505805969
[18,  8000] loss: 1.106788
5089.893744945526
[18,  8500] loss: 1.076830
5101.484547138214
[18,  9000] loss: 1.110209
5113.119412660599
[18,  9500] loss: 1.153386
5124.53783249855
[18, 10000] loss: 1.160053
5136.026068210602
[18, 10500] loss: 1.130917
5147.483162403107
[18, 11000] loss: 1.120995
5159.028665304184
[18, 11500] loss: 1.118567
5170.500158548355
[18, 12000] loss: 1.055125
5182.085718154907
[18, 12500] loss: 1.092058
5193.602910041809
Epoch [18] loss: 3460684.385841
[19,   500] loss: 1.067340
5205.262055635452
[19,  1000] loss: 1.050411
5216.756140232086
[19,  1500] loss: 1.043801
5228.331879377365
[19,  2000] loss: 1.081108
5239.878997802734
[19,  2500] loss: 1.032704
5251.570341587067
[19,  3000] loss: 1.083937
5263.157861948013
[19,  3500] loss: 1.067988
5274.553147554398
[19,  4000] loss: 1.054945
5286.011326789856
[19,  4500] loss: 1.049061
5297.426351308823
[19,  5000] loss: 1.094912
5308.748872756958
[19,  5500] loss: 1.030033
5320.21951675415
[19,  6000] loss: 1.075492
5331.704322814941
[19,  6500] loss: 1.062270
5343.235596418381
[19,  7000] loss: 1.095674
5354.928170204163
[19,  7500] loss: 1.069115
5366.437832832336
[19,  8000] loss: 1.012064
5377.82089972496
[19,  8500] loss: 1.067679
5389.283894062042
[19,  9000] loss: 1.075006
5400.706108570099
[19,  9500] loss: 1.083213
5412.244803190231
[19, 10000] loss: 1.074206
5423.7234082221985
[19, 10500] loss: 1.027854
5435.166797637939
[19, 11000] loss: 1.068876
5446.759246110916
[19, 11500] loss: 1.048903
5458.238346815109
[19, 12000] loss: 1.091753
5469.714440345764
[19, 12500] loss: 1.057501
5481.298428058624
Epoch [19] loss: 3314907.473407
[20,   500] loss: 0.946357
5493.014230251312
[20,  1000] loss: 0.994761
5504.591831684113
[20,  1500] loss: 1.011878
5516.091345310211
[20,  2000] loss: 1.053567
5527.534931182861
[20,  2500] loss: 1.026315
5538.999084472656
[20,  3000] loss: 1.000194
5550.409340143204
[20,  3500] loss: 1.025096
5561.976457357407
[20,  4000] loss: 1.026537
5573.486313343048
[20,  4500] loss: 1.019083
5584.923453330994
[20,  5000] loss: 1.033644
5596.363036870956
[20,  5500] loss: 1.004364
5607.750165224075
[20,  6000] loss: 1.050715
5619.11537361145
[20,  6500] loss: 1.017777
5630.517095088959
[20,  7000] loss: 1.047832
5641.837652683258
[20,  7500] loss: 1.062461
5653.188183069229
[20,  8000] loss: 0.981686
5664.752646684647
[20,  8500] loss: 1.026856
5676.1307055950165
[20,  9000] loss: 1.009444
5687.672050952911
[20,  9500] loss: 1.053905
5699.139404296875
[20, 10000] loss: 1.050686
5710.559131860733
[20, 10500] loss: 0.987762
5722.014182329178
[20, 11000] loss: 1.003111
5733.517701387405
[20, 11500] loss: 0.991624
5744.997414588928
[20, 12000] loss: 1.071401
5756.346772193909
[20, 12500] loss: 1.061364
5767.82816195488
Epoch [20] loss: 3187693.150907
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-25 02:09:08.626666
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[ 0.3853, -2.2266,  1.0900,  3.8202, -1.1335,  2.8915, -2.8359,  0.7268,
         -0.3137, -2.4538],
        [ 2.3789,  4.0936, -1.5357, -1.6482, -2.4759, -2.8083, -3.8768, -3.0602,
          3.5813,  0.7612],
        [ 2.1317, -1.3392,  0.4426, -0.2348, -0.7930, -0.3119, -3.1329, -0.8818,
          3.2342, -1.8274],
        [ 4.4496,  1.0425,  0.4559, -1.2258, -0.1757, -3.4275, -1.9596, -1.2085,
          1.1295, -0.3675]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat   car  ship plane
Accuracy of the network on the 4000.0 test images: 58 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': True, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, load_from_memory=False, model_archi=16, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=True, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 ship  frog plane  ship
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582593000.5015447
[1,   500] loss: 2.256904
11.671228408813477
[1,  1000] loss: 2.111976
23.263169765472412
[1,  1500] loss: 2.096960
34.7997350692749
[1,  2000] loss: 2.037370
46.27491545677185
[1,  2500] loss: 2.030956
57.76408934593201
[1,  3000] loss: 1.974546
69.30206489562988
[1,  3500] loss: 1.923823
80.86122989654541
[1,  4000] loss: 1.921679
92.44901919364929
[1,  4500] loss: 1.844065
104.04799938201904
[1,  5000] loss: 1.868875
115.6657464504242
[1,  5500] loss: 1.838194
127.40863823890686
[1,  6000] loss: 1.859444
139.14754819869995
[1,  6500] loss: 1.778811
150.93028616905212
[1,  7000] loss: 1.747667
162.59792709350586
[1,  7500] loss: 1.720698
174.2077956199646
[1,  8000] loss: 1.699016
185.82780933380127
[1,  8500] loss: 1.697593
197.304851770401
[1,  9000] loss: 1.629686
208.81696915626526
[1,  9500] loss: 1.682040
220.37264919281006
[1, 10000] loss: 1.643602
232.0376377105713
[1, 10500] loss: 1.573305
243.84762406349182
[1, 11000] loss: 1.570458
255.54657411575317
[1, 11500] loss: 1.548863
267.13115525245667
[1, 12000] loss: 1.551855
278.7269778251648
[1, 12500] loss: 1.532968
290.28857827186584
Epoch [1] loss: 5672837.773365
[2,   500] loss: 1.484430
302.0226731300354
[2,  1000] loss: 1.555284
313.4591724872589
[2,  1500] loss: 1.483853
324.931932926178
[2,  2000] loss: 1.433761
336.54343938827515
[2,  2500] loss: 1.432492
348.0966672897339
[2,  3000] loss: 1.469862
359.6485571861267
[2,  3500] loss: 1.418518
371.23793363571167
[2,  4000] loss: 1.425093
382.79761695861816
[2,  4500] loss: 1.408884
394.3126742839813
[2,  5000] loss: 1.401337
405.8998055458069
[2,  5500] loss: 1.389561
417.3747057914734
[2,  6000] loss: 1.386500
428.8958537578583
[2,  6500] loss: 1.346247
440.44182443618774
[2,  7000] loss: 1.382467
452.0888993740082
[2,  7500] loss: 1.295816
463.73710799217224
[2,  8000] loss: 1.276310
475.41538310050964
[2,  8500] loss: 1.332120
487.0473573207855
[2,  9000] loss: 1.279224
498.5880272388458
[2,  9500] loss: 1.329151
510.19994020462036
[2, 10000] loss: 1.281302
521.844170331955
[2, 10500] loss: 1.288864
533.591078042984
[2, 11000] loss: 1.257907
545.1603674888611
[2, 11500] loss: 1.258398
556.8376336097717
[2, 12000] loss: 1.262417
568.4270198345184
[2, 12500] loss: 1.233027
580.2397770881653
Epoch [2] loss: 4278826.816354
[3,   500] loss: 1.196130
592.0541400909424
[3,  1000] loss: 1.203532
603.6562063694
[3,  1500] loss: 1.194891
615.2341799736023
[3,  2000] loss: 1.218169
626.8145859241486
[3,  2500] loss: 1.179804
638.3073351383209
[3,  3000] loss: 1.176237
649.7894215583801
[3,  3500] loss: 1.178732
661.3205993175507
[3,  4000] loss: 1.132120
672.8365392684937
[3,  4500] loss: 1.154964
684.2832658290863
[3,  5000] loss: 1.166661
695.9969613552094
[3,  5500] loss: 1.155342
707.6029808521271
[3,  6000] loss: 1.115059
719.1340491771698
[3,  6500] loss: 1.119739
730.8367698192596
[3,  7000] loss: 1.066232
742.4411945343018
[3,  7500] loss: 1.090876
754.068026304245
[3,  8000] loss: 1.151881
765.7234768867493
[3,  8500] loss: 1.133234
777.4306874275208
[3,  9000] loss: 1.101894
789.0852425098419
[3,  9500] loss: 1.119638
800.6625318527222
[3, 10000] loss: 1.108650
812.1999223232269
[3, 10500] loss: 1.101463
823.6676287651062
[3, 11000] loss: 1.081596
835.2581105232239
[3, 11500] loss: 1.086099
846.7692713737488
[3, 12000] loss: 1.067214
858.2521252632141
[3, 12500] loss: 1.041775
869.8427577018738
Epoch [3] loss: 3533251.344675
[4,   500] loss: 1.023283
881.5646097660065
[4,  1000] loss: 1.078214
893.2824819087982
[4,  1500] loss: 1.018370
904.9676144123077
[4,  2000] loss: 1.031533
916.5557291507721
[4,  2500] loss: 1.000104
928.0773150920868
[4,  3000] loss: 1.071688
939.68377161026
[4,  3500] loss: 1.041359
951.3881151676178
[4,  4000] loss: 0.972494
963.0058760643005
[4,  4500] loss: 0.978767
974.7323625087738
[4,  5000] loss: 0.987117
986.4134182929993
[4,  5500] loss: 1.049949
998.1409018039703
[4,  6000] loss: 0.943209
1009.7674736976624
[4,  6500] loss: 1.008979
1021.5314452648163
[4,  7000] loss: 0.957587
1033.2274236679077
[4,  7500] loss: 0.973398
1044.8674309253693
[4,  8000] loss: 0.978154
1056.4871821403503
[4,  8500] loss: 0.962759
1068.0699360370636
[4,  9000] loss: 0.981484
1079.629534482956
[4,  9500] loss: 0.992602
1091.159338235855
[4, 10000] loss: 0.903386
1102.7491431236267
[4, 10500] loss: 0.993729
1114.3543965816498
[4, 11000] loss: 0.929342
1125.932578086853
[4, 11500] loss: 0.969190
1137.546977519989
[4, 12000] loss: 0.928298
1149.152180671692
[4, 12500] loss: 0.929603
1160.661963224411
Epoch [4] loss: 3092623.350304
[5,   500] loss: 0.892184
1172.3389866352081
[5,  1000] loss: 0.851603
1183.9506702423096
[5,  1500] loss: 0.891241
1195.5169041156769
[5,  2000] loss: 0.914991
1207.1206142902374
[5,  2500] loss: 0.878509
1218.7511854171753
[5,  3000] loss: 0.850835
1230.2545146942139
[5,  3500] loss: 0.864430
1241.9140763282776
[5,  4000] loss: 0.892900
1253.7580361366272
[5,  4500] loss: 0.905704
1265.645886182785
[5,  5000] loss: 0.887090
1277.2605681419373
[5,  5500] loss: 0.847177
1288.9034090042114
[5,  6000] loss: 0.927621
1300.5047118663788
[5,  6500] loss: 0.866645
1312.1610321998596
[5,  7000] loss: 0.830451
1323.7944383621216
[5,  7500] loss: 0.899937
1335.5955469608307
[5,  8000] loss: 0.903997
1347.2388107776642
[5,  8500] loss: 0.837429
1358.9085812568665
[5,  9000] loss: 0.857237
1370.6807622909546
[5,  9500] loss: 0.842303
1382.321012020111
[5, 10000] loss: 0.864551
1393.8928513526917
[5, 10500] loss: 0.850598
1405.4392018318176
[5, 11000] loss: 0.789133
1417.0822186470032
[5, 11500] loss: 0.857746
1428.7149057388306
[5, 12000] loss: 0.867973
1440.4040186405182
[5, 12500] loss: 0.834732
1452.1248757839203
Epoch [5] loss: 2721287.485163
[6,   500] loss: 0.808372
1463.901445388794
[6,  1000] loss: 0.809153
1475.5062792301178
[6,  1500] loss: 0.796468
1487.1476771831512
[6,  2000] loss: 0.767981
1498.681602716446
[6,  2500] loss: 0.768193
1510.4625346660614
[6,  3000] loss: 0.835073
1522.105236530304
[6,  3500] loss: 0.762225
1533.747407913208
[6,  4000] loss: 0.747397
1545.381607055664
[6,  4500] loss: 0.743148
1556.9533417224884
[6,  5000] loss: 0.753644
1568.5714445114136
[6,  5500] loss: 0.792342
1580.2182807922363
[6,  6000] loss: 0.737967
1591.752114057541
[6,  6500] loss: 0.779739
1603.3171634674072
[6,  7000] loss: 0.800290
1614.8719327449799
[6,  7500] loss: 0.798265
1626.5087065696716
[6,  8000] loss: 0.724161
1638.190060377121
[6,  8500] loss: 0.757003
1649.933524608612
[6,  9000] loss: 0.743298
1661.754454612732
[6,  9500] loss: 0.785481
1673.4126634597778
[6, 10000] loss: 0.795682
1685.0806999206543
[6, 10500] loss: 0.761752
1696.8286600112915
[6, 11000] loss: 0.765355
1708.6800010204315
[6, 11500] loss: 0.723671
1720.3836107254028
[6, 12000] loss: 0.736042
1732.0476999282837
[6, 12500] loss: 0.734437
1743.6919560432434
Epoch [6] loss: 2408421.252361
[7,   500] loss: 0.737154
1755.3649916648865
[7,  1000] loss: 0.694694
1766.8459599018097
[7,  1500] loss: 0.719760
1778.368543624878
[7,  2000] loss: 0.707394
1789.9876189231873
[7,  2500] loss: 0.697019
1801.6732246875763
[7,  3000] loss: 0.687235
1813.2424178123474
[7,  3500] loss: 0.739387
1824.8808119297028
[7,  4000] loss: 0.702702
1836.7419023513794
[7,  4500] loss: 0.679411
1848.3913838863373
[7,  5000] loss: 0.712681
1859.9659843444824
[7,  5500] loss: 0.684177
1871.6034495830536
[7,  6000] loss: 0.700243
1883.1385517120361
[7,  6500] loss: 0.743722
1894.6584684848785
[7,  7000] loss: 0.693142
1906.1871659755707
[7,  7500] loss: 0.673726
1917.8697776794434
[7,  8000] loss: 0.702210
1929.6739358901978
[7,  8500] loss: 0.655009
1941.2591876983643
[7,  9000] loss: 0.723089
1952.9245462417603
[7,  9500] loss: 0.703051
1964.5245504379272
[7, 10000] loss: 0.704779
1976.0351209640503
[7, 10500] loss: 0.735135
1987.9406716823578
[7, 11000] loss: 0.680296
1999.6969351768494
[7, 11500] loss: 0.687274
2011.296249628067
[7, 12000] loss: 0.623356
2022.8151688575745
[7, 12500] loss: 0.701935
2034.3451657295227
Epoch [7] loss: 2190539.909471
[8,   500] loss: 0.654639
2046.0302515029907
[8,  1000] loss: 0.598922
2057.5662631988525
[8,  1500] loss: 0.643864
2069.1799323558807
[8,  2000] loss: 0.606803
2080.725264310837
[8,  2500] loss: 0.649206
2092.277729988098
[8,  3000] loss: 0.661746
2103.919047355652
[8,  3500] loss: 0.699654
2115.5766925811768
[8,  4000] loss: 0.620413
2127.1930527687073
[8,  4500] loss: 0.629535
2138.715378522873
[8,  5000] loss: 0.646037
2150.209137439728
[8,  5500] loss: 0.619489
2161.8511667251587
[8,  6000] loss: 0.643981
2173.4239497184753
[8,  6500] loss: 0.663058
2184.9144263267517
[8,  7000] loss: 0.602025
2196.4362404346466
[8,  7500] loss: 0.660070
2208.040546655655
[8,  8000] loss: 0.619803
2219.503505706787
[8,  8500] loss: 0.657148
2231.019513607025
[8,  9000] loss: 0.634177
2242.5751147270203
[8,  9500] loss: 0.611915
2254.1672818660736
[8, 10000] loss: 0.586941
2265.8163549900055
[8, 10500] loss: 0.606745
2277.4303591251373
[8, 11000] loss: 0.646140
2289.074584722519
[8, 11500] loss: 0.665441
2300.731048345566
[8, 12000] loss: 0.604632
2312.442834377289
[8, 12500] loss: 0.667210
2324.090476512909
Epoch [8] loss: 1972513.473869
[9,   500] loss: 0.591703
2335.8834235668182
[9,  1000] loss: 0.537248
2347.4400396347046
[9,  1500] loss: 0.554219
2359.0535006523132
[9,  2000] loss: 0.584337
2370.7203080654144
[9,  2500] loss: 0.589037
2382.2586464881897
[9,  3000] loss: 0.592399
2393.752774953842
[9,  3500] loss: 0.630320
2405.365535259247
[9,  4000] loss: 0.576285
2416.8794662952423
[9,  4500] loss: 0.585659
2428.416585445404
[9,  5000] loss: 0.624111
2439.9246039390564
[9,  5500] loss: 0.587512
2451.6006944179535
[9,  6000] loss: 0.570532
2463.1487171649933
[9,  6500] loss: 0.559135
2474.828179359436
[9,  7000] loss: 0.552797
2486.4647579193115
[9,  7500] loss: 0.592189
2498.0832481384277
[9,  8000] loss: 0.587862
2509.682259082794
[9,  8500] loss: 0.560753
2521.3135249614716
[9,  9000] loss: 0.646157
2532.932158946991
[9,  9500] loss: 0.559808
2544.6050798892975
[9, 10000] loss: 0.588461
2556.320974588394
[9, 10500] loss: 0.589833
2567.933665752411
[9, 11000] loss: 0.573400
2579.629982948303
[9, 11500] loss: 0.556387
2591.252138853073
[9, 12000] loss: 0.635097
2602.777652978897
[9, 12500] loss: 0.599052
2614.39377617836
Epoch [9] loss: 1838452.533963
[10,   500] loss: 0.541682
2626.2037394046783
[10,  1000] loss: 0.553156
2637.8653852939606
[10,  1500] loss: 0.512801
2649.364013195038
[10,  2000] loss: 0.545652
2660.85773730278
[10,  2500] loss: 0.515085
2672.3138086795807
[10,  3000] loss: 0.513587
2683.8176009655
[10,  3500] loss: 0.444573
2695.377986431122
[10,  4000] loss: 0.512636
2706.964272260666
[10,  4500] loss: 0.509053
2718.6025490760803
[10,  5000] loss: 0.479432
2730.192113876343
[10,  5500] loss: 0.519681
2742.0088703632355
[10,  6000] loss: 0.534571
2753.6848800182343
[10,  6500] loss: 0.537460
2765.382607936859
[10,  7000] loss: 0.548274
2777.19597363472
[10,  7500] loss: 0.515010
2788.8184609413147
[10,  8000] loss: 0.506279
2800.483628988266
[10,  8500] loss: 0.574589
2812.1503398418427
[10,  9000] loss: 0.535063
2823.893300294876
[10,  9500] loss: 0.521734
2835.6345064640045
[10, 10000] loss: 0.532946
2847.203046321869
[10, 10500] loss: 0.518258
2858.9059619903564
[10, 11000] loss: 0.532725
2870.5479662418365
[10, 11500] loss: 0.515280
2882.1462438106537
[10, 12000] loss: 0.514240
2893.7841324806213
[10, 12500] loss: 0.502526
2905.47944188118
Epoch [10] loss: 1625426.479822
[11,   500] loss: 0.464905
2917.1880502700806
[11,  1000] loss: 0.475512
2928.657085418701
[11,  1500] loss: 0.428049
2940.3306016921997
[11,  2000] loss: 0.513256
2952.095381259918
[11,  2500] loss: 0.463141
2963.762840270996
[11,  3000] loss: 0.494690
2975.4087195396423
[11,  3500] loss: 0.441302
2987.066106557846
[11,  4000] loss: 0.491907
2998.655221939087
[11,  4500] loss: 0.498229
3010.30136179924
[11,  5000] loss: 0.439185
3021.8251304626465
[11,  5500] loss: 0.493049
3033.4855377674103
[11,  6000] loss: 0.449280
3045.151586532593
[11,  6500] loss: 0.460378
3056.8991117477417
[11,  7000] loss: 0.454436
3068.565586566925
[11,  7500] loss: 0.474497
3080.2776033878326
[11,  8000] loss: 0.485274
3092.0890743732452
[11,  8500] loss: 0.480515
3103.870267391205
[11,  9000] loss: 0.505419
3115.7899148464203
[11,  9500] loss: 0.502251
3127.659152984619
[11, 10000] loss: 0.481449
3139.344671726227
[11, 10500] loss: 0.505767
3150.952523469925
[11, 11000] loss: 0.504141
3162.5478463172913
[11, 11500] loss: 0.459092
3174.126562356949
[11, 12000] loss: 0.490016
3185.8077676296234
[11, 12500] loss: 0.532424
3197.4448778629303
Epoch [11] loss: 1517871.079432
[12,   500] loss: 0.420458
3209.062322616577
[12,  1000] loss: 0.446351
3220.61478638649
[12,  1500] loss: 0.432528
3232.2902257442474
[12,  2000] loss: 0.389885
3243.7905118465424
[12,  2500] loss: 0.428019
3255.368882417679
[12,  3000] loss: 0.444682
3267.0153019428253
[12,  3500] loss: 0.414389
3278.6190094947815
[12,  4000] loss: 0.428282
3290.366012096405
[12,  4500] loss: 0.466417
3302.3232386112213
[12,  5000] loss: 0.442204
3314.05379652977
[12,  5500] loss: 0.421639
3325.8032445907593
[12,  6000] loss: 0.455292
3337.4932832717896
[12,  6500] loss: 0.435370
3348.9866712093353
[12,  7000] loss: 0.451881
3360.486364364624
[12,  7500] loss: 0.420955
3372.06591296196
[12,  8000] loss: 0.438278
3383.5078463554382
[12,  8500] loss: 0.444172
3395.125079870224
[12,  9000] loss: 0.421438
3406.6185562610626
[12,  9500] loss: 0.469900
3418.1486105918884
[12, 10000] loss: 0.426396
3429.6872289180756
[12, 10500] loss: 0.441554
3441.326593875885
[12, 11000] loss: 0.431538
3452.9996333122253
[12, 11500] loss: 0.447292
3464.5708858966827
[12, 12000] loss: 0.435681
3475.988089799881
[12, 12500] loss: 0.426088
3487.4607129096985
Epoch [12] loss: 1360951.548393
[13,   500] loss: 0.367075
3499.171686887741
[13,  1000] loss: 0.374902
3510.6795785427094
[13,  1500] loss: 0.400006
3522.24805188179
[13,  2000] loss: 0.407710
3533.832747936249
[13,  2500] loss: 0.400811
3545.4443151950836
[13,  3000] loss: 0.356593
3557.0223989486694
[13,  3500] loss: 0.419475
3568.669076681137
[13,  4000] loss: 0.400984
3580.191220521927
[13,  4500] loss: 0.393150
3591.7423672676086
[13,  5000] loss: 0.390460
3603.328722000122
[13,  5500] loss: 0.426450
3614.8991763591766
[13,  6000] loss: 0.428797
3626.5153539180756
[13,  6500] loss: 0.395043
3638.063283443451
[13,  7000] loss: 0.461893
3649.7487528324127
[13,  7500] loss: 0.398115
3661.3726930618286
[13,  8000] loss: 0.354004
3673.0675597190857
[13,  8500] loss: 0.414142
3684.8709285259247
[13,  9000] loss: 0.452778
3696.4681828022003
[13,  9500] loss: 0.405965
3708.0064487457275
[13, 10000] loss: 0.408454
3719.6613426208496
[13, 10500] loss: 0.389725
3731.289637327194
[13, 11000] loss: 0.431408
3742.9497151374817
[13, 11500] loss: 0.402815
3754.634774684906
[13, 12000] loss: 0.421891
3766.405102491379
[13, 12500] loss: 0.408540
3777.9878923892975
Epoch [13] loss: 1274811.912676
[14,   500] loss: 0.350070
3789.800594329834
[14,  1000] loss: 0.355673
3801.413319349289
[14,  1500] loss: 0.338238
3813.025357723236
[14,  2000] loss: 0.365346
3824.6182794570923
[14,  2500] loss: 0.383450
3836.302039384842
[14,  3000] loss: 0.362487
3847.933418035507
[14,  3500] loss: 0.379550
3859.551830291748
[14,  4000] loss: 0.341809
3871.1266555786133
[14,  4500] loss: 0.352470
3884.6836190223694
[14,  5000] loss: 0.373519
3898.283057451248
[14,  5500] loss: 0.314239
3911.0922853946686
[14,  6000] loss: 0.375644
3922.654940843582
[14,  6500] loss: 0.408144
3934.1798388957977
[14,  7000] loss: 0.366621
3945.8044612407684
[14,  7500] loss: 0.347606
3957.4191977977753
[14,  8000] loss: 0.329316
3969.0282785892487
[14,  8500] loss: 0.366255
3980.541183233261
[14,  9000] loss: 0.370406
3992.111640930176
[14,  9500] loss: 0.357559
4003.651441335678
[14, 10000] loss: 0.399179
4015.292687177658
[14, 10500] loss: 0.356836
4026.9023423194885
[14, 11000] loss: 0.385750
4038.4652709960938
[14, 11500] loss: 0.387404
4049.9997849464417
[14, 12000] loss: 0.392129
4061.5349855422974
[14, 12500] loss: 0.364785
4073.0872116088867
Epoch [14] loss: 1128639.802037
[15,   500] loss: 0.311069
4085.5061655044556
[15,  1000] loss: 0.339156
4097.2228536605835
[15,  1500] loss: 0.308771
4108.759387731552
[15,  2000] loss: 0.314548
4120.419054269791
[15,  2500] loss: 0.307247
4132.118540287018
[15,  3000] loss: 0.306661
4143.635812044144
[15,  3500] loss: 0.329243
4155.277476787567
[15,  4000] loss: 0.323227
4167.020626068115
[15,  4500] loss: 0.320010
4178.757475614548
[15,  5000] loss: 0.311036
4190.399111509323
[15,  5500] loss: 0.330480
4202.048911809921
[15,  6000] loss: 0.354001
4213.62112402916
[15,  6500] loss: 0.324848
4225.222066164017
[15,  7000] loss: 0.314841
4236.846111774445
[15,  7500] loss: 0.338095
4248.428189754486
[15,  8000] loss: 0.350020
4260.125811100006
[15,  8500] loss: 0.347360
4271.6894788742065
[15,  9000] loss: 0.338521
4283.357304096222
[15,  9500] loss: 0.323136
4294.956310749054
[15, 10000] loss: 0.347302
4306.604134082794
[15, 10500] loss: 0.320187
4318.339692115784
[15, 11000] loss: 0.332800
4330.017274618149
[15, 11500] loss: 0.376467
4341.599203586578
[15, 12000] loss: 0.398066
4353.121597290039
[15, 12500] loss: 0.363712
4364.681092262268
Epoch [15] loss: 1048539.725077
[16,   500] loss: 0.291791
4376.5359427928925
[16,  1000] loss: 0.264823
4388.2084674835205
[16,  1500] loss: 0.309460
4399.892226219177
[16,  2000] loss: 0.298298
4411.559606790543
[16,  2500] loss: 0.300267
4423.230652809143
[16,  3000] loss: 0.295537
4434.844811201096
[16,  3500] loss: 0.283891
4446.596814155579
[16,  4000] loss: 0.282893
4458.258447170258
[16,  4500] loss: 0.321023
4469.945194482803
[16,  5000] loss: 0.281790
4481.476631641388
[16,  5500] loss: 0.316462
4493.059742927551
[16,  6000] loss: 0.330246
4504.758714675903
[16,  6500] loss: 0.292719
4516.415452480316
[16,  7000] loss: 0.312036
4528.033618450165
[16,  7500] loss: 0.312387
4539.764553308487
[16,  8000] loss: 0.315874
4551.305470705032
[16,  8500] loss: 0.299611
4562.946608781815
[16,  9000] loss: 0.329375
4574.634152889252
[16,  9500] loss: 0.344899
4586.249481916428
[16, 10000] loss: 0.309080
4597.858994722366
[16, 10500] loss: 0.342707
4609.449454545975
[16, 11000] loss: 0.295501
4621.269821405411
[16, 11500] loss: 0.325032
4632.998235940933
[16, 12000] loss: 0.307779
4644.665723562241
[16, 12500] loss: 0.280275
4656.276823997498
Epoch [16] loss: 946351.678564
[17,   500] loss: 0.233037
4668.064015388489
[17,  1000] loss: 0.243753
4679.716074466705
[17,  1500] loss: 0.258668
4691.308920621872
[17,  2000] loss: 0.236928
4702.909253358841
[17,  2500] loss: 0.279484
4714.576829910278
[17,  3000] loss: 0.269431
4726.2044467926025
[17,  3500] loss: 0.281535
4737.7840230464935
[17,  4000] loss: 0.304242
4749.537749528885
[17,  4500] loss: 0.301974
4761.195898532867
[17,  5000] loss: 0.288993
4772.845577955246
[17,  5500] loss: 0.282073
4784.50778222084
[17,  6000] loss: 0.239438
4796.225836515427
[17,  6500] loss: 0.277706
4807.853204250336
[17,  7000] loss: 0.285628
4819.461653709412
[17,  7500] loss: 0.323361
4830.956310272217
[17,  8000] loss: 0.282162
4842.472955465317
[17,  8500] loss: 0.338968
4854.09724855423
[17,  9000] loss: 0.307069
4865.667577505112
[17,  9500] loss: 0.271960
4877.284150600433
[17, 10000] loss: 0.297040
4888.895233869553
[17, 10500] loss: 0.235340
4900.533208847046
[17, 11000] loss: 0.323746
4912.080255746841
[17, 11500] loss: 0.282080
4923.662550210953
[17, 12000] loss: 0.284868
4935.174829483032
[17, 12500] loss: 0.296417
4946.698938846588
Epoch [17] loss: 885520.422397
[18,   500] loss: 0.220962
4958.498563289642
[18,  1000] loss: 0.271968
4970.127034902573
[18,  1500] loss: 0.224865
4982.070450305939
[18,  2000] loss: 0.198545
4993.908773899078
[18,  2500] loss: 0.231338
5005.616347312927
[18,  3000] loss: 0.227600
5017.252365112305
[18,  3500] loss: 0.210475
5028.77432847023
[18,  4000] loss: 0.228161
5040.286752700806
[18,  4500] loss: 0.251229
5051.779716968536
[18,  5000] loss: 0.233856
5063.447971582413
[18,  5500] loss: 0.232975
5075.027907133102
[18,  6000] loss: 0.262720
5086.544442176819
[18,  6500] loss: 0.280045
5098.225671052933
[18,  7000] loss: 0.270852
5109.839457273483
[18,  7500] loss: 0.253055
5121.508407831192
[18,  8000] loss: 0.217031
5133.197571992874
[18,  8500] loss: 0.256820
5144.881761789322
[18,  9000] loss: 0.275193
5156.516183137894
[18,  9500] loss: 0.280924
5168.217156171799
[18, 10000] loss: 0.262005
5180.012988805771
[18, 10500] loss: 0.262168
5191.785777568817
[18, 11000] loss: 0.294933
5203.611549854279
[18, 11500] loss: 0.257879
5215.29163312912
[18, 12000] loss: 0.249579
5226.904444932938
[18, 12500] loss: 0.251891
5238.4794754981995
Epoch [18] loss: 777182.873039
[19,   500] loss: 0.198597
5250.237168073654
[19,  1000] loss: 0.195206
5261.873402833939
[19,  1500] loss: 0.242654
5273.431807279587
[19,  2000] loss: 0.208285
5284.996528863907
[19,  2500] loss: 0.230758
5296.63950252533
[19,  3000] loss: 0.215921
5308.241300582886
[19,  3500] loss: 0.233388
5319.835898399353
[19,  4000] loss: 0.219439
5331.47962141037
[19,  4500] loss: 0.215440
5343.199481487274
[19,  5000] loss: 0.253327
5354.83375287056
[19,  5500] loss: 0.242136
5366.579116821289
[19,  6000] loss: 0.222692
5378.300448656082
[19,  6500] loss: 0.232172
5389.8828547000885
[19,  7000] loss: 0.243127
5401.499587774277
[19,  7500] loss: 0.230231
5413.016492605209
[19,  8000] loss: 0.218230
5424.676195859909
[19,  8500] loss: 0.207625
5436.177005767822
[19,  9000] loss: 0.253054
5447.708736896515
[19,  9500] loss: 0.253116
5459.232559204102
[19, 10000] loss: 0.245548
5471.050687551498
[19, 10500] loss: 0.230590
5482.743425607681
[19, 11000] loss: 0.229096
5494.431202411652
[19, 11500] loss: 0.276914
5506.10955953598
[19, 12000] loss: 0.258666
5517.610611915588
[19, 12500] loss: 0.262326
5529.135977983475
Epoch [19] loss: 725545.866675
[20,   500] loss: 0.200277
5540.885335683823
[20,  1000] loss: 0.170408
5552.381458997726
[20,  1500] loss: 0.168529
5563.911891222
[20,  2000] loss: 0.198520
5575.4806253910065
[20,  2500] loss: 0.186395
5587.088739871979
[20,  3000] loss: 0.204850
5598.601241827011
[20,  3500] loss: 0.225045
5610.136524438858
[20,  4000] loss: 0.198456
5621.705039024353
[20,  4500] loss: 0.227386
5633.284711122513
[20,  5000] loss: 0.223515
5644.872293949127
[20,  5500] loss: 0.213968
5656.457682132721
[20,  6000] loss: 0.180318
5667.98441529274
[20,  6500] loss: 0.259751
5679.658372879028
[20,  7000] loss: 0.212484
5691.664242982864
[20,  7500] loss: 0.228433
5703.358443021774
[20,  8000] loss: 0.253156
5714.979475975037
[20,  8500] loss: 0.222389
5726.646758794785
[20,  9000] loss: 0.239733
5738.2174434661865
[20,  9500] loss: 0.233175
5749.80607175827
[20, 10000] loss: 0.215736
5761.436959505081
[20, 10500] loss: 0.238492
5773.088483333588
[20, 11000] loss: 0.202087
5784.692653656006
[20, 11500] loss: 0.210448
5796.182029724121
[20, 12000] loss: 0.253832
5807.761985778809
[20, 12500] loss: 0.216374
5819.417578935623
Epoch [20] loss: 673009.253822
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-25 03:46:59.970734
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[ 0.1555, -3.3832, -1.4306,  6.5797, -0.0503,  1.9629, -1.0372, -1.0492,
         -1.4772, -1.3147],
        [ 0.4266,  2.5776, -0.4871, -0.8378, -0.7853, -3.1737, -1.9948, -4.2071,
          4.8872,  2.6048],
        [ 1.2009,  5.0008, -1.6075, -1.0445, -0.9296, -3.1683, -1.4926, -3.9500,
          4.6434,  0.3635],
        [ 6.2638, -2.6844,  0.2607, -0.3720, -0.7335, -3.1835, -2.2444, -2.7149,
          3.9352, -0.4591]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat  ship   car plane
Accuracy of the network on the 4000.0 test images: 75 %


###############################################################################
Peregrine Cluster
Job 9734793 for user 's4091221'
Finished at: Tue Feb 25 03:47:28 CET 2020

Job details:
============

Name                : resnet50.sh
User                : s4091221
Partition           : gpu
Nodes               : pg-gpu39
Cores               : 12
State               : COMPLETED
Submit              : 2020-02-24T23:58:18
Start               : 2020-02-25T00:31:50
End                 : 2020-02-25T03:47:28
Reserved walltime   : 15:00:00
Used walltime       : 03:15:38
Used CPU time       : 03:26:54 (efficiency:  8.81%)
% User (Computation): 97.10%
% System (I/O)      :  2.90%
Mem reserved        : 12000M/node
Max Mem used        : 2.95G (pg-gpu39)
Max Disk Write      : 274.99M (pg-gpu39)
Max Disk Read       : 1.06G (pg-gpu39)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
