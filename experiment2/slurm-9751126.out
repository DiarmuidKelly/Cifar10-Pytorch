Requirement already satisfied: torchvision in ./.local/lib/python3.7/site-packages (0.5.0)
Requirement already satisfied: six in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from torchvision) (1.12.0)
Requirement already satisfied: pillow>=4.1.1 in ./.local/lib/python3.7/site-packages (from torchvision) (7.0.0)
Requirement already satisfied: torch==1.4.0 in ./.local/lib/python3.7/site-packages (from torchvision) (1.4.0)
Requirement already satisfied: numpy in ./.local/lib/python3.7/site-packages (from torchvision) (1.18.1)
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': True, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=16, momentum=0, normalise=True, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
horse   dog  frog horse
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Number of parameters: 23528522 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582661895.302797
[1,   500] loss: 2.591436
11.631103515625
[1,  1000] loss: 2.541127
23.106924057006836
[1,  1500] loss: 2.524460
34.54243803024292
[1,  2000] loss: 2.496043
45.95982766151428
[1,  2500] loss: 2.506697
57.39881896972656
[1,  3000] loss: 2.475071
68.78920602798462
[1,  3500] loss: 2.481594
80.19843626022339
[1,  4000] loss: 2.429929
91.46970391273499
[1,  4500] loss: 2.456947
103.02156352996826
[1,  5000] loss: 2.406884
114.50719404220581
[1,  5500] loss: 2.408128
126.04496955871582
[1,  6000] loss: 2.378423
137.32955765724182
[1,  6500] loss: 2.380041
148.65780425071716
[1,  7000] loss: 2.326066
160.17203450202942
[1,  7500] loss: 2.358455
171.57954001426697
[1,  8000] loss: 2.355952
182.91495823860168
[1,  8500] loss: 2.370387
194.44141173362732
[1,  9000] loss: 2.338937
205.82603883743286
[1,  9500] loss: 2.356283
217.46810388565063
[1, 10000] loss: 2.319745
229.0126314163208
[1, 10500] loss: 2.267293
240.56522464752197
[1, 11000] loss: 2.275027
252.1806879043579
[1, 11500] loss: 2.263040
263.7226221561432
[1, 12000] loss: 2.250425
275.3332896232605
[1, 12500] loss: 2.266592
286.9766192436218
Epoch [1] loss: 7505555.001933
[2,   500] loss: 2.267632
298.8998050689697
[2,  1000] loss: 2.218179
310.71410393714905
[2,  1500] loss: 2.218243
322.46452498435974
[2,  2000] loss: 2.198908
334.2671320438385
[2,  2500] loss: 2.190337
346.1151239871979
[2,  3000] loss: 2.212741
357.7332592010498
[2,  3500] loss: 2.180246
369.4953622817993
[2,  4000] loss: 2.141591
381.2026786804199
[2,  4500] loss: 2.161512
392.757771730423
[2,  5000] loss: 2.168341
404.3841016292572
[2,  5500] loss: 2.165657
415.9582588672638
[2,  6000] loss: 2.120937
427.5306079387665
[2,  6500] loss: 2.124107
439.1309881210327
[2,  7000] loss: 2.148975
450.66009640693665
[2,  7500] loss: 2.132170
462.2399823665619
[2,  8000] loss: 2.118253
473.8259379863739
[2,  8500] loss: 2.097796
485.5136573314667
[2,  9000] loss: 2.104083
497.0397810935974
[2,  9500] loss: 2.046725
508.6379442214966
[2, 10000] loss: 2.088806
520.2502179145813
[2, 10500] loss: 2.062716
532.0607852935791
[2, 11000] loss: 2.078140
544.1046867370605
[2, 11500] loss: 2.044613
555.6781580448151
[2, 12000] loss: 2.089184
567.2222890853882
[2, 12500] loss: 2.077107
578.818062543869
Epoch [2] loss: 6713759.904349
[3,   500] loss: 2.051725
590.9324193000793
[3,  1000] loss: 2.002409
602.5644910335541
[3,  1500] loss: 2.025587
614.067296743393
[3,  2000] loss: 2.025294
625.9496767520905
[3,  2500] loss: 2.035399
637.6301028728485
[3,  3000] loss: 2.000771
649.1695368289948
[3,  3500] loss: 2.020597
660.7180910110474
[3,  4000] loss: 2.072901
672.3769364356995
[3,  4500] loss: 2.072225
684.0687747001648
[3,  5000] loss: 2.036645
695.6330602169037
[3,  5500] loss: 1.983636
707.1268815994263
[3,  6000] loss: 2.018113
718.8006501197815
[3,  6500] loss: 2.021325
730.37637758255
[3,  7000] loss: 1.993134
741.9360768795013
[3,  7500] loss: 1.968788
753.4176099300385
[3,  8000] loss: 1.915069
764.9941561222076
[3,  8500] loss: 1.973761
776.522979259491
[3,  9000] loss: 1.990771
788.195125579834
[3,  9500] loss: 1.961411
799.6722271442413
[3, 10000] loss: 1.960236
811.1434829235077
[3, 10500] loss: 1.973653
822.7633054256439
[3, 11000] loss: 1.965111
834.403247833252
[3, 11500] loss: 1.920629
845.9433035850525
[3, 12000] loss: 1.935228
857.5632314682007
[3, 12500] loss: 1.967337
868.9863891601562
Epoch [3] loss: 6246057.610414
[4,   500] loss: 1.930766
880.5780227184296
[4,  1000] loss: 1.909218
892.048942565918
[4,  1500] loss: 1.924718
903.447484254837
[4,  2000] loss: 1.896238
915.0246679782867
[4,  2500] loss: 1.892466
926.6625909805298
[4,  3000] loss: 1.876678
938.2979753017426
[4,  3500] loss: 1.860783
949.8885769844055
[4,  4000] loss: 1.889556
961.5921802520752
[4,  4500] loss: 1.875793
973.4366681575775
[4,  5000] loss: 1.895003
985.1945085525513
[4,  5500] loss: 1.872477
997.1051337718964
[4,  6000] loss: 1.840517
1008.9087960720062
[4,  6500] loss: 1.886936
1020.6225318908691
[4,  7000] loss: 1.877599
1032.6558458805084
[4,  7500] loss: 1.914504
1044.6731944084167
[4,  8000] loss: 1.910028
1056.5467991828918
[4,  8500] loss: 1.891941
1068.4513702392578
[4,  9000] loss: 1.853270
1080.0277807712555
[4,  9500] loss: 1.852473
1091.8339223861694
[4, 10000] loss: 1.863313
1103.5011239051819
[4, 10500] loss: 1.832553
1115.4600036144257
[4, 11000] loss: 1.837791
1127.2906885147095
[4, 11500] loss: 1.830447
1139.2725358009338
[4, 12000] loss: 1.823797
1151.3843886852264
[4, 12500] loss: 1.817248
1163.2179961204529
Epoch [4] loss: 5867833.001913
[5,   500] loss: 1.798387
1174.8662929534912
[5,  1000] loss: 1.806564
1186.5716109275818
[5,  1500] loss: 1.805405
1198.391172170639
[5,  2000] loss: 1.838410
1210.2348444461823
[5,  2500] loss: 1.764415
1221.8312249183655
[5,  3000] loss: 1.769530
1233.5846886634827
[5,  3500] loss: 1.792718
1245.3023810386658
[5,  4000] loss: 1.768213
1257.0544221401215
[5,  4500] loss: 1.715146
1268.9715101718903
[5,  5000] loss: 1.784049
1280.7107775211334
[5,  5500] loss: 1.776969
1292.33007979393
[5,  6000] loss: 1.761059
1304.2533428668976
[5,  6500] loss: 1.766109
1316.1798708438873
[5,  7000] loss: 1.777869
1328.3116719722748
[5,  7500] loss: 1.793237
1340.0265712738037
[5,  8000] loss: 1.744543
1351.5232110023499
[5,  8500] loss: 1.800862
1363.0892415046692
[5,  9000] loss: 1.744501
1374.6129999160767
[5,  9500] loss: 1.807067
1386.4263968467712
[5, 10000] loss: 1.738266
1398.0426082611084
[5, 10500] loss: 1.681021
1409.5703485012054
[5, 11000] loss: 1.758769
1421.1452176570892
[5, 11500] loss: 1.751931
1432.7526688575745
[5, 12000] loss: 1.760281
1444.3383193016052
[5, 12500] loss: 1.698246
1456.1020934581757
Epoch [5] loss: 5545956.140609
[6,   500] loss: 1.633774
1467.793063879013
[6,  1000] loss: 1.769939
1479.2707760334015
[6,  1500] loss: 1.699020
1490.7280735969543
[6,  2000] loss: 1.691534
1502.2244153022766
[6,  2500] loss: 1.689747
1513.8141939640045
[6,  3000] loss: 1.671583
1525.42902469635
[6,  3500] loss: 1.692245
1537.1052701473236
[6,  4000] loss: 1.637321
1548.7376871109009
[6,  4500] loss: 1.669407
1560.3611381053925
[6,  5000] loss: 1.725081
1572.0279660224915
[6,  5500] loss: 1.683207
1583.5382075309753
[6,  6000] loss: 1.719297
1594.9100079536438
[6,  6500] loss: 1.684423
1606.4351389408112
[6,  7000] loss: 1.732419
1618.0533907413483
[6,  7500] loss: 1.665518
1629.71825838089
[6,  8000] loss: 1.680447
1641.3794331550598
[6,  8500] loss: 1.687158
1653.059814453125
[6,  9000] loss: 1.668405
1664.811449766159
[6,  9500] loss: 1.686372
1676.4059829711914
[6, 10000] loss: 1.619408
1688.094844341278
[6, 10500] loss: 1.696693
1699.8628985881805
[6, 11000] loss: 1.651780
1711.4397504329681
[6, 11500] loss: 1.639129
1722.9683740139008
[6, 12000] loss: 1.686388
1734.4504699707031
[6, 12500] loss: 1.627425
1746.2235567569733
Epoch [6] loss: 5268754.069895
[7,   500] loss: 1.612155
1757.9797444343567
[7,  1000] loss: 1.681715
1769.5160386562347
[7,  1500] loss: 1.616560
1781.150450706482
[7,  2000] loss: 1.629284
1792.7697715759277
[7,  2500] loss: 1.672479
1804.2624270915985
[7,  3000] loss: 1.607073
1815.7511849403381
[7,  3500] loss: 1.630881
1827.2602870464325
[7,  4000] loss: 1.602130
1838.8920357227325
[7,  4500] loss: 1.639541
1850.3597362041473
[7,  5000] loss: 1.601609
1861.8492321968079
[7,  5500] loss: 1.614204
1873.490035533905
[7,  6000] loss: 1.617557
1885.0024445056915
[7,  6500] loss: 1.618268
1896.614643573761
[7,  7000] loss: 1.596792
1908.1883654594421
[7,  7500] loss: 1.623302
1919.6878261566162
[7,  8000] loss: 1.601895
1931.2533915042877
[7,  8500] loss: 1.587570
1942.720576763153
[7,  9000] loss: 1.583197
1954.2529578208923
[7,  9500] loss: 1.612163
1965.8701343536377
[7, 10000] loss: 1.590880
1977.3530349731445
[7, 10500] loss: 1.558156
1989.023847103119
[7, 11000] loss: 1.591947
2000.6134369373322
[7, 11500] loss: 1.563349
2012.122755765915
[7, 12000] loss: 1.557066
2023.6382653713226
[7, 12500] loss: 1.568739
2035.184134721756
Epoch [7] loss: 5034310.025849
[8,   500] loss: 1.541223
2046.8224337100983
[8,  1000] loss: 1.551981
2058.5066311359406
[8,  1500] loss: 1.578017
2070.0191509723663
[8,  2000] loss: 1.490477
2081.596881389618
[8,  2500] loss: 1.579437
2093.2098472118378
[8,  3000] loss: 1.522307
2104.8113627433777
[8,  3500] loss: 1.545020
2116.623980283737
[8,  4000] loss: 1.552573
2128.0458402633667
[8,  4500] loss: 1.543993
2139.6123189926147
[8,  5000] loss: 1.588498
2151.0821442604065
[8,  5500] loss: 1.513403
2162.6701068878174
[8,  6000] loss: 1.564286
2174.324012517929
[8,  6500] loss: 1.552546
2185.9419248104095
[8,  7000] loss: 1.514258
2197.451436519623
[8,  7500] loss: 1.535401
2208.994575023651
[8,  8000] loss: 1.522039
2220.5938527584076
[8,  8500] loss: 1.518090
2232.262308359146
[8,  9000] loss: 1.513592
2243.822870016098
[8,  9500] loss: 1.580788
2255.5475294589996
[8, 10000] loss: 1.526503
2267.148773908615
[8, 10500] loss: 1.489570
2278.829859018326
[8, 11000] loss: 1.501974
2290.3802111148834
[8, 11500] loss: 1.496658
2301.941139936447
[8, 12000] loss: 1.528160
2313.67458152771
[8, 12500] loss: 1.487659
2325.2392416000366
Epoch [8] loss: 4812469.126764
[9,   500] loss: 1.439948
2336.8892562389374
[9,  1000] loss: 1.554047
2348.5573699474335
[9,  1500] loss: 1.500062
2360.2356588840485
[9,  2000] loss: 1.477308
2371.8029108047485
[9,  2500] loss: 1.551267
2383.3726711273193
[9,  3000] loss: 1.481620
2394.925441503525
[9,  3500] loss: 1.503060
2406.64959859848
[9,  4000] loss: 1.501133
2418.290843963623
[9,  4500] loss: 1.489010
2429.927579164505
[9,  5000] loss: 1.465578
2441.542960166931
[9,  5500] loss: 1.477309
2453.1153342723846
[9,  6000] loss: 1.450630
2464.7505407333374
[9,  6500] loss: 1.512987
2476.356212377548
[9,  7000] loss: 1.442131
2487.921352624893
[9,  7500] loss: 1.459545
2499.668134212494
[9,  8000] loss: 1.477961
2511.3630719184875
[9,  8500] loss: 1.488506
2523.0054256916046
[9,  9000] loss: 1.490084
2534.7673251628876
[9,  9500] loss: 1.438848
2546.3350973129272
[9, 10000] loss: 1.460458
2557.9651505947113
[9, 10500] loss: 1.434638
2569.480685710907
[9, 11000] loss: 1.460830
2581.093855857849
[9, 11500] loss: 1.467291
2592.8561816215515
[9, 12000] loss: 1.443457
2604.5541050434113
[9, 12500] loss: 1.458955
2616.09583735466
Epoch [9] loss: 4635609.650660
[10,   500] loss: 1.437380
2627.515831232071
[10,  1000] loss: 1.476170
2638.901067495346
[10,  1500] loss: 1.443039
2650.2730753421783
[10,  2000] loss: 1.410806
2661.810634613037
[10,  2500] loss: 1.462930
2673.279504299164
[10,  3000] loss: 1.435282
2684.935432434082
[10,  3500] loss: 1.444059
2696.5555396080017
[10,  4000] loss: 1.422205
2708.1925299167633
[10,  4500] loss: 1.416236
2719.846448659897
[10,  5000] loss: 1.424439
2731.4307940006256
[10,  5500] loss: 1.391648
2743.0974378585815
[10,  6000] loss: 1.402043
2754.729842185974
[10,  6500] loss: 1.441563
2766.3486733436584
[10,  7000] loss: 1.432127
2777.9081699848175
[10,  7500] loss: 1.384505
2789.39936709404
[10,  8000] loss: 1.493121
2800.9251832962036
[10,  8500] loss: 1.439156
2812.5069921016693
[10,  9000] loss: 1.386279
2824.2014870643616
[10,  9500] loss: 1.437943
2835.7548718452454
[10, 10000] loss: 1.420619
2847.3502011299133
[10, 10500] loss: 1.384551
2858.7958629131317
[10, 11000] loss: 1.402804
2870.401029586792
[10, 11500] loss: 1.416329
2882.0935637950897
[10, 12000] loss: 1.372282
2893.637395620346
[10, 12500] loss: 1.414070
2905.2207510471344
Epoch [10] loss: 4454198.871267
[11,   500] loss: 1.415683
2917.135307073593
[11,  1000] loss: 1.346276
2928.8443744182587
[11,  1500] loss: 1.356239
2940.368399143219
[11,  2000] loss: 1.352075
2951.937321662903
[11,  2500] loss: 1.371442
2963.502871274948
[11,  3000] loss: 1.428507
2975.2258281707764
[11,  3500] loss: 1.381194
2986.825002193451
[11,  4000] loss: 1.418301
2998.610376596451
[11,  4500] loss: 1.361577
3010.3617277145386
[11,  5000] loss: 1.363172
3021.98610496521
[11,  5500] loss: 1.330767
3033.7127335071564
[11,  6000] loss: 1.373471
3045.3522732257843
[11,  6500] loss: 1.338758
3056.929830312729
[11,  7000] loss: 1.385082
3068.8310747146606
[11,  7500] loss: 1.335865
3080.432641506195
[11,  8000] loss: 1.381365
3092.018503665924
[11,  8500] loss: 1.369351
3103.8498570919037
[11,  9000] loss: 1.375646
3115.4786846637726
[11,  9500] loss: 1.392733
3127.1084172725677
[11, 10000] loss: 1.358698
3138.796706199646
[11, 10500] loss: 1.341386
3150.3690531253815
[11, 11000] loss: 1.391280
3162.0016515254974
[11, 11500] loss: 1.294100
3173.702561855316
[11, 12000] loss: 1.318592
3185.20272564888
[11, 12500] loss: 1.333673
3196.6425619125366
Epoch [11] loss: 4290734.909423
[12,   500] loss: 1.338930
3208.3993175029755
[12,  1000] loss: 1.306863
3220.0466299057007
[12,  1500] loss: 1.358565
3231.613358974457
[12,  2000] loss: 1.305699
3243.218697309494
[12,  2500] loss: 1.324066
3254.755131483078
[12,  3000] loss: 1.356584
3266.2625892162323
[12,  3500] loss: 1.293392
3277.690288543701
[12,  4000] loss: 1.264231
3289.195502758026
[12,  4500] loss: 1.317855
3300.8219077587128
[12,  5000] loss: 1.275274
3312.3763036727905
[12,  5500] loss: 1.323699
3324.1496665477753
[12,  6000] loss: 1.272239
3335.6801364421844
[12,  6500] loss: 1.261265
3347.274752140045
[12,  7000] loss: 1.363581
3358.8397693634033
[12,  7500] loss: 1.274836
3370.478105545044
[12,  8000] loss: 1.332177
3382.1018495559692
[12,  8500] loss: 1.303208
3393.683231830597
[12,  9000] loss: 1.321473
3405.2798295021057
[12,  9500] loss: 1.309758
3416.867834329605
[12, 10000] loss: 1.291113
3428.4552738666534
[12, 10500] loss: 1.332517
3440.127072572708
[12, 11000] loss: 1.282764
3451.7448756694794
[12, 11500] loss: 1.289356
3463.242709875107
[12, 12000] loss: 1.317733
3474.8362379074097
[12, 12500] loss: 1.324616
3486.3581669330597
Epoch [12] loss: 4102044.105193
[13,   500] loss: 1.257444
3498.1945571899414
[13,  1000] loss: 1.288417
3509.7409415245056
[13,  1500] loss: 1.232926
3521.3992063999176
[13,  2000] loss: 1.287153
3532.965472459793
[13,  2500] loss: 1.228136
3544.4852776527405
[13,  3000] loss: 1.233200
3556.2064414024353
[13,  3500] loss: 1.276204
3567.9297173023224
[13,  4000] loss: 1.324038
3579.539887189865
[13,  4500] loss: 1.303589
3591.35654091835
[13,  5000] loss: 1.282661
3603.0155231952667
[13,  5500] loss: 1.245617
3614.5354549884796
[13,  6000] loss: 1.270334
3626.0998060703278
[13,  6500] loss: 1.256514
3637.7034089565277
[13,  7000] loss: 1.246812
3649.327977657318
[13,  7500] loss: 1.287172
3660.892829179764
[13,  8000] loss: 1.287150
3672.4656517505646
[13,  8500] loss: 1.226328
3684.1133358478546
[13,  9000] loss: 1.240985
3695.728439807892
[13,  9500] loss: 1.241714
3707.5017971992493
[13, 10000] loss: 1.297696
3719.252319574356
[13, 10500] loss: 1.285927
3730.8550143241882
[13, 11000] loss: 1.303793
3742.3784534931183
[13, 11500] loss: 1.276106
3754.0495903491974
[13, 12000] loss: 1.280669
3765.7255585193634
[13, 12500] loss: 1.252318
3777.224766731262
Epoch [13] loss: 3968435.082491
[14,   500] loss: 1.197524
3788.767625808716
[14,  1000] loss: 1.216505
3800.331378698349
[14,  1500] loss: 1.259776
3811.974838256836
[14,  2000] loss: 1.263465
3823.608540058136
[14,  2500] loss: 1.229906
3835.266124725342
[14,  3000] loss: 1.233190
3846.9727716445923
[14,  3500] loss: 1.249679
3858.7084329128265
[14,  4000] loss: 1.225485
3870.3144629001617
[14,  4500] loss: 1.242064
3881.9511671066284
[14,  5000] loss: 1.175803
3893.64909863472
[14,  5500] loss: 1.234338
3937.2502388954163
[14,  6000] loss: 1.210522
3948.959164381027
[14,  6500] loss: 1.210104
3960.4950931072235
[14,  7000] loss: 1.212711
3972.3110365867615
[14,  7500] loss: 1.161400
3983.9496841430664
[14,  8000] loss: 1.207562
3995.5802931785583
[14,  8500] loss: 1.203717
4007.2153055667877
[14,  9000] loss: 1.219544
4018.8135261535645
[14,  9500] loss: 1.233260
4030.3628222942352
[14, 10000] loss: 1.267978
4041.9028317928314
[14, 10500] loss: 1.213157
4053.5341396331787
[14, 11000] loss: 1.236345
4065.2894740104675
[14, 11500] loss: 1.202585
4076.9529235363007
[14, 12000] loss: 1.176862
4088.537487745285
[14, 12500] loss: 1.206348
4100.225113868713
Epoch [14] loss: 3833601.654024
[15,   500] loss: 1.177480
4115.259681463242
[15,  1000] loss: 1.203664
4126.880962371826
[15,  1500] loss: 1.150959
4138.412845611572
[15,  2000] loss: 1.215456
4149.936858892441
[15,  2500] loss: 1.167191
4161.661456346512
[15,  3000] loss: 1.149748
4173.255299091339
[15,  3500] loss: 1.164090
4184.927871227264
[15,  4000] loss: 1.194223
4196.793247699738
[15,  4500] loss: 1.153443
4208.430703639984
[15,  5000] loss: 1.198188
4220.221526622772
[15,  5500] loss: 1.213455
4231.817547082901
[15,  6000] loss: 1.209551
4243.752435922623
[15,  6500] loss: 1.199461
4255.476054906845
[15,  7000] loss: 1.182181
4267.155878305435
[15,  7500] loss: 1.199937
4278.762389659882
[15,  8000] loss: 1.163552
4290.40668797493
[15,  8500] loss: 1.221415
4302.1017735004425
[15,  9000] loss: 1.187243
4313.6611959934235
[15,  9500] loss: 1.165021
4325.370429515839
[15, 10000] loss: 1.168560
4336.930708408356
[15, 10500] loss: 1.196652
4348.557000160217
[15, 11000] loss: 1.197946
4360.14736366272
[15, 11500] loss: 1.176899
4371.625172138214
[15, 12000] loss: 1.157588
4383.148968935013
[15, 12500] loss: 1.172268
4394.664305448532
Epoch [15] loss: 3718106.210855
[16,   500] loss: 1.173536
4406.474135637283
[16,  1000] loss: 1.128598
4418.06712937355
[16,  1500] loss: 1.126567
4429.846986532211
[16,  2000] loss: 1.145041
4441.365375757217
[16,  2500] loss: 1.182163
4452.991769552231
[16,  3000] loss: 1.168291
4464.5911865234375
[16,  3500] loss: 1.131231
4476.092829465866
[16,  4000] loss: 1.181641
4487.538365364075
[16,  4500] loss: 1.124202
4499.1662673950195
[16,  5000] loss: 1.145171
4510.714310646057
[16,  5500] loss: 1.117579
4522.374492645264
[16,  6000] loss: 1.136708
4533.99213385582
[16,  6500] loss: 1.100731
4545.511256694794
[16,  7000] loss: 1.128007
4557.092839241028
[16,  7500] loss: 1.139589
4568.794403791428
[16,  8000] loss: 1.163259
4580.373755931854
[16,  8500] loss: 1.183897
4592.023343324661
[16,  9000] loss: 1.112800
4603.6880486011505
[16,  9500] loss: 1.138807
4615.595301866531
[16, 10000] loss: 1.128825
4627.494619131088
[16, 10500] loss: 1.109133
4638.951585531235
[16, 11000] loss: 1.183518
4650.492197751999
[16, 11500] loss: 1.144109
4662.161972761154
[16, 12000] loss: 1.145146
4673.877751588821
[16, 12500] loss: 1.132145
4685.578305482864
Epoch [16] loss: 3585850.832107
[17,   500] loss: 1.089770
4697.263448476791
[17,  1000] loss: 1.123561
4708.950176715851
[17,  1500] loss: 1.143913
4720.668698549271
[17,  2000] loss: 1.138426
4732.352102279663
[17,  2500] loss: 1.093816
4743.936783075333
[17,  3000] loss: 1.082800
4755.469934940338
[17,  3500] loss: 1.128684
4766.992270231247
[17,  4000] loss: 1.078876
4778.758709430695
[17,  4500] loss: 1.103064
4790.480826377869
[17,  5000] loss: 1.058050
4802.152327299118
[17,  5500] loss: 1.109768
4813.78285074234
[17,  6000] loss: 1.077864
4825.299599409103
[17,  6500] loss: 1.125295
4836.852809429169
[17,  7000] loss: 1.153891
4848.402806758881
[17,  7500] loss: 1.083310
4859.969288825989
[17,  8000] loss: 1.085165
4871.550934314728
[17,  8500] loss: 1.096550
4883.148761510849
[17,  9000] loss: 1.087794
4894.608605861664
[17,  9500] loss: 1.095862
4906.109435558319
[17, 10000] loss: 1.043182
4917.620986700058
[17, 10500] loss: 1.120771
4929.0517578125
[17, 11000] loss: 1.091989
4940.567848920822
[17, 11500] loss: 1.149314
4952.025637388229
[17, 12000] loss: 1.068601
4963.601986646652
[17, 12500] loss: 1.072986
4975.180982589722
Epoch [17] loss: 3445563.119710
[18,   500] loss: 1.045363
4986.8549654483795
[18,  1000] loss: 1.075218
4998.421975135803
[18,  1500] loss: 1.084838
5010.1663365364075
[18,  2000] loss: 1.050072
5021.799863815308
[18,  2500] loss: 1.049531
5033.585386991501
[18,  3000] loss: 1.100935
5045.430003166199
[18,  3500] loss: 1.040093
5057.211881875992
[18,  4000] loss: 1.063092
5068.844208955765
[18,  4500] loss: 1.137639
5080.651101350784
[18,  5000] loss: 1.075841
5092.294448852539
[18,  5500] loss: 1.074998
5103.966591835022
[18,  6000] loss: 1.079610
5115.6471927165985
[18,  6500] loss: 1.125423
5127.234604358673
[18,  7000] loss: 1.028505
5138.691629886627
[18,  7500] loss: 1.094621
5150.3950452804565
[18,  8000] loss: 1.074739
5162.010203361511
[18,  8500] loss: 1.026016
5173.610881328583
[18,  9000] loss: 1.111302
5185.1755912303925
[18,  9500] loss: 1.026678
5196.620438337326
[18, 10000] loss: 1.067469
5208.1884479522705
[18, 10500] loss: 1.094824
5219.779538154602
[18, 11000] loss: 1.083550
5231.442580699921
[18, 11500] loss: 1.094086
5243.088998794556
[18, 12000] loss: 1.031452
5254.773096084595
[18, 12500] loss: 1.067064
5266.392597436905
Epoch [18] loss: 3349635.515529
[19,   500] loss: 1.011995
5278.195283651352
[19,  1000] loss: 1.023696
5289.787951231003
[19,  1500] loss: 1.078212
5301.250757694244
[19,  2000] loss: 1.004029
5312.706384420395
[19,  2500] loss: 1.067078
5324.236573219299
[19,  3000] loss: 1.071457
5335.82639503479
[19,  3500] loss: 1.042469
5347.502014875412
[19,  4000] loss: 1.090835
5359.063654899597
[19,  4500] loss: 1.030225
5370.640408277512
[19,  5000] loss: 1.054537
5382.414135694504
[19,  5500] loss: 0.999925
5393.895935535431
[19,  6000] loss: 0.989999
5405.573299884796
[19,  6500] loss: 0.973623
5417.134152889252
[19,  7000] loss: 0.992318
5428.519641876221
[19,  7500] loss: 0.999883
5440.073356628418
[19,  8000] loss: 1.028575
5451.518861293793
[19,  8500] loss: 1.018391
5462.968544960022
[19,  9000] loss: 1.000857
5474.4976007938385
[19,  9500] loss: 1.009654
5486.17272400856
[19, 10000] loss: 1.052514
5497.75110578537
[19, 10500] loss: 0.948881
5509.367421388626
[19, 11000] loss: 1.021057
5520.990879774094
[19, 11500] loss: 1.023411
5532.926544427872
[19, 12000] loss: 1.047676
5544.476091861725
[19, 12500] loss: 1.092865
5556.151129484177
Epoch [19] loss: 3198516.350651
[20,   500] loss: 1.004451
5567.885192632675
[20,  1000] loss: 1.025644
5579.550335884094
[20,  1500] loss: 0.998570
5591.203025341034
[20,  2000] loss: 1.026968
5603.022285223007
[20,  2500] loss: 1.002528
5614.657764911652
[20,  3000] loss: 1.024448
5626.443980455399
[20,  3500] loss: 0.962216
5638.045381307602
[20,  4000] loss: 0.963984
5649.656977653503
[20,  4500] loss: 0.990982
5661.297569036484
[20,  5000] loss: 1.003503
5672.922186613083
[20,  5500] loss: 0.945208
5684.615057229996
[20,  6000] loss: 1.027439
5696.162160396576
[20,  6500] loss: 1.024443
5707.709904432297
[20,  7000] loss: 0.983727
5719.202811002731
[20,  7500] loss: 0.974454
5730.676411867142
[20,  8000] loss: 0.992543
5742.3525390625
[20,  8500] loss: 1.035785
5753.9900460243225
[20,  9000] loss: 0.982352
5765.564349412918
[20,  9500] loss: 1.015648
5777.191561698914
[20, 10000] loss: 1.020646
5788.641580104828
[20, 10500] loss: 0.999349
5800.228429555893
[20, 11000] loss: 0.979604
5812.04000043869
[20, 11500] loss: 1.061723
5823.672669172287
[20, 12000] loss: 1.008248
5835.266442060471
[20, 12500] loss: 0.988905
5846.913268566132
Epoch [20] loss: 3142890.636291
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-25 22:55:42.263116
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-2.5896, -2.3374, -0.6066,  2.7160, -0.6509,  2.6753,  1.0564,  0.0723,
         -0.2990, -2.1695],
        [ 1.3907,  2.6764, -1.3565,  0.1428, -2.3500, -1.5802, -3.2415,  0.0906,
          4.7083,  2.0007],
        [ 1.1148,  2.5088, -1.3897,  0.2752, -2.1559, -1.3007, -3.3089, -0.4690,
          3.8194,  0.3208],
        [ 2.9570, -1.0260,  0.4644,  0.8774, -1.4779, -0.4012, -3.0988, -0.8047,
          1.7475,  0.3127]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat  ship  ship plane
Accuracy of the network on the 4000.0 test images: 59 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': True, 'load_from_memory': False, 'pretrain': True, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=16, momentum=0, normalise=True, optimizer_choice=1, peregrine=True, pretrain=True, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
horse   cat  ship plane
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Number of parameters: 23528522 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582667837.5004814
[1,   500] loss: 2.233408
11.721159934997559
[1,  1000] loss: 2.133622
23.19745445251465
[1,  1500] loss: 2.075948
34.69832444190979
[1,  2000] loss: 2.014447
46.21882367134094
[1,  2500] loss: 1.963393
57.94271993637085
[1,  3000] loss: 1.931469
69.7793231010437
[1,  3500] loss: 1.944192
81.44883799552917
[1,  4000] loss: 1.856903
93.1506278514862
[1,  4500] loss: 1.834763
104.88963317871094
[1,  5000] loss: 1.877926
116.48061299324036
[1,  5500] loss: 1.848744
128.17042994499207
[1,  6000] loss: 1.774973
139.8677258491516
[1,  6500] loss: 1.754016
151.4586853981018
[1,  7000] loss: 1.769527
163.1582317352295
[1,  7500] loss: 1.692979
174.732750415802
[1,  8000] loss: 1.708493
186.26851272583008
[1,  8500] loss: 1.680276
197.7898850440979
[1,  9000] loss: 1.663075
209.35975074768066
[1,  9500] loss: 1.663141
220.95848941802979
[1, 10000] loss: 1.626863
232.6009418964386
[1, 10500] loss: 1.592296
244.269437789917
[1, 11000] loss: 1.575807
256.17505741119385
[1, 11500] loss: 1.562313
267.8906593322754
[1, 12000] loss: 1.527491
279.52809405326843
[1, 12500] loss: 1.561818
290.95598316192627
Epoch [1] loss: 5629970.856884
[2,   500] loss: 1.504055
302.57355785369873
[2,  1000] loss: 1.521006
314.06937980651855
[2,  1500] loss: 1.497480
325.61497259140015
[2,  2000] loss: 1.471978
337.2508509159088
[2,  2500] loss: 1.471408
349.03798174858093
[2,  3000] loss: 1.422105
360.6975519657135
[2,  3500] loss: 1.442348
372.19791984558105
[2,  4000] loss: 1.410720
383.7300205230713
[2,  4500] loss: 1.415960
395.8329346179962
[2,  5000] loss: 1.367959
407.47320342063904
[2,  5500] loss: 1.375333
419.2505569458008
[2,  6000] loss: 1.395711
430.98067116737366
[2,  6500] loss: 1.337664
442.47941160202026
[2,  7000] loss: 1.364657
454.1970717906952
[2,  7500] loss: 1.363520
465.7741677761078
[2,  8000] loss: 1.334196
477.26244616508484
[2,  8500] loss: 1.310645
488.83370304107666
[2,  9000] loss: 1.309632
500.3022871017456
[2,  9500] loss: 1.266275
511.9389889240265
[2, 10000] loss: 1.301340
523.6237573623657
[2, 10500] loss: 1.302603
535.2519736289978
[2, 11000] loss: 1.279556
546.8928864002228
[2, 11500] loss: 1.284440
558.6822850704193
[2, 12000] loss: 1.238191
570.2754600048065
[2, 12500] loss: 1.271513
581.8279294967651
Epoch [2] loss: 4301575.751989
[3,   500] loss: 1.239156
593.5619492530823
[3,  1000] loss: 1.197802
605.1476135253906
[3,  1500] loss: 1.209797
616.7900433540344
[3,  2000] loss: 1.223050
628.26145362854
[3,  2500] loss: 1.188168
639.6834969520569
[3,  3000] loss: 1.205386
651.2487282752991
[3,  3500] loss: 1.231323
662.8665692806244
[3,  4000] loss: 1.180515
674.5677354335785
[3,  4500] loss: 1.169755
685.9625334739685
[3,  5000] loss: 1.131388
697.5566070079803
[3,  5500] loss: 1.111946
709.1880230903625
[3,  6000] loss: 1.119240
720.8352334499359
[3,  6500] loss: 1.120807
732.4283301830292
[3,  7000] loss: 1.142613
744.0435454845428
[3,  7500] loss: 1.125869
755.5577001571655
[3,  8000] loss: 1.116235
767.1668109893799
[3,  8500] loss: 1.113386
778.8695542812347
[3,  9000] loss: 1.099437
790.4366087913513
[3,  9500] loss: 1.118598
801.9964256286621
[3, 10000] loss: 1.112630
813.6333253383636
[3, 10500] loss: 1.072867
825.3439919948578
[3, 11000] loss: 1.065023
836.8657813072205
[3, 11500] loss: 1.014158
848.6866888999939
[3, 12000] loss: 1.081149
860.1907896995544
[3, 12500] loss: 1.112337
871.6845815181732
Epoch [3] loss: 3579643.098905
[4,   500] loss: 1.060581
883.3383255004883
[4,  1000] loss: 1.006042
894.9264624118805
[4,  1500] loss: 1.090170
906.5384798049927
[4,  2000] loss: 1.038091
918.1762092113495
[4,  2500] loss: 1.004052
929.7197415828705
[4,  3000] loss: 0.975463
941.2460451126099
[4,  3500] loss: 1.039697
952.9614109992981
[4,  4000] loss: 1.015991
964.6420428752899
[4,  4500] loss: 1.032073
976.1603245735168
[4,  5000] loss: 1.010875
987.6597301959991
[4,  5500] loss: 0.966000
999.1961278915405
[4,  6000] loss: 1.001360
1010.709554195404
[4,  6500] loss: 1.008050
1022.2852864265442
[4,  7000] loss: 1.012257
1033.914794921875
[4,  7500] loss: 1.019166
1045.4841582775116
[4,  8000] loss: 0.988727
1056.962788105011
[4,  8500] loss: 0.987432
1068.8323044776917
[4,  9000] loss: 0.968060
1080.5068700313568
[4,  9500] loss: 0.996762
1091.9317219257355
[4, 10000] loss: 0.949108
1103.469417333603
[4, 10500] loss: 0.930374
1115.0379178524017
[4, 11000] loss: 0.924179
1126.6234652996063
[4, 11500] loss: 0.942801
1138.2029144763947
[4, 12000] loss: 0.935537
1149.8575205802917
[4, 12500] loss: 0.936291
1161.5721793174744
Epoch [4] loss: 3106095.721859
[5,   500] loss: 0.908645
1173.3111999034882
[5,  1000] loss: 0.884926
1184.917578458786
[5,  1500] loss: 0.915319
1196.5674393177032
[5,  2000] loss: 0.838194
1208.2614431381226
[5,  2500] loss: 0.863609
1219.8276419639587
[5,  3000] loss: 0.855287
1231.4239358901978
[5,  3500] loss: 0.919331
1242.9623267650604
[5,  4000] loss: 0.909411
1254.4630398750305
[5,  4500] loss: 0.918548
1266.1473879814148
[5,  5000] loss: 0.888518
1277.9350004196167
[5,  5500] loss: 0.867104
1289.682149887085
[5,  6000] loss: 0.887599
1301.501720905304
[5,  6500] loss: 0.893632
1313.1106262207031
[5,  7000] loss: 0.936141
1324.7632539272308
[5,  7500] loss: 0.881269
1336.3232746124268
[5,  8000] loss: 0.882487
1347.865258693695
[5,  8500] loss: 0.831245
1359.5722308158875
[5,  9000] loss: 0.834131
1371.3253769874573
[5,  9500] loss: 0.833947
1382.9914441108704
[5, 10000] loss: 0.898658
1394.840401649475
[5, 10500] loss: 0.834643
1406.4944140911102
[5, 11000] loss: 0.874883
1418.1810927391052
[5, 11500] loss: 0.885915
1429.6483166217804
[5, 12000] loss: 0.856715
1441.3090167045593
[5, 12500] loss: 0.869831
1453.022697210312
Epoch [5] loss: 2759594.211397
[6,   500] loss: 0.770722
1464.732711315155
[6,  1000] loss: 0.794324
1476.3442873954773
[6,  1500] loss: 0.798166
1487.984650850296
[6,  2000] loss: 0.856402
1499.5056853294373
[6,  2500] loss: 0.780475
1511.0604453086853
[6,  3000] loss: 0.761453
1522.9225370883942
[6,  3500] loss: 0.849297
1534.6026020050049
[6,  4000] loss: 0.787658
1546.0357110500336
[6,  4500] loss: 0.749310
1557.4504268169403
[6,  5000] loss: 0.789327
1569.0211460590363
[6,  5500] loss: 0.855817
1580.6721148490906
[6,  6000] loss: 0.731334
1592.1875603199005
[6,  6500] loss: 0.789338
1603.837798833847
[6,  7000] loss: 0.806563
1615.3725616931915
[6,  7500] loss: 0.804030
1627.129352092743
[6,  8000] loss: 0.841434
1638.9749703407288
[6,  8500] loss: 0.768258
1650.6530396938324
[6,  9000] loss: 0.804007
1662.4533836841583
[6,  9500] loss: 0.764336
1674.1322541236877
[6, 10000] loss: 0.826207
1685.8356165885925
[6, 10500] loss: 0.774053
1697.4642300605774
[6, 11000] loss: 0.784293
1709.1200621128082
[6, 11500] loss: 0.804753
1720.5389103889465
[6, 12000] loss: 0.793725
1732.0878398418427
[6, 12500] loss: 0.752328
1743.7297232151031
Epoch [6] loss: 2489506.570326
[7,   500] loss: 0.693334
1755.4122478961945
[7,  1000] loss: 0.700335
1767.0011012554169
[7,  1500] loss: 0.779105
1778.8250172138214
[7,  2000] loss: 0.737263
1790.4652268886566
[7,  2500] loss: 0.768173
1802.0959899425507
[7,  3000] loss: 0.776379
1813.8421785831451
[7,  3500] loss: 0.711414
1825.418062210083
[7,  4000] loss: 0.671725
1837.1010789871216
[7,  4500] loss: 0.729965
1848.7345836162567
[7,  5000] loss: 0.683885
1860.2599391937256
[7,  5500] loss: 0.687774
1871.8828268051147
[7,  6000] loss: 0.713702
1883.5953125953674
[7,  6500] loss: 0.715858
1895.2656157016754
[7,  7000] loss: 0.732820
1906.9066672325134
[7,  7500] loss: 0.665659
1918.4640655517578
[7,  8000] loss: 0.677437
1930.0476579666138
[7,  8500] loss: 0.669099
1941.7072641849518
[7,  9000] loss: 0.667219
1953.45889210701
[7,  9500] loss: 0.717909
1965.0673995018005
[7, 10000] loss: 0.697194
1976.6141426563263
[7, 10500] loss: 0.691402
1988.141786813736
[7, 11000] loss: 0.681143
1999.6775801181793
[7, 11500] loss: 0.723719
2011.4250793457031
[7, 12000] loss: 0.690010
2023.1181857585907
[7, 12500] loss: 0.693272
2034.6319534778595
Epoch [7] loss: 2205522.619843
[8,   500] loss: 0.644068
2046.4057922363281
[8,  1000] loss: 0.649473
2057.87868976593
[8,  1500] loss: 0.681067
2069.3851618766785
[8,  2000] loss: 0.684678
2081.0164840221405
[8,  2500] loss: 0.660715
2092.544680595398
[8,  3000] loss: 0.615064
2104.0809848308563
[8,  3500] loss: 0.633676
2115.681893825531
[8,  4000] loss: 0.678282
2127.464725971222
[8,  4500] loss: 0.662365
2139.1562564373016
[8,  5000] loss: 0.660899
2150.7414162158966
[8,  5500] loss: 0.621037
2162.3551909923553
[8,  6000] loss: 0.640683
2174.143439769745
[8,  6500] loss: 0.633182
2185.633875846863
[8,  7000] loss: 0.647642
2197.2571363449097
[8,  7500] loss: 0.618537
2208.940353155136
[8,  8000] loss: 0.645084
2220.4678449630737
[8,  8500] loss: 0.646158
2232.1254527568817
[8,  9000] loss: 0.684590
2243.6465759277344
[8,  9500] loss: 0.665410
2255.255261182785
[8, 10000] loss: 0.652781
2266.913412809372
[8, 10500] loss: 0.663693
2278.438116312027
[8, 11000] loss: 0.628045
2290.0866055488586
[8, 11500] loss: 0.680152
2301.826257944107
[8, 12000] loss: 0.614481
2313.435052394867
[8, 12500] loss: 0.644092
2325.1580407619476
Epoch [8] loss: 2035452.287567
[9,   500] loss: 0.579567
2336.8814690113068
[9,  1000] loss: 0.554258
2348.6068511009216
[9,  1500] loss: 0.617342
2360.318326473236
[9,  2000] loss: 0.589548
2371.8838226795197
[9,  2500] loss: 0.575028
2383.5456144809723
[9,  3000] loss: 0.609211
2395.0945353507996
[9,  3500] loss: 0.604180
2406.708708524704
[9,  4000] loss: 0.595315
2418.2491085529327
[9,  4500] loss: 0.606791
2430.0219542980194
[9,  5000] loss: 0.635436
2441.7750487327576
[9,  5500] loss: 0.606781
2453.3365054130554
[9,  6000] loss: 0.613653
2464.929778814316
[9,  6500] loss: 0.579071
2476.628716468811
[9,  7000] loss: 0.576001
2488.3683140277863
[9,  7500] loss: 0.573716
2499.992760658264
[9,  8000] loss: 0.571029
2511.5917949676514
[9,  8500] loss: 0.580494
2523.2311697006226
[9,  9000] loss: 0.575479
2534.757614135742
[9,  9500] loss: 0.593427
2546.2292127609253
[9, 10000] loss: 0.584442
2557.860889196396
[9, 10500] loss: 0.571168
2569.4660425186157
[9, 11000] loss: 0.609516
2581.2056975364685
[9, 11500] loss: 0.667394
2593.1328201293945
[9, 12000] loss: 0.588119
2604.9084844589233
[9, 12500] loss: 0.620383
2616.5556259155273
Epoch [9] loss: 1871724.865938
[10,   500] loss: 0.570897
2628.354692697525
[10,  1000] loss: 0.547271
2639.978147506714
[10,  1500] loss: 0.518824
2651.695036649704
[10,  2000] loss: 0.541304
2663.4100255966187
[10,  2500] loss: 0.566352
2675.2387130260468
[10,  3000] loss: 0.533438
2686.9435455799103
[10,  3500] loss: 0.575274
2698.6316916942596
[10,  4000] loss: 0.532827
2710.160500764847
[10,  4500] loss: 0.503990
2721.779134273529
[10,  5000] loss: 0.509918
2733.3491027355194
[10,  5500] loss: 0.558356
2744.8478519916534
[10,  6000] loss: 0.540051
2756.5076851844788
[10,  6500] loss: 0.492654
2768.0774698257446
[10,  7000] loss: 0.558020
2779.8484501838684
[10,  7500] loss: 0.542134
2791.4074223041534
[10,  8000] loss: 0.559414
2802.9462671279907
[10,  8500] loss: 0.524443
2814.7328057289124
[10,  9000] loss: 0.516287
2826.456664800644
[10,  9500] loss: 0.575298
2838.0951302051544
[10, 10000] loss: 0.553473
2849.8819675445557
[10, 10500] loss: 0.544364
2861.5743613243103
[10, 11000] loss: 0.582841
2873.342108488083
[10, 11500] loss: 0.555279
2885.0143949985504
[10, 12000] loss: 0.551208
2896.4960222244263
[10, 12500] loss: 0.516634
2908.0020203590393
Epoch [10] loss: 1685818.400570
[11,   500] loss: 0.465223
2919.8280024528503
[11,  1000] loss: 0.496163
2931.328797340393
[11,  1500] loss: 0.471207
2942.9963047504425
[11,  2000] loss: 0.501733
2954.5302748680115
[11,  2500] loss: 0.477456
2966.1070578098297
[11,  3000] loss: 0.505681
2977.6640524864197
[11,  3500] loss: 0.529798
2989.4225690364838
[11,  4000] loss: 0.448949
3001.150543689728
[11,  4500] loss: 0.525026
3012.7524712085724
[11,  5000] loss: 0.523501
3024.2694227695465
[11,  5500] loss: 0.461399
3035.9480419158936
[11,  6000] loss: 0.490332
3047.6878032684326
[11,  6500] loss: 0.508110
3059.287029504776
[11,  7000] loss: 0.526551
3071.0576088428497
[11,  7500] loss: 0.510194
3082.654422044754
[11,  8000] loss: 0.465688
3094.2488770484924
[11,  8500] loss: 0.549130
3105.778511762619
[11,  9000] loss: 0.456502
3117.354917049408
[11,  9500] loss: 0.510767
3129.0214643478394
[11, 10000] loss: 0.508095
3140.603145837784
[11, 10500] loss: 0.500906
3152.5255382061005
[11, 11000] loss: 0.507994
3164.296198129654
[11, 11500] loss: 0.468062
3175.9542324543
[11, 12000] loss: 0.542912
3187.5932116508484
[11, 12500] loss: 0.541019
3199.058360338211
Epoch [11] loss: 1565553.190927
[12,   500] loss: 0.410925
3210.7809529304504
[12,  1000] loss: 0.457475
3222.478549718857
[12,  1500] loss: 0.494751
3234.2148683071136
[12,  2000] loss: 0.439584
3245.9797410964966
[12,  2500] loss: 0.426645
3257.770359277725
[12,  3000] loss: 0.427134
3269.4575638771057
[12,  3500] loss: 0.468410
3281.14777636528
[12,  4000] loss: 0.446306
3292.8447449207306
[12,  4500] loss: 0.463092
3304.4463591575623
[12,  5000] loss: 0.458673
3316.0232717990875
[12,  5500] loss: 0.461480
3327.6074080467224
[12,  6000] loss: 0.441328
3339.321131706238
[12,  6500] loss: 0.457604
3350.8687493801117
[12,  7000] loss: 0.470091
3362.462323665619
[12,  7500] loss: 0.464047
3374.1541969776154
[12,  8000] loss: 0.450318
3385.7938640117645
[12,  8500] loss: 0.453247
3397.471964120865
[12,  9000] loss: 0.456803
3409.2479631900787
[12,  9500] loss: 0.475157
3421.192070245743
[12, 10000] loss: 0.487690
3432.769148349762
[12, 10500] loss: 0.428683
3444.3827233314514
[12, 11000] loss: 0.450052
3455.9539964199066
[12, 11500] loss: 0.486012
3467.5978088378906
[12, 12000] loss: 0.470541
3479.0780062675476
[12, 12500] loss: 0.434921
3490.708482027054
Epoch [12] loss: 1441823.874536
[13,   500] loss: 0.395693
3502.4895057678223
[13,  1000] loss: 0.395735
3514.1718962192535
[13,  1500] loss: 0.399878
3525.7841045856476
[13,  2000] loss: 0.457854
3537.2882668972015
[13,  2500] loss: 0.425285
3549.1503033638
[13,  3000] loss: 0.382309
3560.991303920746
[13,  3500] loss: 0.417184
3572.6787872314453
[13,  4000] loss: 0.388757
3584.355896949768
[13,  4500] loss: 0.464009
3596.006388902664
[13,  5000] loss: 0.397911
3607.71301817894
[13,  5500] loss: 0.424541
3619.351986885071
[13,  6000] loss: 0.425102
3631.1030373573303
[13,  6500] loss: 0.456236
3642.767553806305
[13,  7000] loss: 0.446942
3654.527508020401
[13,  7500] loss: 0.425189
3666.1497056484222
[13,  8000] loss: 0.386834
3677.7505791187286
[13,  8500] loss: 0.411675
3689.331995487213
[13,  9000] loss: 0.407134
3700.8907556533813
[13,  9500] loss: 0.430772
3712.3760170936584
[13, 10000] loss: 0.413542
3723.905642271042
[13, 10500] loss: 0.451665
3735.483744621277
[13, 11000] loss: 0.419248
3747.0427482128143
[13, 11500] loss: 0.381016
3758.516177892685
[13, 12000] loss: 0.409577
3770.1242723464966
[13, 12500] loss: 0.446961
3781.6653950214386
Epoch [13] loss: 1316106.339567
[14,   500] loss: 0.402067
3793.489572763443
[14,  1000] loss: 0.369159
3805.0616493225098
[14,  1500] loss: 0.367613
3816.693113088608
[14,  2000] loss: 0.360671
3828.294872045517
[14,  2500] loss: 0.359475
3839.9796431064606
[14,  3000] loss: 0.366461
3851.480515241623
[14,  3500] loss: 0.380222
3863.2786469459534
[14,  4000] loss: 0.396965
3875.541633605957
[14,  4500] loss: 0.406929
3887.366322994232
[14,  5000] loss: 0.366152
3900.5563218593597
[14,  5500] loss: 0.358604
3926.9792444705963
[14,  6000] loss: 0.358223
3938.525461912155
[14,  6500] loss: 0.366085
3950.224924325943
[14,  7000] loss: 0.404320
3961.8657417297363
[14,  7500] loss: 0.383686
3973.5489315986633
[14,  8000] loss: 0.383755
3985.377895116806
[14,  8500] loss: 0.369424
3996.9075162410736
[14,  9000] loss: 0.401520
4008.7407128810883
[14,  9500] loss: 0.358245
4020.4127225875854
[14, 10000] loss: 0.432371
4032.0477011203766
[14, 10500] loss: 0.385799
4043.7055444717407
[14, 11000] loss: 0.411046
4055.216847896576
[14, 11500] loss: 0.394891
4066.788434267044
[14, 12000] loss: 0.425576
4078.543934583664
[14, 12500] loss: 0.414572
4090.183482646942
Epoch [14] loss: 1206409.465018
[15,   500] loss: 0.347573
4103.995591163635
[15,  1000] loss: 0.323063
4115.614092350006
[15,  1500] loss: 0.345077
4127.2367079257965
[15,  2000] loss: 0.344613
4138.787942647934
[15,  2500] loss: 0.336609
4150.415413141251
[15,  3000] loss: 0.348939
4161.997767925262
[15,  3500] loss: 0.361970
4173.538851976395
[15,  4000] loss: 0.352741
4185.254460811615
[15,  4500] loss: 0.339269
4196.907683134079
[15,  5000] loss: 0.380220
4208.570800304413
[15,  5500] loss: 0.339816
4220.046583652496
[15,  6000] loss: 0.333620
4231.744226694107
[15,  6500] loss: 0.383344
4243.415864229202
[15,  7000] loss: 0.378129
4254.967933654785
[15,  7500] loss: 0.348730
4266.683369398117
[15,  8000] loss: 0.328527
4278.357082605362
[15,  8500] loss: 0.350122
4289.943999290466
[15,  9000] loss: 0.352505
4301.542019128799
[15,  9500] loss: 0.348960
4313.2389125823975
[15, 10000] loss: 0.364159
4325.068912267685
[15, 10500] loss: 0.372216
4336.736397266388
[15, 11000] loss: 0.363458
4348.216292381287
[15, 11500] loss: 0.344821
4359.8609755039215
[15, 12000] loss: 0.329524
4371.615636110306
[15, 12500] loss: 0.367522
4383.213734865189
Epoch [15] loss: 1111074.781666
[16,   500] loss: 0.321015
4395.064453363419
[16,  1000] loss: 0.296292
4406.630651473999
[16,  1500] loss: 0.323427
4418.408132314682
[16,  2000] loss: 0.299767
4430.150482177734
[16,  2500] loss: 0.328520
4441.895284414291
[16,  3000] loss: 0.308886
4453.547758579254
[16,  3500] loss: 0.314503
4465.1020493507385
[16,  4000] loss: 0.282819
4476.81538772583
[16,  4500] loss: 0.326790
4488.506107091904
[16,  5000] loss: 0.305158
4499.949950456619
[16,  5500] loss: 0.327044
4511.548575162888
[16,  6000] loss: 0.366038
4523.113207578659
[16,  6500] loss: 0.314192
4534.91327381134
[16,  7000] loss: 0.272681
4546.556928634644
[16,  7500] loss: 0.352301
4558.31057715416
[16,  8000] loss: 0.332772
4570.039873600006
[16,  8500] loss: 0.310224
4581.583689212799
[16,  9000] loss: 0.348874
4593.152915716171
[16,  9500] loss: 0.337560
4604.816737174988
[16, 10000] loss: 0.322439
4616.414942026138
[16, 10500] loss: 0.328125
4628.036083698273
[16, 11000] loss: 0.330601
4639.5938901901245
[16, 11500] loss: 0.347346
4651.13708114624
[16, 12000] loss: 0.303076
4662.689007520676
[16, 12500] loss: 0.301207
4674.408370256424
Epoch [16] loss: 999720.063042
[17,   500] loss: 0.295653
4686.326644420624
[17,  1000] loss: 0.266401
4698.036336183548
[17,  1500] loss: 0.257487
4709.5790503025055
[17,  2000] loss: 0.281934
4721.168861627579
[17,  2500] loss: 0.271200
4732.713914632797
[17,  3000] loss: 0.291365
4744.506271362305
[17,  3500] loss: 0.300271
4756.183564424515
[17,  4000] loss: 0.242742
4768.031929731369
[17,  4500] loss: 0.276893
4779.742484331131
[17,  5000] loss: 0.291771
4791.442488908768
[17,  5500] loss: 0.313334
4802.958058834076
[17,  6000] loss: 0.292202
4814.5416169166565
[17,  6500] loss: 0.261312
4826.005887031555
[17,  7000] loss: 0.270743
4837.666022777557
[17,  7500] loss: 0.311851
4849.365891695023
[17,  8000] loss: 0.291567
4861.161733150482
[17,  8500] loss: 0.270523
4872.988200187683
[17,  9000] loss: 0.277349
4884.696770191193
[17,  9500] loss: 0.315049
4896.3417592048645
[17, 10000] loss: 0.322477
4907.958338975906
[17, 10500] loss: 0.324427
4919.527554035187
[17, 11000] loss: 0.303583
4931.208650827408
[17, 11500] loss: 0.306775
4942.764356851578
[17, 12000] loss: 0.288123
4954.475239992142
[17, 12500] loss: 0.317632
4965.949006080627
Epoch [17] loss: 898961.968703
[18,   500] loss: 0.274890
4977.728060245514
[18,  1000] loss: 0.247355
4989.365791082382
[18,  1500] loss: 0.248598
5001.095483541489
[18,  2000] loss: 0.238887
5012.71719622612
[18,  2500] loss: 0.240283
5024.338088274002
[18,  3000] loss: 0.249019
5036.068780422211
[18,  3500] loss: 0.246354
5047.918295383453
[18,  4000] loss: 0.260201
5059.518718004227
[18,  4500] loss: 0.260567
5071.056445598602
[18,  5000] loss: 0.264230
5082.639361143112
[18,  5500] loss: 0.271327
5094.196291446686
[18,  6000] loss: 0.298128
5105.815317153931
[18,  6500] loss: 0.248874
5117.439404726028
[18,  7000] loss: 0.263361
5128.947425365448
[18,  7500] loss: 0.249827
5140.514317512512
[18,  8000] loss: 0.232424
5152.043383836746
[18,  8500] loss: 0.325397
5163.638613700867
[18,  9000] loss: 0.285995
5175.228116989136
[18,  9500] loss: 0.312039
5186.8837032318115
[18, 10000] loss: 0.285632
5198.380204439163
[18, 10500] loss: 0.246833
5209.989382266998
[18, 11000] loss: 0.280783
5221.652863264084
[18, 11500] loss: 0.258999
5233.309425354004
[18, 12000] loss: 0.267217
5244.851775169373
[18, 12500] loss: 0.267473
5256.391466856003
Epoch [18] loss: 824601.528965
[19,   500] loss: 0.227358
5268.43642282486
[19,  1000] loss: 0.236038
5280.04475235939
[19,  1500] loss: 0.203372
5291.607740402222
[19,  2000] loss: 0.210552
5303.245562553406
[19,  2500] loss: 0.269561
5315.026420116425
[19,  3000] loss: 0.238490
5326.578193664551
[19,  3500] loss: 0.210909
5338.156656265259
[19,  4000] loss: 0.275598
5349.718658208847
[19,  4500] loss: 0.245707
5361.147240638733
[19,  5000] loss: 0.229917
5372.605419635773
[19,  5500] loss: 0.256731
5384.33903837204
[19,  6000] loss: 0.223300
5395.760342359543
[19,  6500] loss: 0.242489
5407.20774102211
[19,  7000] loss: 0.209256
5418.64349937439
[19,  7500] loss: 0.262715
5430.060890674591
[19,  8000] loss: 0.260105
5441.483665466309
[19,  8500] loss: 0.235576
5452.888147354126
[19,  9000] loss: 0.238268
5464.401668071747
[19,  9500] loss: 0.247225
5475.959610700607
[19, 10000] loss: 0.251862
5487.511553287506
[19, 10500] loss: 0.255418
5499.039009332657
[19, 11000] loss: 0.244995
5510.577639102936
[19, 11500] loss: 0.253952
5522.049058198929
[19, 12000] loss: 0.258275
5533.581657171249
[19, 12500] loss: 0.234929
5545.038336753845
Epoch [19] loss: 754211.902121
[20,   500] loss: 0.215504
5556.592684984207
[20,  1000] loss: 0.189441
5568.047427892685
[20,  1500] loss: 0.196438
5579.498084783554
[20,  2000] loss: 0.202989
5590.935940504074
[20,  2500] loss: 0.192886
5602.560752391815
[20,  3000] loss: 0.213042
5614.055192232132
[20,  3500] loss: 0.187095
5625.548434019089
[20,  4000] loss: 0.219817
5636.959377288818
[20,  4500] loss: 0.193629
5648.502640962601
[20,  5000] loss: 0.208840
5660.033652067184
[20,  5500] loss: 0.235068
5671.435479402542
[20,  6000] loss: 0.230381
5682.9426527023315
[20,  6500] loss: 0.222648
5694.535335302353
[20,  7000] loss: 0.229700
5705.949500083923
[20,  7500] loss: 0.218484
5717.465991020203
[20,  8000] loss: 0.241300
5728.9943816661835
[20,  8500] loss: 0.211864
5740.461477279663
[20,  9000] loss: 0.251007
5751.965947151184
[20,  9500] loss: 0.201404
5763.525010108948
[20, 10000] loss: 0.224727
5774.993638753891
[20, 10500] loss: 0.199131
5786.433976173401
[20, 11000] loss: 0.225778
5797.910464763641
[20, 11500] loss: 0.225896
5809.489326238632
[20, 12000] loss: 0.253514
5821.029726743698
[20, 12500] loss: 0.195353
5832.514233589172
Epoch [20] loss: 679385.227590
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-26 00:34:30.065798
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-0.8720, -0.6090, -1.3607,  5.7315, -1.9575,  2.8510,  0.0258, -0.9690,
         -3.0092, -1.1788],
        [-0.1722,  3.1526, -0.8800, -0.6894, -1.3168, -0.8922, -1.3713, -3.4814,
          5.0206, -0.0823],
        [ 2.7637,  3.6845, -1.3179, -1.0587, -2.8847, -3.0845, -2.3101, -2.4385,
          5.1205,  0.7970],
        [ 6.9932, -1.0353, -0.6516,  0.8436, -1.3804, -2.2967, -2.2664, -2.3891,
          2.2330, -0.0939]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat  ship  ship plane
Accuracy of the network on the 4000.0 test images: 82 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=16, momentum=0.9, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 bird   cat horse  deer
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Number of parameters: 23528522 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0.9
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582673757.9295318
[1,   500] loss: 6.251385
13.382012367248535
[1,  1000] loss: 5.661209
26.59135103225708
[1,  1500] loss: 5.762707
39.92321443557739
[1,  2000] loss: 5.353064
53.22157907485962
[1,  2500] loss: 5.001218
66.51007294654846
[1,  3000] loss: 4.985479
79.7854175567627
[1,  3500] loss: 4.565898
92.95522451400757
[1,  4000] loss: 4.512252
106.07337403297424
[1,  4500] loss: 4.458671
119.4133472442627
[1,  5000] loss: 4.215023
132.74768018722534
[1,  5500] loss: 4.200768
145.95170640945435
[1,  6000] loss: 4.189165
159.27944588661194
[1,  6500] loss: 4.096101
172.4065420627594
[1,  7000] loss: 3.745325
185.69150161743164
[1,  7500] loss: 3.780323
199.08497524261475
[1,  8000] loss: 3.813911
212.33117961883545
[1,  8500] loss: 3.405023
225.75420808792114
[1,  9000] loss: 3.521532
239.19206190109253
[1,  9500] loss: 3.391706
252.61707019805908
[1, 10000] loss: 3.397688
265.8866059780121
[1, 10500] loss: 3.259074
279.14083552360535
[1, 11000] loss: 3.434065
292.44522070884705
[1, 11500] loss: 3.176215
305.6999683380127
[1, 12000] loss: 3.269580
318.92616295814514
[1, 12500] loss: 3.205773
332.25782585144043
Epoch [1] loss: 13125387.948121
[2,   500] loss: 3.196096
345.85045409202576
[2,  1000] loss: 3.175319
359.1953763961792
[2,  1500] loss: 3.054875
372.37832260131836
[2,  2000] loss: 3.106402
385.72863030433655
[2,  2500] loss: 2.995850
398.9062395095825
[2,  3000] loss: 3.045753
412.2570848464966
[2,  3500] loss: 2.966203
425.6156840324402
[2,  4000] loss: 2.855067
439.23836731910706
[2,  4500] loss: 2.962920
452.528608083725
[2,  5000] loss: 2.811885
465.8642649650574
[2,  5500] loss: 2.851195
479.24312591552734
[2,  6000] loss: 2.834220
493.3031573295593
[2,  6500] loss: 2.828749
506.7817487716675
[2,  7000] loss: 2.859513
520.0163803100586
[2,  7500] loss: 2.766369
533.2573311328888
[2,  8000] loss: 2.775489
546.5079555511475
[2,  8500] loss: 2.715478
559.8272895812988
[2,  9000] loss: 2.698908
573.1142280101776
[2,  9500] loss: 2.725604
586.4478421211243
[2, 10000] loss: 2.611429
599.7648847103119
[2, 10500] loss: 2.577634
613.4571413993835
[2, 11000] loss: 2.558486
626.9201698303223
[2, 11500] loss: 2.635904
640.305269241333
[2, 12000] loss: 2.556598
653.6437516212463
[2, 12500] loss: 2.586795
666.936309337616
Epoch [2] loss: 8874666.316606
[3,   500] loss: 2.581027
680.4592473506927
[3,  1000] loss: 2.525809
693.7984433174133
[3,  1500] loss: 2.520376
707.1087889671326
[3,  2000] loss: 2.521576
720.3762621879578
[3,  2500] loss: 2.517326
733.5608830451965
[3,  3000] loss: 2.455683
746.8103725910187
[3,  3500] loss: 2.463240
760.1789968013763
[3,  4000] loss: 2.452834
773.3206570148468
[3,  4500] loss: 2.427985
786.4996447563171
[3,  5000] loss: 2.482035
799.640855550766
[3,  5500] loss: 2.426702
812.988632440567
[3,  6000] loss: 2.466197
826.4139759540558
[3,  6500] loss: 2.472456
839.783285856247
[3,  7000] loss: 2.427492
853.0246052742004
[3,  7500] loss: 2.351572
866.3208458423615
[3,  8000] loss: 2.387476
879.3763217926025
[3,  8500] loss: 2.358293
892.7096281051636
[3,  9000] loss: 2.381842
905.9822180271149
[3,  9500] loss: 2.355583
919.2969355583191
[3, 10000] loss: 2.366748
932.4760503768921
[3, 10500] loss: 2.314490
945.6775631904602
[3, 11000] loss: 2.278145
959.1038053035736
[3, 11500] loss: 2.303497
972.3513343334198
[3, 12000] loss: 2.297396
985.7345240116119
[3, 12500] loss: 2.284278
998.9631202220917
Epoch [3] loss: 7573126.428230
[4,   500] loss: 2.272763
1012.3693461418152
[4,  1000] loss: 2.360138
1025.6293816566467
[4,  1500] loss: 2.291409
1038.9847297668457
[4,  2000] loss: 2.255596
1052.4306709766388
[4,  2500] loss: 2.262481
1065.7066688537598
[4,  3000] loss: 2.258489
1078.9132783412933
[4,  3500] loss: 2.207808
1092.213710784912
[4,  4000] loss: 2.252829
1105.4098420143127
[4,  4500] loss: 2.224967
1118.6169047355652
[4,  5000] loss: 2.207427
1131.6460540294647
[4,  5500] loss: 2.208291
1144.8042430877686
[4,  6000] loss: 2.248207
1158.0200009346008
[4,  6500] loss: 2.177425
1171.138752937317
[4,  7000] loss: 2.237947
1184.306335926056
[4,  7500] loss: 2.187355
1197.4769842624664
[4,  8000] loss: 2.209227
1210.57830286026
[4,  8500] loss: 2.174153
1223.8384947776794
[4,  9000] loss: 2.172511
1237.0305161476135
[4,  9500] loss: 2.119767
1250.256138086319
[4, 10000] loss: 2.160098
1263.524216413498
[4, 10500] loss: 2.158846
1276.7078597545624
[4, 11000] loss: 2.101623
1289.853788614273
[4, 11500] loss: 2.136308
1303.0116930007935
[4, 12000] loss: 2.128924
1316.3742761611938
[4, 12500] loss: 2.119128
1329.6274726390839
Epoch [4] loss: 6911535.478877
[5,   500] loss: 2.110604
1342.98592710495
[5,  1000] loss: 2.105578
1356.1839871406555
[5,  1500] loss: 2.098369
1369.6196684837341
[5,  2000] loss: 2.100753
1383.0062289237976
[5,  2500] loss: 2.111058
1396.1847925186157
[5,  3000] loss: 2.047276
1409.5248484611511
[5,  3500] loss: 2.039448
1422.8470511436462
[5,  4000] loss: 2.042409
1436.0982089042664
[5,  4500] loss: 2.083439
1449.501226425171
[5,  5000] loss: 2.074562
1462.7143671512604
[5,  5500] loss: 2.048040
1475.9073848724365
[5,  6000] loss: 2.019503
1489.1251623630524
[5,  6500] loss: 1.990995
1502.3131792545319
[5,  7000] loss: 2.043370
1515.5306377410889
[5,  7500] loss: 2.029823
1528.7400732040405
[5,  8000] loss: 2.046094
1542.358045578003
[5,  8500] loss: 2.010134
1555.640212059021
[5,  9000] loss: 2.052026
1568.9104945659637
[5,  9500] loss: 1.976876
1582.1590242385864
[5, 10000] loss: 2.000444
1595.3623609542847
[5, 10500] loss: 1.958399
1608.5825803279877
[5, 11000] loss: 1.983384
1621.7202837467194
[5, 11500] loss: 1.987142
1634.9112396240234
[5, 12000] loss: 1.978615
1648.1011188030243
[5, 12500] loss: 1.977592
1661.2018535137177
Epoch [5] loss: 6387044.382205
[6,   500] loss: 1.970001
1674.4196527004242
[6,  1000] loss: 1.975747
1687.6217472553253
[6,  1500] loss: 1.960932
1700.9669225215912
[6,  2000] loss: 1.943555
1714.210312128067
[6,  2500] loss: 1.953329
1727.2966175079346
[6,  3000] loss: 1.951184
1740.3964326381683
[6,  3500] loss: 1.942731
1753.6170868873596
[6,  4000] loss: 1.916600
1766.8405313491821
[6,  4500] loss: 1.958449
1780.080288887024
[6,  5000] loss: 1.926367
1793.348925113678
[6,  5500] loss: 1.927359
1806.5450403690338
[6,  6000] loss: 1.921295
1819.7947385311127
[6,  6500] loss: 2.148284
1832.9295976161957
[6,  7000] loss: 2.080140
1846.305624961853
[6,  7500] loss: 2.061145
1859.4485111236572
[6,  8000] loss: 2.022388
1872.8263788223267
[6,  8500] loss: 2.041787
1886.1567277908325
[6,  9000] loss: 2.032973
1899.459040403366
[6,  9500] loss: 2.003099
1912.7320489883423
[6, 10000] loss: 2.026463
1925.8884978294373
[6, 10500] loss: 2.118103
1939.1475141048431
[6, 11000] loss: 2.058935
1952.4122700691223
[6, 11500] loss: 2.013337
1965.7277297973633
[6, 12000] loss: 2.017667
1979.0311238765717
[6, 12500] loss: 2.034267
1992.265469789505
Epoch [6] loss: 6266363.258721
[7,   500] loss: 2.013502
2005.758755683899
[7,  1000] loss: 2.002797
2019.1317102909088
[7,  1500] loss: 1.979873
2032.3434524536133
[7,  2000] loss: 2.001824
2045.456874370575
[7,  2500] loss: 2.008295
2058.946519136429
[7,  3000] loss: 1.955994
2072.243237018585
[7,  3500] loss: 1.959831
2085.4138391017914
[7,  4000] loss: 1.921540
2098.6992666721344
[7,  4500] loss: 1.947261
2111.9153785705566
[7,  5000] loss: 1.949672
2125.0977404117584
[7,  5500] loss: 1.869768
2138.288426399231
[7,  6000] loss: 1.934821
2151.437386751175
[7,  6500] loss: 1.938603
2164.7597444057465
[7,  7000] loss: 1.943331
2177.873508453369
[7,  7500] loss: 1.911009
2191.0611679553986
[7,  8000] loss: 1.914219
2204.134640455246
[7,  8500] loss: 1.854678
2217.2158744335175
[7,  9000] loss: 1.939706
2230.51402592659
[7,  9500] loss: 1.893848
2243.8573956489563
[7, 10000] loss: 1.892408
2257.1876957416534
[7, 10500] loss: 1.901284
2270.429228067398
[7, 11000] loss: 1.886359
2283.5973613262177
[7, 11500] loss: 1.922637
2296.843519926071
[7, 12000] loss: 1.873091
2310.123349905014
[7, 12500] loss: 1.888123
2323.2680220603943
Epoch [7] loss: 6045900.972917
[8,   500] loss: 1.902996
2336.8848638534546
[8,  1000] loss: 1.889296
2350.4736046791077
[8,  1500] loss: 1.868952
2363.7762455940247
[8,  2000] loss: 1.861925
2377.158892393112
[8,  2500] loss: 1.858709
2390.3875522613525
[8,  3000] loss: 1.827432
2403.6518750190735
[8,  3500] loss: 1.816652
2416.8210928440094
[8,  4000] loss: 1.841961
2430.10422039032
[8,  4500] loss: 1.873447
2443.3642404079437
[8,  5000] loss: 1.851530
2456.5627884864807
[8,  5500] loss: 1.816810
2469.7616798877716
[8,  6000] loss: 1.897309
2483.0634536743164
[8,  6500] loss: 1.825804
2496.2432577610016
[8,  7000] loss: 1.807641
2509.4175651073456
[8,  7500] loss: 1.807826
2522.6317942142487
[8,  8000] loss: 1.783010
2535.8446278572083
[8,  8500] loss: 1.820885
2549.106650352478
[8,  9000] loss: 1.804591
2562.34903550148
[8,  9500] loss: 1.792407
2575.567042350769
[8, 10000] loss: 1.856142
2588.670390367508
[8, 10500] loss: 1.777756
2601.7001774311066
[8, 11000] loss: 1.828448
2614.824717760086
[8, 11500] loss: 1.854611
2627.893995285034
[8, 12000] loss: 1.781096
2641.019544839859
[8, 12500] loss: 1.820903
2654.2080080509186
Epoch [8] loss: 5759037.361897
[9,   500] loss: 1.784764
2667.5074439048767
[9,  1000] loss: 1.799209
2680.7258961200714
[9,  1500] loss: 1.784263
2694.1554985046387
[9,  2000] loss: 1.792410
2707.3192744255066
[9,  2500] loss: 1.815621
2720.519455909729
[9,  3000] loss: 1.809708
2733.7202758789062
[9,  3500] loss: 1.779568
2746.913914203644
[9,  4000] loss: 1.762675
2760.096084833145
[9,  4500] loss: 1.807251
2773.029522895813
[9,  5000] loss: 1.793890
2786.0037608146667
[9,  5500] loss: 1.742766
2798.950384616852
[9,  6000] loss: 1.792972
2812.0389606952667
[9,  6500] loss: 1.784134
2825.171720981598
[9,  7000] loss: 1.749634
2838.182653903961
[9,  7500] loss: 1.770950
2851.170299053192
[9,  8000] loss: 1.767810
2864.1631002426147
[9,  8500] loss: 1.751304
2877.356945037842
[9,  9000] loss: 1.742923
2890.5547008514404
[9,  9500] loss: 1.747788
2904.2851963043213
[9, 10000] loss: 1.776376
2917.479239463806
[9, 10500] loss: 1.697604
2930.8318586349487
[9, 11000] loss: 1.746878
2944.12934756279
[9, 11500] loss: 1.803897
2957.3394927978516
[9, 12000] loss: 1.806426
2970.551003932953
[9, 12500] loss: 1.880598
2983.8922023773193
Epoch [9] loss: 5570747.498384
[10,   500] loss: 1.833369
2997.3251461982727
[10,  1000] loss: 1.822771
3010.5321877002716
[10,  1500] loss: 1.837259
3023.8255491256714
[10,  2000] loss: 1.791644
3037.1023137569427
[10,  2500] loss: 1.793165
3050.228907585144
[10,  3000] loss: 1.780725
3063.348302602768
[10,  3500] loss: 1.778006
3076.44837975502
[10,  4000] loss: 1.729147
3089.551741838455
[10,  4500] loss: 1.770546
3102.6368317604065
[10,  5000] loss: 1.805172
3115.660503387451
[10,  5500] loss: 1.727720
3128.803572177887
[10,  6000] loss: 1.712539
3141.8914489746094
[10,  6500] loss: 1.742458
3155.0421385765076
[10,  7000] loss: 1.753503
3168.1083331108093
[10,  7500] loss: 1.740246
3181.257601737976
[10,  8000] loss: 1.703284
3194.3713154792786
[10,  8500] loss: 1.748525
3207.442848443985
[10,  9000] loss: 1.786243
3220.5511214733124
[10,  9500] loss: 1.770887
3233.686908721924
[10, 10000] loss: 1.750155
3246.7590510845184
[10, 10500] loss: 1.738888
3259.8190615177155
[10, 11000] loss: 1.720858
3272.967558860779
[10, 11500] loss: 1.722642
3286.0596175193787
[10, 12000] loss: 1.717684
3299.185649871826
[10, 12500] loss: 1.734213
3312.3129324913025
Epoch [10] loss: 5516216.187339
[11,   500] loss: 1.687580
3325.5936801433563
[11,  1000] loss: 1.712322
3338.6753919124603
[11,  1500] loss: 1.707270
3351.7743394374847
[11,  2000] loss: 1.716321
3364.8720529079437
[11,  2500] loss: 1.681939
3377.9897434711456
[11,  3000] loss: 1.692641
3391.0683813095093
[11,  3500] loss: 1.683341
3404.1585023403168
[11,  4000] loss: 1.655958
3417.353862285614
[11,  4500] loss: 1.646412
3430.451882839203
[11,  5000] loss: 1.689660
3443.6735215187073
[11,  5500] loss: 1.688021
3456.7580082416534
[11,  6000] loss: 1.679153
3469.75021982193
[11,  6500] loss: 1.725926
3482.8148834705353
[11,  7000] loss: 1.673717
3495.975341796875
[11,  7500] loss: 1.675976
3509.0994486808777
[11,  8000] loss: 1.673394
3522.142706632614
[11,  8500] loss: 1.710449
3535.333087682724
[11,  9000] loss: 1.696668
3548.5194687843323
[11,  9500] loss: 1.647495
3561.623468875885
[11, 10000] loss: 1.625664
3574.9257864952087
[11, 10500] loss: 1.650308
3588.0297486782074
[11, 11000] loss: 1.672055
3601.174440383911
[11, 11500] loss: 1.668479
3614.3484909534454
[11, 12000] loss: 1.656015
3627.4935483932495
[11, 12500] loss: 1.648801
3640.6220836639404
Epoch [11] loss: 5261663.849489
[12,   500] loss: 1.635042
3653.946373939514
[12,  1000] loss: 1.630928
3667.0268235206604
[12,  1500] loss: 1.661738
3680.3516092300415
[12,  2000] loss: 1.607022
3693.556036710739
[12,  2500] loss: 1.612305
3706.6210770606995
[12,  3000] loss: 1.652885
3719.6221611499786
[12,  3500] loss: 1.631652
3732.7651369571686
[12,  4000] loss: 1.624422
3745.883711576462
[12,  4500] loss: 1.687186
3758.9632411003113
[12,  5000] loss: 1.901131
3772.0468003749847
[12,  5500] loss: 1.807762
3785.1380088329315
[12,  6000] loss: 1.714659
3798.2626383304596
[12,  6500] loss: 1.698031
3811.2707228660583
[12,  7000] loss: 1.676352
3824.318969488144
[12,  7500] loss: 1.654587
3837.4786207675934
[12,  8000] loss: 1.632422
3850.611517906189
[12,  8500] loss: 1.706805
3863.720869779587
[12,  9000] loss: 2.086124
3879.4795367717743
[12,  9500] loss: 1.973601
3892.756410598755
[12, 10000] loss: 1.833922
3905.7526738643646
[12, 10500] loss: 1.799545
3922.954664707184
[12, 11000] loss: 1.812907
3936.196247816086
[12, 11500] loss: 1.792639
3949.26345038414
[12, 12000] loss: 1.734729
3962.4808394908905
[12, 12500] loss: 1.711624
3975.558311700821
Epoch [12] loss: 5405701.858671
[13,   500] loss: 1.749507
3991.7823350429535
[13,  1000] loss: 1.729856
4004.847589492798
[13,  1500] loss: 1.703387
4018.1493442058563
[13,  2000] loss: 1.665252
4031.3351814746857
[13,  2500] loss: 1.717547
4044.442762374878
[13,  3000] loss: 1.770081
4057.5619716644287
[13,  3500] loss: 1.827607
4070.6480910778046
[13,  4000] loss: 1.899309
4083.9000177383423
[13,  4500] loss: 1.802688
4097.033849477768
[13,  5000] loss: 1.755433
4110.222380876541
[13,  5500] loss: 1.737054
4123.29768037796
[13,  6000] loss: 1.830787
4136.410856962204
[13,  6500] loss: 1.789588
4149.554585456848
[13,  7000] loss: 1.740605
4162.708770513535
[13,  7500] loss: 1.811977
4175.848323583603
[13,  8000] loss: 1.747549
4188.921288967133
[13,  8500] loss: 1.743337
4201.970555305481
[13,  9000] loss: 1.738307
4215.157094240189
[13,  9500] loss: 1.694491
4228.264588356018
[13, 10000] loss: 1.710145
4241.481839418411
[13, 10500] loss: 1.711907
4254.522330522537
[13, 11000] loss: 1.679509
4267.540884017944
[13, 11500] loss: 1.676375
4280.541842460632
[13, 12000] loss: 1.719046
4293.596002817154
[13, 12500] loss: 1.664451
4306.845165252686
Epoch [13] loss: 5453773.223299
[14,   500] loss: 1.639513
4320.2648413181305
[14,  1000] loss: 1.627375
4333.42581152916
[14,  1500] loss: 1.634391
4346.598055839539
[14,  2000] loss: 1.631817
4359.691583156586
[14,  2500] loss: 1.666926
4372.781668186188
[14,  3000] loss: 1.642502
4385.862408399582
[14,  3500] loss: 1.622529
4398.971354722977
[14,  4000] loss: 1.613518
4412.091669797897
[14,  4500] loss: 1.622850
4425.240147352219
[14,  5000] loss: 1.781261
4438.436025619507
[14,  5500] loss: 1.749143
4451.672254085541
[14,  6000] loss: 1.683030
4464.831048965454
[14,  6500] loss: 1.645138
4477.912853479385
[14,  7000] loss: 1.604267
4490.995856285095
[14,  7500] loss: 1.650621
4504.10897397995
[14,  8000] loss: 1.658830
4517.134179353714
[14,  8500] loss: 1.657674
4530.293399333954
[14,  9000] loss: 1.631538
4543.468097686768
[14,  9500] loss: 1.658821
4556.5137004852295
[14, 10000] loss: 1.597510
4569.742425441742
[14, 10500] loss: 1.641718
4582.987243175507
[14, 11000] loss: 1.646357
4596.074373722076
[14, 11500] loss: 1.605427
4609.20339512825
[14, 12000] loss: 1.594401
4622.347233533859
[14, 12500] loss: 1.582190
4635.42088508606
Epoch [14] loss: 5156215.108729
[15,   500] loss: 1.582499
4648.69579744339
[15,  1000] loss: 1.601777
4661.689064025879
[15,  1500] loss: 1.623479
4674.8125920295715
[15,  2000] loss: 1.617361
4688.036252737045
[15,  2500] loss: 1.576655
4701.182008266449
[15,  3000] loss: 1.613669
4714.2809891700745
[15,  3500] loss: 1.575951
4727.394288063049
[15,  4000] loss: 1.600700
4740.579111099243
[15,  4500] loss: 1.590943
4753.689186573029
[15,  5000] loss: 1.637716
4766.767920732498
[15,  5500] loss: 1.619393
4779.828501939774
[15,  6000] loss: 1.609499
4792.935222387314
[15,  6500] loss: 1.583817
4805.998892068863
[15,  7000] loss: 1.613259
4819.046762228012
[15,  7500] loss: 1.595185
4832.162457227707
[15,  8000] loss: 1.600563
4845.269767522812
[15,  8500] loss: 1.621012
4858.3485019207
[15,  9000] loss: 1.577021
4871.483152151108
[15,  9500] loss: 1.546694
4884.59011721611
[15, 10000] loss: 1.565365
4897.638924360275
[15, 10500] loss: 1.566279
4910.68327832222
[15, 11000] loss: 1.581957
4923.817267179489
[15, 11500] loss: 1.613985
4936.978127479553
[15, 12000] loss: 1.565940
4950.076313018799
[15, 12500] loss: 1.530513
4963.217127084732
Epoch [15] loss: 4987876.987265
[16,   500] loss: 1.518886
4976.4793009758
[16,  1000] loss: 1.582474
4989.648478984833
[16,  1500] loss: 1.490493
5002.86430311203
[16,  2000] loss: 1.545810
5016.015119314194
[16,  2500] loss: 1.529169
5029.136579751968
[16,  3000] loss: 1.532603
5042.4975917339325
[16,  3500] loss: 1.539392
5055.807344198227
[16,  4000] loss: 1.532663
5069.270197153091
[16,  4500] loss: 1.569327
5082.711487054825
[16,  5000] loss: 1.556929
5096.077885389328
[16,  5500] loss: 1.535179
5109.545552492142
[16,  6000] loss: 1.542668
5122.925040483475
[16,  6500] loss: 1.561242
5136.258073806763
[16,  7000] loss: 1.532269
5149.537649869919
[16,  7500] loss: 1.533302
5162.873811244965
[16,  8000] loss: 1.510238
5176.178786754608
[16,  8500] loss: 1.534293
5189.327915906906
[16,  9000] loss: 1.535973
5202.607929468155
[16,  9500] loss: 1.505405
5216.058496952057
[16, 10000] loss: 1.502121
5229.628525257111
[16, 10500] loss: 1.511111
5243.347908496857
[16, 11000] loss: 1.475194
5256.753192424774
[16, 11500] loss: 1.559995
5270.126328229904
[16, 12000] loss: 1.557149
5283.880903959274
[16, 12500] loss: 1.543587
5297.337092876434
Epoch [16] loss: 4807427.734215
[17,   500] loss: 1.485626
5310.866910457611
[17,  1000] loss: 1.545460
5324.26469874382
[17,  1500] loss: 1.483788
5337.599056959152
[17,  2000] loss: 1.487315
5351.053186416626
[17,  2500] loss: 1.500254
5364.51929807663
[17,  3000] loss: 1.504360
5378.029953718185
[17,  3500] loss: 1.463790
5391.26526594162
[17,  4000] loss: 1.532989
5404.473748922348
[17,  4500] loss: 1.493539
5418.20759510994
[17,  5000] loss: 1.481330
5431.713674545288
[17,  5500] loss: 1.533282
5445.344742536545
[17,  6000] loss: 1.536010
5458.817573308945
[17,  6500] loss: 1.459348
5472.3039882183075
[17,  7000] loss: 1.516250
5485.765889644623
[17,  7500] loss: 1.529545
5499.130241155624
[17,  8000] loss: 1.508009
5512.598902225494
[17,  8500] loss: 1.502763
5526.1770532131195
[17,  9000] loss: 1.469353
5539.542804718018
[17,  9500] loss: 1.495822
5552.9767599105835
[17, 10000] loss: 1.548024
5566.350423812866
[17, 10500] loss: 1.489269
5579.53267621994
[17, 11000] loss: 1.511957
5592.945516347885
[17, 11500] loss: 1.487943
5606.260075807571
[17, 12000] loss: 1.502909
5620.040240764618
[17, 12500] loss: 1.491607
5633.305417537689
Epoch [17] loss: 4700893.842913
[18,   500] loss: 1.452680
5646.7952489852905
[18,  1000] loss: 1.562929
5660.132338285446
[18,  1500] loss: 1.527166
5673.571147203445
[18,  2000] loss: 1.530841
5687.090835571289
[18,  2500] loss: 1.510583
5700.50265955925
[18,  3000] loss: 1.477956
5713.826813697815
[18,  3500] loss: 1.514625
5727.168433666229
[18,  4000] loss: 1.501854
5740.326584339142
[18,  4500] loss: 1.553069
5753.703643321991
[18,  5000] loss: 1.481557
5766.933377742767
[18,  5500] loss: 1.517799
5780.2068400383
[18,  6000] loss: 1.550385
5793.570436000824
[18,  6500] loss: 1.499788
5806.890590190887
[18,  7000] loss: 1.516101
5820.1076810359955
[18,  7500] loss: 1.491478
5833.324687004089
[18,  8000] loss: 1.472978
5846.80934548378
[18,  8500] loss: 1.524198
5860.138896226883
[18,  9000] loss: 1.663387
5873.593290567398
[18,  9500] loss: 1.678371
5886.915811061859
[18, 10000] loss: 1.758879
5900.312017202377
[18, 10500] loss: 2.052083
5913.725383043289
[18, 11000] loss: 1.990416
5926.940919876099
[18, 11500] loss: 1.939229
5940.147214174271
[18, 12000] loss: 1.895910
5953.453889846802
[18, 12500] loss: 1.825081
5966.691653728485
Epoch [18] loss: 5053259.337934
[19,   500] loss: 1.804323
5979.966924905777
[19,  1000] loss: 1.810045
5993.331958770752
[19,  1500] loss: 1.776064
6006.812061548233
[19,  2000] loss: 1.778677
6020.138496637344
[19,  2500] loss: 1.770717
6033.522808551788
[19,  3000] loss: 1.701204
6046.780036449432
[19,  3500] loss: 1.746567
6060.21835064888
[19,  4000] loss: 1.731390
6073.523619174957
[19,  4500] loss: 1.734132
6086.759045124054
[19,  5000] loss: 1.709110
6099.925575971603
[19,  5500] loss: 1.685461
6113.263670682907
[19,  6000] loss: 1.700974
6126.547559976578
[19,  6500] loss: 1.689315
6139.933611392975
[19,  7000] loss: 1.723919
6153.431367397308
[19,  7500] loss: 1.678698
6166.631925821304
[19,  8000] loss: 1.667141
6180.113193511963
[19,  8500] loss: 1.680616
6193.594174861908
[19,  9000] loss: 1.661763
6206.961040973663
[19,  9500] loss: 1.662407
6220.108848571777
[19, 10000] loss: 1.663396
6233.2961094379425
[19, 10500] loss: 1.596139
6246.707593679428
[19, 11000] loss: 1.638783
6259.917762994766
[19, 11500] loss: 1.638980
6273.1531817913055
[19, 12000] loss: 1.635963
6286.298277139664
[19, 12500] loss: 1.597282
6299.586848020554
Epoch [19] loss: 5319706.854050
[20,   500] loss: 1.631259
6313.181176662445
[20,  1000] loss: 1.618683
6326.401034355164
[20,  1500] loss: 1.621857
6339.6790244579315
[20,  2000] loss: 1.615254
6353.035965681076
[20,  2500] loss: 1.600132
6366.385178089142
[20,  3000] loss: 1.600438
6379.609479427338
[20,  3500] loss: 1.652132
6392.997176647186
[20,  4000] loss: 1.579771
6406.306222200394
[20,  4500] loss: 1.578693
6419.538322925568
[20,  5000] loss: 1.581606
6432.762941360474
[20,  5500] loss: 1.603033
6446.037195920944
[20,  6000] loss: 1.567057
6459.427045106888
[20,  6500] loss: 1.587926
6472.672522783279
[20,  7000] loss: 1.566662
6485.935993432999
[20,  7500] loss: 1.524846
6499.152339696884
[20,  8000] loss: 1.618083
6512.519092082977
[20,  8500] loss: 1.622636
6525.921565532684
[20,  9000] loss: 1.575288
6539.07793545723
[20,  9500] loss: 1.605279
6552.300612211227
[20, 10000] loss: 1.558071
6565.553283929825
[20, 10500] loss: 1.564934
6578.853757381439
[20, 11000] loss: 1.569387
6592.37260055542
[20, 11500] loss: 1.583688
6605.559150457382
[20, 12000] loss: 1.606211
6618.797325849533
[20, 12500] loss: 1.573381
6632.040004253387
Epoch [20] loss: 4988415.617475
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-26 02:26:30.024801
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-0.5441, -1.2916,  0.3420,  1.5566,  0.1083,  1.8114,  0.2191, -0.2601,
         -0.4673, -1.2858],
        [ 2.0986,  2.8732, -1.7826, -1.8914, -1.3212, -2.5727, -1.9427, -1.5751,
          3.2969,  2.7997],
        [ 1.4260,  1.2270, -1.0865, -0.7944, -1.0295, -1.3118, -0.6313, -1.7331,
          3.3797,  0.5875],
        [ 2.5367,  0.3103, -0.4983, -1.4189, -0.1206, -1.7796, -1.3355, -0.9701,
          2.5173,  0.7321]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    dog  ship  ship plane
Accuracy of the network on the 4000.0 test images: 40 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 16, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=16, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=16, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
3125
 ship   car   dog truck
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Number of parameters: 23528522 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582680477.628893
[1,   500] loss: 2.419078
11.974313259124756
[1,  1000] loss: 2.250983
23.607816696166992
[1,  1500] loss: 2.170295
35.322861433029175
[1,  2000] loss: 2.134142
46.98519682884216
[1,  2500] loss: 2.079154
58.63039755821228
[1,  3000] loss: 2.045551
70.25467538833618
Epoch [1] loss: 1671603.672006
[2,   500] loss: 1.984120
85.0909206867218
[2,  1000] loss: 1.946655
96.76891136169434
[2,  1500] loss: 1.908031
108.39506340026855
[2,  2000] loss: 1.894533
120.04105067253113
[2,  2500] loss: 1.913360
131.66624236106873
[2,  3000] loss: 1.878645
143.29305911064148
Epoch [2] loss: 1460599.666107
[3,   500] loss: 1.824700
157.88478350639343
[3,  1000] loss: 1.816573
169.44394779205322
[3,  1500] loss: 1.803172
181.0340793132782
[3,  2000] loss: 1.784624
192.73827862739563
[3,  2500] loss: 1.784257
204.3093388080597
[3,  3000] loss: 1.754005
216.08911442756653
Epoch [3] loss: 1364024.486686
[4,   500] loss: 1.723912
230.81999444961548
[4,  1000] loss: 1.708791
242.45399689674377
[4,  1500] loss: 1.681608
254.04180884361267
[4,  2000] loss: 1.696249
265.6251919269562
[4,  2500] loss: 1.691341
277.3268144130707
[4,  3000] loss: 1.682027
288.9902582168579
Epoch [4] loss: 1284198.050727
[5,   500] loss: 1.628506
303.5767295360565
[5,  1000] loss: 1.622276
315.19520926475525
[5,  1500] loss: 1.617332
326.7739722728729
[5,  2000] loss: 1.594047
338.49351978302
[5,  2500] loss: 1.610734
350.0647671222687
[5,  3000] loss: 1.625856
361.70606207847595
Epoch [5] loss: 1226446.492544
[6,   500] loss: 1.564107
376.2734935283661
[6,  1000] loss: 1.565500
387.7681381702423
[6,  1500] loss: 1.570288
399.4027545452118
[6,  2000] loss: 1.542953
410.994517326355
[6,  2500] loss: 1.530733
422.5910382270813
[6,  3000] loss: 1.536147
434.18420577049255
Epoch [6] loss: 1178101.236841
[7,   500] loss: 1.494855
448.94907307624817
[7,  1000] loss: 1.504879
460.6090805530548
[7,  1500] loss: 1.485724
472.42299365997314
[7,  2000] loss: 1.486921
484.0788896083832
[7,  2500] loss: 1.470662
495.7453987598419
[7,  3000] loss: 1.479355
507.5703880786896
Epoch [7] loss: 1130324.278804
[8,   500] loss: 1.446623
522.1714038848877
[8,  1000] loss: 1.430750
533.6176183223724
[8,  1500] loss: 1.432715
545.0274965763092
[8,  2000] loss: 1.437282
556.5059254169464
[8,  2500] loss: 1.434785
568.0369462966919
[8,  3000] loss: 1.440239
579.5244464874268
Epoch [8] loss: 1089782.228923
[9,   500] loss: 1.394395
593.9782729148865
[9,  1000] loss: 1.378641
605.5274398326874
[9,  1500] loss: 1.395280
617.0142359733582
[9,  2000] loss: 1.382890
628.5114879608154
[9,  2500] loss: 1.408592
640.0430707931519
[9,  3000] loss: 1.400107
651.5503351688385
Epoch [9] loss: 1057761.268026
[10,   500] loss: 1.342474
666.1409363746643
[10,  1000] loss: 1.346781
677.7350335121155
[10,  1500] loss: 1.353317
689.3152482509613
[10,  2000] loss: 1.349400
700.9431536197662
[10,  2500] loss: 1.368826
712.5547823905945
[10,  3000] loss: 1.333461
724.105539560318
Epoch [10] loss: 1027708.148636
[11,   500] loss: 1.314832
738.5555305480957
[11,  1000] loss: 1.298716
749.9941809177399
[11,  1500] loss: 1.294954
761.5182638168335
[11,  2000] loss: 1.317552
772.913822889328
[11,  2500] loss: 1.307824
784.368702173233
[11,  3000] loss: 1.306825
795.8537344932556
Epoch [11] loss: 993898.217264
[12,   500] loss: 1.250655
810.309770822525
[12,  1000] loss: 1.247239
821.8084592819214
[12,  1500] loss: 1.260857
833.3068835735321
[12,  2000] loss: 1.275130
844.807671546936
[12,  2500] loss: 1.271731
856.3034155368805
[12,  3000] loss: 1.257558
867.7809107303619
Epoch [12] loss: 957960.995662
[13,   500] loss: 1.234153
882.275309085846
[13,  1000] loss: 1.195527
893.7195911407471
[13,  1500] loss: 1.225245
905.2524375915527
[13,  2000] loss: 1.198380
916.7305173873901
[13,  2500] loss: 1.219307
928.1947700977325
[13,  3000] loss: 1.238061
939.6406054496765
Epoch [13] loss: 928219.822713
[14,   500] loss: 1.158552
954.1890950202942
[14,  1000] loss: 1.183819
965.660477399826
[14,  1500] loss: 1.179478
977.1520426273346
[14,  2000] loss: 1.168730
988.582846403122
[14,  2500] loss: 1.183779
1000.0572855472565
[14,  3000] loss: 1.196852
1011.5729267597198
Epoch [14] loss: 897996.915899
[15,   500] loss: 1.129412
1026.1993947029114
[15,  1000] loss: 1.143625
1037.7410852909088
[15,  1500] loss: 1.136896
1049.2645301818848
[15,  2000] loss: 1.129218
1060.8123652935028
[15,  2500] loss: 1.141590
1072.4099581241608
[15,  3000] loss: 1.159259
1083.9505581855774
Epoch [15] loss: 867016.171619
[16,   500] loss: 1.081420
1098.4829409122467
[16,  1000] loss: 1.095118
1109.9788966178894
[16,  1500] loss: 1.095890
1121.4834611415863
[16,  2000] loss: 1.096948
1132.9956855773926
[16,  2500] loss: 1.090994
1144.4587116241455
[16,  3000] loss: 1.093832
1155.866854429245
Epoch [16] loss: 831294.967761
[17,   500] loss: 1.079056
1170.391005039215
[17,  1000] loss: 1.062315
1181.794486284256
[17,  1500] loss: 1.059918
1193.1857879161835
[17,  2000] loss: 1.056290
1204.701403617859
[17,  2500] loss: 1.070409
1216.1429481506348
[17,  3000] loss: 1.047909
1227.5891981124878
Epoch [17] loss: 805076.096033
[18,   500] loss: 1.008402
1242.0061781406403
[18,  1000] loss: 1.001749
1253.4671413898468
[18,  1500] loss: 1.020957
1264.9118292331696
[18,  2000] loss: 1.041657
1276.3859903812408
[18,  2500] loss: 1.032855
1287.8295304775238
[18,  3000] loss: 1.034051
1299.2751350402832
Epoch [18] loss: 774968.090495
[19,   500] loss: 0.963829
1313.7300338745117
[19,  1000] loss: 0.995652
1325.2668406963348
[19,  1500] loss: 0.992578
1336.7152400016785
[19,  2000] loss: 0.974506
1348.1741743087769
[19,  2500] loss: 0.978943
1359.628746509552
[19,  3000] loss: 0.990194
1371.084801197052
Epoch [19] loss: 743098.362707
[20,   500] loss: 0.942379
1385.571166753769
[20,  1000] loss: 0.937467
1397.0505170822144
[20,  1500] loss: 0.924334
1408.542030096054
[20,  2000] loss: 0.951136
1420.0825333595276
[20,  2500] loss: 0.962816
1431.5217292308807
[20,  3000] loss: 0.967956
1442.9344971179962
Epoch [20] loss: 719974.933404
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-26 02:52:03.464392
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-3.5754e-01, -2.7088e+00,  3.5199e-01,  2.6994e+00, -7.3732e-01,
          3.5247e+00, -6.7384e-01,  9.2796e-01, -2.0286e-01, -1.1603e+00],
        [ 2.1973e+00,  6.8390e+00, -1.0124e+00, -3.2410e+00, -2.2666e+00,
         -3.4602e+00, -4.4296e+00, -4.4692e+00,  2.6677e+00,  2.0070e+00],
        [ 1.8470e+00,  4.5233e+00, -1.2169e+00, -1.5658e+00, -2.2431e+00,
         -1.2610e+00, -2.7845e+00, -4.0868e+00,  4.5192e+00,  2.1758e+00],
        [ 3.1746e+00,  1.0812e+00, -2.1028e-01, -1.4230e+00,  4.6759e-01,
         -2.2156e+00, -3.4025e+00, -6.8594e-01,  5.6863e-01, -4.7336e-01],
        [-4.0313e+00, -7.1908e+00,  3.2526e+00,  3.7352e+00,  5.3845e+00,
          2.2080e+00,  6.8903e+00, -5.2588e-01, -5.4967e+00, -4.9789e+00],
        [-8.4898e-04, -5.0793e+00,  1.4867e+00,  1.8876e+00,  2.2738e+00,
          2.0472e+00,  6.5247e+00, -9.6736e-02, -4.7460e+00, -3.1536e+00],
        [-9.3737e-01,  3.3608e+00, -9.9804e-01,  2.5521e+00, -2.6631e+00,
          1.5901e+00, -1.2027e+00, -3.7240e-01, -1.0246e+00,  2.0408e+00],
        [-5.4469e-01, -7.8327e+00,  3.5733e+00,  1.3297e+00,  4.1426e+00,
          5.1269e-01,  3.4409e+00,  9.2187e-01, -3.1855e+00, -3.7974e+00],
        [-1.4664e+00, -7.1630e+00,  1.8114e+00,  3.4490e+00,  3.6801e+00,
          3.5574e+00,  4.0719e+00,  3.2281e+00, -5.6904e+00, -4.9144e+00],
        [-1.9556e+00,  9.9164e+00, -2.0096e+00, -8.5039e-01, -2.2922e+00,
         -1.3460e+00, -9.2778e-01, -5.8789e+00,  1.9152e+00,  3.8629e+00],
        [ 2.1367e+00, -4.1458e+00,  8.9470e-01,  9.6071e-01, -1.6055e-01,
         -8.2107e-02,  9.4996e-01, -7.8934e-01,  2.3577e+00, -1.7371e+00],
        [-1.3407e+00,  4.2393e+00, -3.6291e-01, -1.2698e+00, -1.8106e+00,
         -9.5380e-01, -1.8052e+00, -2.2824e+00, -2.1329e+00,  8.0869e+00],
        [-3.4881e+00,  6.6495e-01, -5.4112e-01,  1.2294e+00,  8.8512e-01,
          1.1783e+00,  3.3467e+00,  6.5679e-01, -3.7131e+00, -6.6679e-01],
        [-1.9157e-01, -2.2016e+00, -6.7109e-01,  1.0412e+00, -2.9445e-01,
          1.1691e+00, -4.5594e-01,  5.7144e+00, -2.5240e+00, -3.4126e+00],
        [-7.3104e-01,  5.4177e+00,  1.1517e+00, -5.8038e-01, -2.3863e+00,
         -3.4748e-01,  1.4399e+00, -3.8961e+00, -2.5858e+00,  5.0959e+00],
        [ 8.0431e-01,  3.6505e-01,  2.7299e-01, -1.6321e+00,  2.0132e-01,
         -1.6631e+00, -2.9024e-01, -1.3525e+00,  4.3947e+00, -1.1412e+00]],
       device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    dog   car   car plane
Accuracy of the network on the 4000.0 test images: 58 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.9, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=16, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0.9, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
  car  frog  ship   dog
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Number of parameters: 23528522 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0.9, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582681936.747229
[1,   500] loss: 2.441059
12.188285112380981
[1,  1000] loss: 2.286335
24.308257818222046
[1,  1500] loss: 2.233003
36.40310764312744
[1,  2000] loss: 2.203115
48.51296091079712
[1,  2500] loss: 2.203554
60.55044102668762
[1,  3000] loss: 2.215882
72.56118988990784
[1,  3500] loss: 2.255267
84.64147186279297
[1,  4000] loss: 2.301372
96.7407295703888
[1,  4500] loss: 2.302456
108.78385519981384
[1,  5000] loss: 2.302623
120.82772374153137
[1,  5500] loss: 2.302531
132.8924491405487
[1,  6000] loss: 2.302456
144.99800515174866
[1,  6500] loss: 2.302562
157.04225516319275
[1,  7000] loss: 2.302584
169.1782684326172
[1,  7500] loss: 2.302584
181.30781173706055
[1,  8000] loss: 2.302729
193.43994736671448
[1,  8500] loss: 2.302577
205.42841482162476
[1,  9000] loss: 2.302632
217.43456315994263
[1,  9500] loss: 2.302627
229.51866936683655
[1, 10000] loss: 2.302428
241.586332321167
[1, 10500] loss: 2.302681
253.61379551887512
[1, 11000] loss: 2.302628
265.7061417102814
[1, 11500] loss: 2.302513
277.80040764808655
[1, 12000] loss: 2.302589
289.91924357414246
[1, 12500] loss: 2.302649
302.0159709453583
Epoch [1] loss: 7183455.384456
[2,   500] loss: 2.302558
314.34492802619934
[2,  1000] loss: 2.302699
326.41060853004456
[2,  1500] loss: 2.302670
338.4617190361023
[2,  2000] loss: 2.302601
350.5009036064148
[2,  2500] loss: 2.302625
362.59106636047363
[2,  3000] loss: 2.302653
374.67038202285767
[2,  3500] loss: 2.302545
386.821485042572
[2,  4000] loss: 2.302610
398.9823634624481
[2,  4500] loss: 2.302617
411.05668592453003
[2,  5000] loss: 2.302701
423.0694885253906
[2,  5500] loss: 2.302619
435.1760413646698
[2,  6000] loss: 2.302684
447.3026125431061
[2,  6500] loss: 2.302598
459.41595792770386
[2,  7000] loss: 2.302560
471.63374948501587
[2,  7500] loss: 2.302584
483.77417612075806
[2,  8000] loss: 2.302563
495.8081784248352
[2,  8500] loss: 2.302545
507.9736816883087
[2,  9000] loss: 2.302548
520.0110125541687
[2,  9500] loss: 2.302572
532.1962199211121
[2, 10000] loss: 2.302658
544.3449423313141
[2, 10500] loss: 2.302562
556.4797711372375
[2, 11000] loss: 2.302647
568.6121113300323
[2, 11500] loss: 2.302598
580.7184450626373
[2, 12000] loss: 2.302607
592.8964688777924
[2, 12500] loss: 2.302583
604.987556219101
Epoch [2] loss: 7210025.462691
[3,   500] loss: 2.302668
617.2216687202454
[3,  1000] loss: 2.302655
629.3787062168121
[3,  1500] loss: 2.302602
641.6130776405334
[3,  2000] loss: 2.302625
653.7596981525421
[3,  2500] loss: 2.302565
665.848356962204
[3,  3000] loss: 2.302615
677.8970370292664
[3,  3500] loss: 2.302639
689.9527671337128
[3,  4000] loss: 2.302691
702.0129199028015
[3,  4500] loss: 2.302646
714.0708522796631
[3,  5000] loss: 2.302579
726.1718258857727
[3,  5500] loss: 2.302498
738.3525013923645
[3,  6000] loss: 2.302767
750.6524276733398
[3,  6500] loss: 2.302668
762.7773659229279
[3,  7000] loss: 2.302534
774.9391801357269
[3,  7500] loss: 2.302561
787.1510400772095
[3,  8000] loss: 2.302581
799.3452672958374
[3,  8500] loss: 2.302659
811.5375280380249
[3,  9000] loss: 2.302693
823.7062213420868
[3,  9500] loss: 2.302666
835.8467833995819
[3, 10000] loss: 2.302643
847.940680027008
[3, 10500] loss: 2.302620
860.0015277862549
[3, 11000] loss: 2.302695
872.076648235321
[3, 11500] loss: 2.302582
884.2435541152954
[3, 12000] loss: 2.302590
896.3922352790833
[3, 12500] loss: 2.302626
908.5476610660553
Epoch [3] loss: 7210099.249563
[4,   500] loss: 2.302608
920.7892854213715
[4,  1000] loss: 2.302488
932.9421784877777
[4,  1500] loss: 2.302697
945.1406779289246
[4,  2000] loss: 2.302697
957.2903156280518
[4,  2500] loss: 2.302601
969.418970823288
[4,  3000] loss: 2.302646
981.5486907958984
[4,  3500] loss: 2.302603
993.6798293590546
[4,  4000] loss: 2.302577
1005.7840750217438
[4,  4500] loss: 2.302642
1017.857709646225
[4,  5000] loss: 2.302618
1029.9389817714691
[4,  5500] loss: 2.302539
1042.0295317173004
[4,  6000] loss: 2.302638
1054.1138241291046
[4,  6500] loss: 2.302645
1066.28582239151
[4,  7000] loss: 2.302517
1078.4486639499664
[4,  7500] loss: 2.302588
1090.5518786907196
[4,  8000] loss: 2.302684
1102.6838834285736
[4,  8500] loss: 2.302667
1114.8027732372284
[4,  9000] loss: 2.302635
1126.8621499538422
[4,  9500] loss: 2.302585
1138.9229600429535
[4, 10000] loss: 2.302512
1151.0236322879791
[4, 10500] loss: 2.302496
1163.1751673221588
[4, 11000] loss: 2.302630
1175.3170459270477
[4, 11500] loss: 2.302610
1187.376394033432
[4, 12000] loss: 2.302689
1199.526298046112
[4, 12500] loss: 2.302644
1211.729552268982
Epoch [4] loss: 7210033.598120
[5,   500] loss: 2.302574
1223.993238210678
[5,  1000] loss: 2.302575
1236.0901782512665
[5,  1500] loss: 2.302542
1248.1289155483246
[5,  2000] loss: 2.302659
1260.3229196071625
[5,  2500] loss: 2.302634
1272.484896183014
[5,  3000] loss: 2.302692
1284.540005683899
[5,  3500] loss: 2.302572
1296.6019008159637
[5,  4000] loss: 2.302693
1308.6838159561157
[5,  4500] loss: 2.302614
1320.8209481239319
[5,  5000] loss: 2.302596
1332.9649903774261
[5,  5500] loss: 2.302614
1345.0295848846436
[5,  6000] loss: 2.302644
1357.1464972496033
[5,  6500] loss: 2.302644
1369.282175540924
[5,  7000] loss: 2.302660
1381.426626443863
[5,  7500] loss: 2.302601
1393.5879468917847
[5,  8000] loss: 2.302665
1405.6938030719757
[5,  8500] loss: 2.302589
1417.765648841858
[5,  9000] loss: 2.302620
1429.8005990982056
[5,  9500] loss: 2.302524
1441.8968744277954
[5, 10000] loss: 2.302624
1454.0835585594177
[5, 10500] loss: 2.302595
1466.207157611847
[5, 11000] loss: 2.302669
1478.3490629196167
[5, 11500] loss: 2.302671
1490.4854805469513
[5, 12000] loss: 2.302553
1502.5809063911438
[5, 12500] loss: 2.302585
1514.693205356598
Epoch [5] loss: 7210081.923518
[6,   500] loss: 2.302700
1526.9407322406769
[6,  1000] loss: 2.302619
1539.064989566803
[6,  1500] loss: 2.302591
1551.2874941825867
[6,  2000] loss: 2.302665
1563.4279732704163
[6,  2500] loss: 2.302586
1575.593130350113
[6,  3000] loss: 2.302631
1587.7673041820526
[6,  3500] loss: 2.302582
1599.8498857021332
[6,  4000] loss: 2.302497
1611.9165818691254
[6,  4500] loss: 2.302563
1624.0308923721313
[6,  5000] loss: 2.302570
1636.0431616306305
[6,  5500] loss: 2.302570
1648.1838760375977
[6,  6000] loss: 2.302636
1660.2977113723755
[6,  6500] loss: 2.302656
1672.4396483898163
[6,  7000] loss: 2.302545
1684.6111674308777
[6,  7500] loss: 2.302615
1696.7751007080078
[6,  8000] loss: 2.302611
1708.9939556121826
[6,  8500] loss: 2.302557
1721.2472088336945
[6,  9000] loss: 2.302608
1733.6058304309845
[6,  9500] loss: 2.302523
1745.853731393814
[6, 10000] loss: 2.302559
1758.0155301094055
[6, 10500] loss: 2.302454
1770.223028421402
[6, 11000] loss: 2.302731
1782.4418487548828
[6, 11500] loss: 2.302642
1794.820864200592
[6, 12000] loss: 2.302602
1807.169397354126
[6, 12500] loss: 2.302665
1819.4605765342712
Epoch [6] loss: 7210038.422119
[7,   500] loss: 2.302665
1831.863183259964
[7,  1000] loss: 2.302607
1844.0857028961182
[7,  1500] loss: 2.302638
1856.4208388328552
[7,  2000] loss: 2.302507
1868.7042543888092
[7,  2500] loss: 2.302608
1880.853655576706
[7,  3000] loss: 2.302689
1892.9905049800873
[7,  3500] loss: 2.302587
1905.1638214588165
[7,  4000] loss: 2.302629
1917.4237315654755
[7,  4500] loss: 2.302587
1929.6576781272888
[7,  5000] loss: 2.302544
1941.798255443573
[7,  5500] loss: 2.302522
1954.012915611267
[7,  6000] loss: 2.302561
1966.20086145401
[7,  6500] loss: 2.302627
1978.3465390205383
[7,  7000] loss: 2.302606
1990.5438332557678
[7,  7500] loss: 2.302679
2002.6714506149292
[7,  8000] loss: 2.302630
2014.7793192863464
[7,  8500] loss: 2.302581
2027.0053069591522
[7,  9000] loss: 2.302618
2039.2685794830322
[7,  9500] loss: 2.302641
2051.453379392624
[7, 10000] loss: 2.302649
2063.6786189079285
[7, 10500] loss: 2.302667
2075.8439280986786
[7, 11000] loss: 2.302647
2088.0173676013947
[7, 11500] loss: 2.302613
2100.1771216392517
[7, 12000] loss: 2.302624
2112.391588449478
[7, 12500] loss: 2.302561
2124.545809984207
Epoch [7] loss: 7210047.700873
[8,   500] loss: 2.302560
2136.951320886612
[8,  1000] loss: 2.302593
2149.1137058734894
[8,  1500] loss: 2.302523
2161.284782886505
[8,  2000] loss: 2.302539
2173.6007311344147
[8,  2500] loss: 2.302529
2185.7061400413513
[8,  3000] loss: 2.302702
2197.9001734256744
[8,  3500] loss: 2.302543
2210.1314027309418
[8,  4000] loss: 2.302646
2222.2969224452972
[8,  4500] loss: 2.302601
2234.4349410533905
[8,  5000] loss: 2.302516
2246.741934776306
[8,  5500] loss: 2.302624
2258.9241333007812
[8,  6000] loss: 2.302680
2271.1907107830048
[8,  6500] loss: 2.302498
2283.403512239456
[8,  7000] loss: 2.302615
2295.630685567856
[8,  7500] loss: 2.302692
2307.770516872406
[8,  8000] loss: 2.302621
2319.894864320755
[8,  8500] loss: 2.302689
2332.0500020980835
[8,  9000] loss: 2.302597
2344.237054824829
[8,  9500] loss: 2.302567
2356.3051154613495
[8, 10000] loss: 2.302626
2368.405848264694
[8, 10500] loss: 2.302442
2380.537953853607
[8, 11000] loss: 2.302691
2392.692483663559
[8, 11500] loss: 2.302412
2404.851131916046
[8, 12000] loss: 2.302634
2416.9648249149323
[8, 12500] loss: 2.302742
2429.1541793346405
Epoch [8] loss: 7209981.162631
[9,   500] loss: 2.302550
2441.528562068939
[9,  1000] loss: 2.302623
2453.6216225624084
[9,  1500] loss: 2.302493
2465.8112149238586
[9,  2000] loss: 2.302652
2478.021831512451
[9,  2500] loss: 2.302662
2490.3167793750763
[9,  3000] loss: 2.302667
2502.485047340393
[9,  3500] loss: 2.302622
2514.6733112335205
[9,  4000] loss: 2.302502
2527.019690990448
[9,  4500] loss: 2.302622
2539.134106874466
[9,  5000] loss: 2.302562
2551.24977684021
[9,  5500] loss: 2.302727
2563.331201314926
[9,  6000] loss: 2.302570
2575.6426424980164
[9,  6500] loss: 2.302608
2587.795355796814
[9,  7000] loss: 2.302704
2599.917309999466
[9,  7500] loss: 2.302630
2612.2054767608643
[9,  8000] loss: 2.302667
2624.43887925148
[9,  8500] loss: 2.302562
2636.7378273010254
[9,  9000] loss: 2.302573
2649.03204369545
[9,  9500] loss: 2.302550
2661.1254897117615
[9, 10000] loss: 2.302525
2673.3015854358673
[9, 10500] loss: 2.302630
2685.502145767212
[9, 11000] loss: 2.302638
2697.6717524528503
[9, 11500] loss: 2.302670
2709.9822793006897
[9, 12000] loss: 2.302585
2722.229095220566
[9, 12500] loss: 2.302670
2734.5316264629364
Epoch [9] loss: 7210045.159294
[10,   500] loss: 2.302614
2746.9065194129944
[10,  1000] loss: 2.302518
2759.1805350780487
[10,  1500] loss: 2.302586
2771.5376868247986
[10,  2000] loss: 2.302503
2783.643035173416
[10,  2500] loss: 2.302609
2795.8265063762665
[10,  3000] loss: 2.302608
2808.1300241947174
[10,  3500] loss: 2.302626
2820.3633918762207
[10,  4000] loss: 2.302626
2832.5921754837036
[10,  4500] loss: 2.302576
2844.896994113922
[10,  5000] loss: 2.302556
2857.1879284381866
[10,  5500] loss: 2.302482
2869.370526790619
[10,  6000] loss: 2.302534
2881.5124039649963
[10,  6500] loss: 2.302590
2893.6570801734924
[10,  7000] loss: 2.302715
2905.9709570407867
[10,  7500] loss: 2.302546
2918.2688539028168
[10,  8000] loss: 2.302504
2930.4897134304047
[10,  8500] loss: 2.302541
2942.7973008155823
[10,  9000] loss: 2.302630
2954.9483411312103
[10,  9500] loss: 2.302588
2967.0957946777344
[10, 10000] loss: 2.302613
2979.29914355278
[10, 10500] loss: 2.302580
2991.5064792633057
[10, 11000] loss: 2.302637
3004.0196363925934
[10, 11500] loss: 2.302585
3016.1554493904114
[10, 12000] loss: 2.302472
3028.344868183136
[10, 12500] loss: 2.302625
3040.552820920944
Epoch [10] loss: 7209946.947007
[11,   500] loss: 2.302552
3053.2210121154785
[11,  1000] loss: 2.302467
3065.439696073532
[11,  1500] loss: 2.302778
3077.6630957126617
[11,  2000] loss: 2.302591
3089.8352043628693
[11,  2500] loss: 2.302537
3102.008507490158
[11,  3000] loss: 2.302613
3114.3011708259583
[11,  3500] loss: 2.302636
3126.4732217788696
[11,  4000] loss: 2.302521
3138.737238407135
[11,  4500] loss: 2.302674
3150.92636013031
[11,  5000] loss: 2.302563
3163.493263721466
[11,  5500] loss: 2.302554
3175.7010176181793
[11,  6000] loss: 2.302597
3187.8663215637207
[11,  6500] loss: 2.302625
3199.946853160858
[11,  7000] loss: 2.302623
3212.101074695587
[11,  7500] loss: 2.302706
3224.3819901943207
[11,  8000] loss: 2.302593
3236.6266429424286
[11,  8500] loss: 2.302557
3248.7479813098907
[11,  9000] loss: 2.302608
3260.8602249622345
[11,  9500] loss: 2.302628
3273.0441076755524
[11, 10000] loss: 2.302632
3285.2419579029083
[11, 10500] loss: 2.302656
3297.374990940094
[11, 11000] loss: 2.302519
3309.544303894043
[11, 11500] loss: 2.302555
3321.6672201156616
[11, 12000] loss: 2.302625
3333.9533536434174
[11, 12500] loss: 2.302630
3346.1774966716766
Epoch [11] loss: 7210003.661025
[12,   500] loss: 2.302674
3358.4872756004333
[12,  1000] loss: 2.302635
3370.725055217743
[12,  1500] loss: 2.302609
3382.922496318817
[12,  2000] loss: 2.302617
3395.1760313510895
[12,  2500] loss: 2.302527
3407.4249470233917
[12,  3000] loss: 2.302676
3419.654882669449
[12,  3500] loss: 2.302577
3432.0688264369965
[12,  4000] loss: 2.302587
3444.225769996643
[12,  4500] loss: 2.302648
3456.385473012924
[12,  5000] loss: 2.302579
3468.561799287796
[12,  5500] loss: 2.302576
3480.8334786891937
[12,  6000] loss: 2.302653
3493.1701612472534
[12,  6500] loss: 2.302592
3505.3350088596344
[12,  7000] loss: 2.302695
3517.4934153556824
[12,  7500] loss: 2.302603
3529.6604421138763
[12,  8000] loss: 2.302553
3541.8430585861206
[12,  8500] loss: 2.302602
3554.066635608673
[12,  9000] loss: 2.302608
3566.288683652878
[12,  9500] loss: 2.302684
3578.509750366211
[12, 10000] loss: 2.302578
3590.61642742157
[12, 10500] loss: 2.302484
3602.7589247226715
[12, 11000] loss: 2.302517
3614.859190940857
[12, 11500] loss: 2.302543
3627.0088613033295
[12, 12000] loss: 2.302653
3639.240925550461
[12, 12500] loss: 2.302628
3651.4802463054657
Epoch [12] loss: 7210053.567040
[13,   500] loss: 2.302582
3663.7653460502625
[13,  1000] loss: 2.302452
3675.935935020447
[13,  1500] loss: 2.302670
3688.2394864559174
[13,  2000] loss: 2.302601
3700.5034403800964
[13,  2500] loss: 2.302656
3712.745861053467
[13,  3000] loss: 2.302613
3725.0334239006042
[13,  3500] loss: 2.302613
3737.2593343257904
[13,  4000] loss: 2.302634
3749.4707837104797
[13,  4500] loss: 2.302596
3761.646961450577
[13,  5000] loss: 2.302488
3773.807143688202
[13,  5500] loss: 2.302631
3786.0665452480316
[13,  6000] loss: 2.302716
3798.3482921123505
[13,  6500] loss: 2.302679
3810.662118434906
[13,  7000] loss: 2.302624
3822.994861841202
[13,  7500] loss: 2.302508
3835.1088366508484
[13,  8000] loss: 2.302533
3847.397787332535
[13,  8500] loss: 2.302644
3859.678389072418
[13,  9000] loss: 2.302566
3872.0575654506683
[13,  9500] loss: 2.302684
3884.459631919861
[13, 10000] loss: 2.302698
3896.8857321739197
[13, 10500] loss: 2.302580
3913.336575269699
[13, 11000] loss: 2.302635
3925.6716289520264
[13, 11500] loss: 2.302529
3938.024255990982
[13, 12000] loss: 2.302564
3950.4315321445465
[13, 12500] loss: 2.302631
3962.854513168335
Epoch [13] loss: 7210011.887868
[14,   500] loss: 2.302672
3977.514068841934
[14,  1000] loss: 2.302583
3990.0242779254913
[14,  1500] loss: 2.302565
4002.5637414455414
[14,  2000] loss: 2.302557
4015.0363557338715
[14,  2500] loss: 2.302634
4027.4033105373383
[14,  3000] loss: 2.302587
4039.727660179138
[14,  3500] loss: 2.302626
4052.2518439292908
[14,  4000] loss: 2.302651
4064.6883416175842
[14,  4500] loss: 2.302634
4077.097068309784
[14,  5000] loss: 2.302531
4089.6712033748627
[14,  5500] loss: 2.302673
4102.187344074249
[14,  6000] loss: 2.302592
4114.506258010864
[14,  6500] loss: 2.302694
4127.001064538956
[14,  7000] loss: 2.302600
4139.468723773956
[14,  7500] loss: 2.302580
4152.039057016373
[14,  8000] loss: 2.302622
4164.497825145721
[14,  8500] loss: 2.302518
4176.8781814575195
[14,  9000] loss: 2.302596
4189.233959913254
[14,  9500] loss: 2.302675
4201.608701944351
[14, 10000] loss: 2.302550
4213.927200317383
[14, 10500] loss: 2.302667
4226.286993503571
[14, 11000] loss: 2.302622
4238.728729248047
[14, 11500] loss: 2.302549
4251.202445745468
[14, 12000] loss: 2.302610
4263.474154472351
[14, 12500] loss: 2.302586
4275.8674240112305
Epoch [14] loss: 7210047.742859
[15,   500] loss: 2.302685
4288.275371789932
[15,  1000] loss: 2.302662
4300.611168146133
[15,  1500] loss: 2.302639
4312.962327480316
[15,  2000] loss: 2.302648
4325.323165178299
[15,  2500] loss: 2.302565
4337.809413194656
[15,  3000] loss: 2.302669
4350.23650097847
[15,  3500] loss: 2.302651
4362.6234385967255
[15,  4000] loss: 2.302609
4374.954164505005
[15,  4500] loss: 2.302698
4387.3809316158295
[15,  5000] loss: 2.302603
4399.798659563065
[15,  5500] loss: 2.302657
4412.26500415802
[15,  6000] loss: 2.302553
4424.551095962524
[15,  6500] loss: 2.302604
4437.014942646027
[15,  7000] loss: 2.302662
4449.505634307861
[15,  7500] loss: 2.302609
4461.838642835617
[15,  8000] loss: 2.302645
4474.232580900192
[15,  8500] loss: 2.302498
4486.780353307724
[15,  9000] loss: 2.302677
4499.091384887695
[15,  9500] loss: 2.302656
4511.483746290207
[15, 10000] loss: 2.302643
4523.888110160828
[15, 10500] loss: 2.302663
4536.278271913528
[15, 11000] loss: 2.302586
4548.690429925919
[15, 11500] loss: 2.302691
4561.120947122574
[15, 12000] loss: 2.302636
4573.470880746841
[15, 12500] loss: 2.302572
4585.717985630035
Epoch [15] loss: 7210120.732570
[16,   500] loss: 2.302574
4598.352102518082
[16,  1000] loss: 2.302663
4610.760945796967
[16,  1500] loss: 2.302618
4623.09695315361
[16,  2000] loss: 2.302517
4635.485299825668
[16,  2500] loss: 2.302710
4647.860798835754
[16,  3000] loss: 2.302638
4660.151316642761
[16,  3500] loss: 2.302684
4672.595524787903
[16,  4000] loss: 2.302519
4684.984854459763
[16,  4500] loss: 2.302646
4697.298774957657
[16,  5000] loss: 2.302612
4709.521842241287
[16,  5500] loss: 2.302669
4721.846555233002
[16,  6000] loss: 2.302655
4734.185770273209
[16,  6500] loss: 2.302582
4746.539734840393
[16,  7000] loss: 2.302587
4758.899431228638
[16,  7500] loss: 2.302748
4771.1479613780975
[16,  8000] loss: 2.302630
4783.525372505188
[16,  8500] loss: 2.302579
4796.028299808502
[16,  9000] loss: 2.302679
4808.496222019196
[16,  9500] loss: 2.302564
4821.000386714935
[16, 10000] loss: 2.302648
4833.278031587601
[16, 10500] loss: 2.302555
4845.660393953323
[16, 11000] loss: 2.302637
4858.036063909531
[16, 11500] loss: 2.302586
4870.465316295624
[16, 12000] loss: 2.302671
4882.836283683777
[16, 12500] loss: 2.302522
4895.065452098846
Epoch [16] loss: 7210071.016724
[17,   500] loss: 2.302610
4907.595781803131
[17,  1000] loss: 2.302621
4919.947638034821
[17,  1500] loss: 2.302583
4932.324731588364
[17,  2000] loss: 2.302527
4944.872327566147
[17,  2500] loss: 2.302566
4957.252160787582
[17,  3000] loss: 2.302686
4969.647709608078
[17,  3500] loss: 2.302601
4982.094140529633
[17,  4000] loss: 2.302612
4994.497188091278
[17,  4500] loss: 2.302578
5006.876446723938
[17,  5000] loss: 2.302707
5019.372001647949
[17,  5500] loss: 2.302645
5031.898453474045
[17,  6000] loss: 2.302643
5044.312026023865
[17,  6500] loss: 2.302670
5056.756886005402
[17,  7000] loss: 2.302618
5069.064077615738
[17,  7500] loss: 2.302652
5081.459547042847
[17,  8000] loss: 2.302577
5093.810969829559
[17,  8500] loss: 2.302707
5106.112726926804
[17,  9000] loss: 2.302594
5118.292583703995
[17,  9500] loss: 2.302523
5130.820529222488
[17, 10000] loss: 2.302615
5143.11728310585
[17, 10500] loss: 2.302513
5155.576894760132
[17, 11000] loss: 2.302702
5168.054508924484
[17, 11500] loss: 2.302600
5180.463186979294
[17, 12000] loss: 2.302578
5192.709223031998
[17, 12500] loss: 2.302580
5205.096347332001
Epoch [17] loss: 7210011.138700
[18,   500] loss: 2.302719
5217.608545064926
[18,  1000] loss: 2.302668
5230.091706752777
[18,  1500] loss: 2.302566
5242.423372268677
[18,  2000] loss: 2.302421
5254.784567832947
[18,  2500] loss: 2.302693
5267.348166704178
[18,  3000] loss: 2.302599
5279.578721523285
[18,  3500] loss: 2.302586
5292.199051856995
[18,  4000] loss: 2.302628
5304.459963083267
[18,  4500] loss: 2.302557
5316.740595817566
[18,  5000] loss: 2.302626
5329.0770175457
[18,  5500] loss: 2.302673
5341.534326314926
[18,  6000] loss: 2.302568
5353.843759536743
[18,  6500] loss: 2.302619
5366.27170753479
[18,  7000] loss: 2.302667
5378.52361035347
[18,  7500] loss: 2.302571
5390.7818014621735
[18,  8000] loss: 2.302594
5403.17329955101
[18,  8500] loss: 2.302590
5415.640464067459
[18,  9000] loss: 2.302559
5428.1135630607605
[18,  9500] loss: 2.302643
5440.5101981163025
[18, 10000] loss: 2.302726
5452.964206695557
[18, 10500] loss: 2.302623
5465.343069553375
[18, 11000] loss: 2.302607
5477.740533828735
[18, 11500] loss: 2.302483
5490.037019252777
[18, 12000] loss: 2.302692
5502.604530334473
[18, 12500] loss: 2.302645
5515.103052377701
Epoch [18] loss: 7210079.278007
[19,   500] loss: 2.302557
5527.659681558609
[19,  1000] loss: 2.302630
5539.9751052856445
[19,  1500] loss: 2.302503
5552.435008049011
[19,  2000] loss: 2.302687
5564.730496644974
[19,  2500] loss: 2.302587
5577.101118803024
[19,  3000] loss: 2.302612
5589.378009319305
[19,  3500] loss: 2.302572
5601.658898830414
[19,  4000] loss: 2.302607
5614.094270706177
[19,  4500] loss: 2.302639
5626.5371260643005
[19,  5000] loss: 2.302610
5638.927013874054
[19,  5500] loss: 2.302597
5651.354250669479
[19,  6000] loss: 2.302529
5664.00933432579
[19,  6500] loss: 2.302671
5676.292586326599
[19,  7000] loss: 2.302638
5688.595719575882
[19,  7500] loss: 2.302699
5700.780628204346
[19,  8000] loss: 2.302580
5713.0849068164825
[19,  8500] loss: 2.302669
5725.298226118088
[19,  9000] loss: 2.302573
5737.731901168823
[19,  9500] loss: 2.302670
5750.197234630585
[19, 10000] loss: 2.302601
5762.559617519379
[19, 10500] loss: 2.302646
5775.131039381027
[19, 11000] loss: 2.302551
5787.345625162125
[19, 11500] loss: 2.302576
5799.567867040634
[19, 12000] loss: 2.302613
5811.870037794113
[19, 12500] loss: 2.302414
5824.400024652481
Epoch [19] loss: 7210031.532509
[20,   500] loss: 2.302589
5836.99881029129
[20,  1000] loss: 2.302624
5849.38126373291
[20,  1500] loss: 2.302597
5862.022832632065
[20,  2000] loss: 2.302610
5874.545836687088
[20,  2500] loss: 2.302477
5887.003871679306
[20,  3000] loss: 2.302581
5899.316233634949
[20,  3500] loss: 2.302524
5911.724134206772
[20,  4000] loss: 2.302451
5924.009760379791
[20,  4500] loss: 2.302580
5936.544333219528
[20,  5000] loss: 2.302679
5948.91036772728
[20,  5500] loss: 2.302574
5961.2068037986755
[20,  6000] loss: 2.302522
5973.545145511627
[20,  6500] loss: 2.302596
5985.796852350235
[20,  7000] loss: 2.302637
5998.22492814064
[20,  7500] loss: 2.302612
6010.536005496979
[20,  8000] loss: 2.302677
6022.889738559723
[20,  8500] loss: 2.302600
6035.254621505737
[20,  9000] loss: 2.302511
6047.896500587463
[20,  9500] loss: 2.302658
6060.381862878799
[20, 10000] loss: 2.302598
6072.831847190857
[20, 10500] loss: 2.302630
6085.1401245594025
[20, 11000] loss: 2.302418
6097.613305568695
[20, 11500] loss: 2.302644
6109.868416309357
[20, 12000] loss: 2.302635
6122.208784580231
[20, 12500] loss: 2.302585
6134.465672016144
Epoch [20] loss: 7209964.534970
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-26 04:34:31.262882
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[ 0.0002, -0.0010,  0.0020, -0.0049,  0.0032, -0.0034,  0.0002,  0.0086,
         -0.0037, -0.0011],
        [ 0.0002, -0.0010,  0.0020, -0.0049,  0.0032, -0.0034,  0.0002,  0.0086,
         -0.0037, -0.0011],
        [ 0.0002, -0.0010,  0.0020, -0.0049,  0.0032, -0.0034,  0.0002,  0.0086,
         -0.0037, -0.0011],
        [ 0.0002, -0.0010,  0.0020, -0.0049,  0.0032, -0.0034,  0.0002,  0.0086,
         -0.0037, -0.0011]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:  horse horse horse horse
Accuracy of the network on the 4000.0 test images: 10 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': True, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.05, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.05, learning_rate_scheduler=True, load_from_memory=False, model_archi=16, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 frog plane truck horse
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Number of parameters: 23528522 
Sending model to GPU
Learning Rate: 0.05, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582688131.6994607
[1,   500] loss: 2.567387
10.81620740890503
[1,  1000] loss: 2.554981
21.507227182388306
[1,  1500] loss: 2.561655
32.201284646987915
[1,  2000] loss: 2.537319
43.00420808792114
[1,  2500] loss: 2.540503
53.80864214897156
[1,  3000] loss: 2.564440
64.51491713523865
[1,  3500] loss: 2.560000
75.0549213886261
[1,  4000] loss: 2.577814
85.82310581207275
[1,  4500] loss: 2.568840
96.50558876991272
[1,  5000] loss: 2.521563
107.25741052627563
[1,  5500] loss: 2.565831
118.02914524078369
[1,  6000] loss: 2.556011
128.82102727890015
[1,  6500] loss: 2.547125
139.4471185207367
[1,  7000] loss: 2.566821
150.5416922569275
[1,  7500] loss: 2.564281
161.1797285079956
[1,  8000] loss: 2.581928
172.00429368019104
[1,  8500] loss: 2.560910
182.6883749961853
[1,  9000] loss: 2.563841
193.3733730316162
[1,  9500] loss: 2.551396
204.08560299873352
[1, 10000] loss: 2.576810
214.77628231048584
[1, 10500] loss: 2.553020
225.46626257896423
[1, 11000] loss: 2.571506
236.301176071167
[1, 11500] loss: 2.565460
246.98994064331055
[1, 12000] loss: 2.595391
257.75455260276794
[1, 12500] loss: 2.567938
268.3465654850006
Epoch [1] loss: 8024902.687047
/home/s4091221/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[2,   500] loss: 2.560884
279.18177127838135
[2,  1000] loss: 2.568672
290.0774552822113
[2,  1500] loss: 2.563423
300.72034788131714
[2,  2000] loss: 2.579948
311.3780360221863
[2,  2500] loss: 2.556691
322.090918302536
[2,  3000] loss: 2.568863
332.7423210144043
[2,  3500] loss: 2.532388
343.4381718635559
[2,  4000] loss: 2.561394
354.2612409591675
[2,  4500] loss: 2.536179
365.0337312221527
[2,  5000] loss: 2.545220
375.72360491752625
[2,  5500] loss: 2.566571
386.6074330806732
[2,  6000] loss: 2.541672
397.40748834609985
[2,  6500] loss: 2.540214
408.07568764686584
[2,  7000] loss: 2.560393
419.068076133728
[2,  7500] loss: 2.559285
429.7837986946106
[2,  8000] loss: 2.545923
440.43654704093933
[2,  8500] loss: 2.552446
451.1692063808441
[2,  9000] loss: 2.543254
461.95710921287537
[2,  9500] loss: 2.534927
472.7117877006531
[2, 10000] loss: 2.572327
483.4846887588501
[2, 10500] loss: 2.544368
494.1029028892517
[2, 11000] loss: 2.564508
504.73995542526245
[2, 11500] loss: 2.545826
515.4812936782837
[2, 12000] loss: 2.536904
526.2227654457092
[2, 12500] loss: 2.580535
536.9717655181885
Epoch [2] loss: 7999981.433724
[3,   500] loss: 2.570430
547.7263667583466
[3,  1000] loss: 2.554739
558.3225452899933
[3,  1500] loss: 2.551786
569.2038309574127
[3,  2000] loss: 2.556597
580.0873548984528
[3,  2500] loss: 2.541217
590.9065115451813
[3,  3000] loss: 2.557726
601.8271825313568
[3,  3500] loss: 2.545330
612.5226407051086
[3,  4000] loss: 2.575127
623.2985258102417
[3,  4500] loss: 2.564372
634.0809240341187
[3,  5000] loss: 2.561555
644.7397611141205
[3,  5500] loss: 2.556979
655.5809752941132
[3,  6000] loss: 2.538104
666.3819813728333
[3,  6500] loss: 2.561407
677.0989999771118
[3,  7000] loss: 2.553850
687.8944571018219
[3,  7500] loss: 2.561091
698.6222929954529
[3,  8000] loss: 2.567086
709.4910044670105
[3,  8500] loss: 2.589232
720.3851797580719
[3,  9000] loss: 2.580173
731.0342962741852
[3,  9500] loss: 2.565507
741.6459667682648
[3, 10000] loss: 2.583013
752.4301354885101
[3, 10500] loss: 2.560627
763.1200716495514
[3, 11000] loss: 2.540997
773.8328387737274
[3, 11500] loss: 2.574636
784.5506792068481
[3, 12000] loss: 2.567770
795.1338229179382
[3, 12500] loss: 2.555521
806.055361032486
Epoch [3] loss: 8015635.582230
[4,   500] loss: 2.556119
816.9628846645355
[4,  1000] loss: 2.553730
827.6294982433319
[4,  1500] loss: 2.537692
838.3395652770996
[4,  2000] loss: 2.567858
849.0743305683136
[4,  2500] loss: 2.562446
859.7315812110901
[4,  3000] loss: 2.531584
870.3517677783966
[4,  3500] loss: 2.586611
881.0099241733551
[4,  4000] loss: 2.550753
891.7600638866425
[4,  4500] loss: 2.580226
902.4567902088165
[4,  5000] loss: 2.535616
913.3083548545837
[4,  5500] loss: 2.585428
924.1722230911255
[4,  6000] loss: 2.561300
934.9701735973358
[4,  6500] loss: 2.543655
945.7166481018066
[4,  7000] loss: 2.565438
956.4701917171478
[4,  7500] loss: 2.560457
967.2136943340302
[4,  8000] loss: 2.550247
977.9154706001282
[4,  8500] loss: 2.565606
988.6880438327789
[4,  9000] loss: 2.560420
999.3848359584808
[4,  9500] loss: 2.548762
1010.0439217090607
[4, 10000] loss: 2.585591
1021.1125826835632
[4, 10500] loss: 2.568274
1032.1071770191193
[4, 11000] loss: 2.571956
1043.0181875228882
[4, 11500] loss: 2.563144
1053.8119840621948
[4, 12000] loss: 2.573208
1064.5981612205505
[4, 12500] loss: 2.558427
1075.4035234451294
Epoch [4] loss: 8013824.356199
[5,   500] loss: 2.553097
1086.2915585041046
[5,  1000] loss: 2.571998
1097.0540606975555
[5,  1500] loss: 2.553260
1107.8622171878815
[5,  2000] loss: 2.574889
1118.6291241645813
[5,  2500] loss: 2.563544
1129.3106005191803
[5,  3000] loss: 2.535910
1140.1311795711517
[5,  3500] loss: 2.576645
1150.8665754795074
[5,  4000] loss: 2.544810
1161.6546816825867
[5,  4500] loss: 2.579768
1172.271560907364
[5,  5000] loss: 2.569405
1183.084950208664
[5,  5500] loss: 2.562835
1193.9012763500214
[5,  6000] loss: 2.555111
1204.8053913116455
[5,  6500] loss: 2.550790
1215.6252567768097
[5,  7000] loss: 2.559121
1226.432594537735
[5,  7500] loss: 2.562175
1237.2016968727112
[5,  8000] loss: 2.585140
1247.908760547638
[5,  8500] loss: 2.537433
1258.7243750095367
[5,  9000] loss: 2.573740
1269.5032811164856
[5,  9500] loss: 2.553464
1280.278448343277
[5, 10000] loss: 2.552962
1291.0253462791443
[5, 10500] loss: 2.547523
1301.7247412204742
[5, 11000] loss: 2.605723
1312.5574913024902
[5, 11500] loss: 2.534983
1323.321932554245
[5, 12000] loss: 2.554507
1334.1422400474548
[5, 12500] loss: 2.559942
1344.905577659607
Epoch [5] loss: 8021413.400530
[6,   500] loss: 2.556579
1355.7401595115662
[6,  1000] loss: 2.551605
1366.5802884101868
[6,  1500] loss: 2.535700
1377.4306600093842
[6,  2000] loss: 2.565651
1388.1304650306702
[6,  2500] loss: 2.531408
1399.138973236084
[6,  3000] loss: 2.553155
1409.9256746768951
[6,  3500] loss: 2.545020
1420.808274269104
[6,  4000] loss: 2.577022
1431.6078333854675
[6,  4500] loss: 2.553097
1442.2815942764282
[6,  5000] loss: 2.560188
1453.1709394454956
[6,  5500] loss: 2.540395
1463.9383990764618
[6,  6000] loss: 2.552513
1474.8560943603516
[6,  6500] loss: 2.575163
1485.647288799286
[6,  7000] loss: 2.569495
1496.3763992786407
[6,  7500] loss: 2.554223
1507.1414585113525
[6,  8000] loss: 2.570834
1517.842900276184
[6,  8500] loss: 2.527870
1528.6656157970428
[6,  9000] loss: 2.538920
1539.3669412136078
[6,  9500] loss: 2.568348
1550.3089680671692
[6, 10000] loss: 2.585048
1561.01757645607
[6, 10500] loss: 2.559898
1571.7346534729004
[6, 11000] loss: 2.575114
1582.477823972702
[6, 11500] loss: 2.566846
1593.201034784317
[6, 12000] loss: 2.568370
1604.1873588562012
[6, 12500] loss: 2.579176
1615.208889722824
Epoch [6] loss: 8016994.236944
[7,   500] loss: 2.576252
1626.1048758029938
[7,  1000] loss: 2.563180
1636.935792684555
[7,  1500] loss: 2.547761
1647.6954715251923
[7,  2000] loss: 2.547071
1658.469337940216
[7,  2500] loss: 2.544185
1669.2410945892334
[7,  3000] loss: 2.540640
1679.936402797699
[7,  3500] loss: 2.549627
1690.6168134212494
[7,  4000] loss: 2.574639
1701.3381943702698
[7,  4500] loss: 2.555856
1712.0451345443726
[7,  5000] loss: 2.585404
1722.6490116119385
[7,  5500] loss: 2.544956
1733.754273891449
[7,  6000] loss: 2.556945
1744.5062401294708
[7,  6500] loss: 2.555764
1755.099074602127
[7,  7000] loss: 2.578476
1765.7783071994781
[7,  7500] loss: 2.538701
1776.4826951026917
[7,  8000] loss: 2.546843
1787.2893178462982
[7,  8500] loss: 2.560939
1798.0985352993011
[7,  9000] loss: 2.565876
1808.7359743118286
[7,  9500] loss: 2.562822
1819.4837222099304
[7, 10000] loss: 2.551794
1830.190361738205
[7, 10500] loss: 2.549955
1840.8493416309357
[7, 11000] loss: 2.587670
1851.584350824356
[7, 11500] loss: 2.536376
1862.1561870574951
[7, 12000] loss: 2.541127
1872.8202006816864
[7, 12500] loss: 2.534466
1883.5104069709778
Epoch [7] loss: 7994269.468205
[8,   500] loss: 2.566342
1894.3833663463593
[8,  1000] loss: 2.567542
1904.9450271129608
[8,  1500] loss: 2.565306
1915.5787999629974
[8,  2000] loss: 2.570960
1926.031198501587
[8,  2500] loss: 2.562019
1936.6294295787811
[8,  3000] loss: 2.554734
1947.1151835918427
[8,  3500] loss: 2.571884
1957.6814014911652
[8,  4000] loss: 2.549392
1968.326553106308
[8,  4500] loss: 2.539887
1979.061378955841
[8,  5000] loss: 2.560644
1989.7642238140106
[8,  5500] loss: 2.550104
2000.4851479530334
[8,  6000] loss: 2.568510
2011.1253702640533
[8,  6500] loss: 2.586811
2021.680958032608
[8,  7000] loss: 2.579523
2032.2571995258331
[8,  7500] loss: 2.588501
2042.7863879203796
[8,  8000] loss: 2.567540
2053.3201563358307
[8,  8500] loss: 2.559844
2063.810975790024
[8,  9000] loss: 2.580390
2074.464796066284
[8,  9500] loss: 2.572068
2085.0707445144653
[8, 10000] loss: 2.547868
2095.699444293976
[8, 10500] loss: 2.537562
2106.5141649246216
[8, 11000] loss: 2.570683
2117.1743199825287
[8, 11500] loss: 2.563313
2127.751613140106
[8, 12000] loss: 2.554950
2138.3349940776825
[8, 12500] loss: 2.564474
2148.9828119277954
Epoch [8] loss: 8031195.609507
[9,   500] loss: 2.549193
2159.8356354236603
[9,  1000] loss: 2.528533
2170.5807616710663
[9,  1500] loss: 2.565136
2181.265728712082
[9,  2000] loss: 2.623197
2191.7915422916412
[9,  2500] loss: 2.574036
2202.3106474876404
[9,  3000] loss: 2.572068
2212.80331325531
[9,  3500] loss: 2.546231
2223.6031239032745
[9,  4000] loss: 2.568453
2234.2623569965363
[9,  4500] loss: 2.570504
2244.8440356254578
[9,  5000] loss: 2.562720
2255.407931804657
[9,  5500] loss: 2.564586
2266.04678440094
[9,  6000] loss: 2.572566
2276.6770811080933
[9,  6500] loss: 2.577564
2287.3843626976013
[9,  7000] loss: 2.570182
2297.9146645069122
[9,  7500] loss: 2.568243
2308.520340681076
[9,  8000] loss: 2.566595
2319.1203129291534
[9,  8500] loss: 2.563920
2329.683439731598
[9,  9000] loss: 2.550106
2340.217047691345
[9,  9500] loss: 2.566353
2350.765874147415
[9, 10000] loss: 2.540195
2361.387439250946
[9, 10500] loss: 2.556338
2372.030377149582
[9, 11000] loss: 2.569576
2382.5984575748444
[9, 11500] loss: 2.575168
2393.2073051929474
[9, 12000] loss: 2.556637
2403.774418115616
[9, 12500] loss: 2.564380
2414.4237751960754
Epoch [9] loss: 8013378.415950
[10,   500] loss: 2.557413
2425.1011950969696
[10,  1000] loss: 2.580462
2435.632649421692
[10,  1500] loss: 2.567077
2446.1494386196136
[10,  2000] loss: 2.550702
2456.786527633667
[10,  2500] loss: 2.546139
2467.329495191574
[10,  3000] loss: 2.544148
2477.7715363502502
[10,  3500] loss: 2.569669
2488.3637371063232
[10,  4000] loss: 2.552897
2498.9355585575104
[10,  4500] loss: 2.580413
2509.645622253418
[10,  5000] loss: 2.581845
2520.2277822494507
[10,  5500] loss: 2.567013
2530.9745609760284
[10,  6000] loss: 2.573245
2541.4653644561768
[10,  6500] loss: 2.541806
2551.974123239517
[10,  7000] loss: 2.530716
2562.577383995056
[10,  7500] loss: 2.569036
2573.186847925186
[10,  8000] loss: 2.583591
2583.860286951065
[10,  8500] loss: 2.556372
2594.524538755417
[10,  9000] loss: 2.548566
2605.181846857071
[10,  9500] loss: 2.564540
2615.769825220108
[10, 10000] loss: 2.599563
2626.4300196170807
[10, 10500] loss: 2.562394
2637.026502609253
[10, 11000] loss: 2.583376
2647.814602136612
[10, 11500] loss: 2.572774
2658.3739240169525
[10, 12000] loss: 2.554950
2668.9174268245697
[10, 12500] loss: 2.535440
2679.4655039310455
Epoch [10] loss: 8022981.277186
[11,   500] loss: 2.564247
2690.215833425522
[11,  1000] loss: 2.538921
2701.148320198059
[11,  1500] loss: 2.548434
2711.8085584640503
[11,  2000] loss: 2.558762
2722.418538093567
[11,  2500] loss: 2.584847
2733.1331906318665
[11,  3000] loss: 2.574939
2743.7051153182983
[11,  3500] loss: 2.573840
2754.4952816963196
[11,  4000] loss: 2.564071
2765.1759881973267
[11,  4500] loss: 2.575792
2775.78738117218
[11,  5000] loss: 2.556721
2786.58922624588
[11,  5500] loss: 2.574218
2797.4578337669373
[11,  6000] loss: 2.553120
2808.431401491165
[11,  6500] loss: 2.531574
2819.417581796646
[11,  7000] loss: 2.532941
2830.0808670520782
[11,  7500] loss: 2.575707
2840.7740335464478
[11,  8000] loss: 2.562225
2851.4907495975494
[11,  8500] loss: 2.576685
2862.241883993149
[11,  9000] loss: 2.571896
2873.0582501888275
[11,  9500] loss: 2.540137
2883.7992033958435
[11, 10000] loss: 2.583563
2894.451508283615
[11, 10500] loss: 2.579098
2905.3341948986053
[11, 11000] loss: 2.582407
2916.1976771354675
[11, 11500] loss: 2.565677
2926.8957719802856
[11, 12000] loss: 2.542021
2937.6597599983215
[11, 12500] loss: 2.575156
2948.5910029411316
Epoch [11] loss: 8027728.803216
[12,   500] loss: 2.550752
2959.516684293747
[12,  1000] loss: 2.557544
2970.26829123497
[12,  1500] loss: 2.552855
2981.0814969539642
[12,  2000] loss: 2.527313
2992.0701456069946
[12,  2500] loss: 2.535887
3002.841280937195
[12,  3000] loss: 2.538895
3013.4926285743713
[12,  3500] loss: 2.564073
3024.283955812454
[12,  4000] loss: 2.550568
3035.1381134986877
[12,  4500] loss: 2.566709
3045.9512372016907
[12,  5000] loss: 2.555753
3056.808732509613
[12,  5500] loss: 2.562431
3067.618590593338
[12,  6000] loss: 2.591027
3078.389942407608
[12,  6500] loss: 2.548831
3089.290624141693
[12,  7000] loss: 2.567250
3100.3458609580994
[12,  7500] loss: 2.562923
3111.245228767395
[12,  8000] loss: 2.560398
3122.076442718506
[12,  8500] loss: 2.551085
3132.8172664642334
[12,  9000] loss: 2.565228
3143.535276412964
[12,  9500] loss: 2.559409
3154.564425945282
[12, 10000] loss: 2.550973
3165.4251897335052
[12, 10500] loss: 2.577303
3176.4859170913696
[12, 11000] loss: 2.584210
3187.2115321159363
[12, 11500] loss: 2.547803
3197.9094846248627
[12, 12000] loss: 2.546904
3208.7515847682953
[12, 12500] loss: 2.547836
3219.4578857421875
Epoch [12] loss: 7998600.040305
[13,   500] loss: 2.554206
3230.5216817855835
[13,  1000] loss: 2.538265
3241.455008983612
[13,  1500] loss: 2.591914
3252.1411814689636
[13,  2000] loss: 2.527239
3262.9505195617676
[13,  2500] loss: 2.596773
3273.838393688202
[13,  3000] loss: 2.587671
3284.5659890174866
[13,  3500] loss: 2.538530
3295.240447282791
[13,  4000] loss: 2.565574
3305.9507763385773
[13,  4500] loss: 2.558067
3316.6144173145294
[13,  5000] loss: 2.538710
3327.34317445755
[13,  5500] loss: 2.557819
3338.1257004737854
[13,  6000] loss: 2.543214
3348.8810892105103
[13,  6500] loss: 2.578273
3359.6941363811493
[13,  7000] loss: 2.546489
3370.5061540603638
[13,  7500] loss: 2.565675
3381.245994091034
[13,  8000] loss: 2.551465
3391.995662212372
[13,  8500] loss: 2.584176
3402.580560684204
[13,  9000] loss: 2.571304
3413.2325162887573
[13,  9500] loss: 2.520613
3424.0175619125366
[13, 10000] loss: 2.566575
3434.7282819747925
[13, 10500] loss: 2.580948
3445.483480453491
[13, 11000] loss: 2.568624
3456.2022743225098
[13, 11500] loss: 2.541986
3466.9762568473816
[13, 12000] loss: 2.567238
3477.8727629184723
[13, 12500] loss: 2.551775
3488.5737521648407
Epoch [13] loss: 8013900.647239
[14,   500] loss: 2.567675
3499.5279502868652
[14,  1000] loss: 2.558233
3510.32257938385
[14,  1500] loss: 2.559147
3521.065142393112
[14,  2000] loss: 2.575226
3531.841742515564
[14,  2500] loss: 2.580929
3542.636615037918
[14,  3000] loss: 2.562067
3553.4908134937286
[14,  3500] loss: 2.547446
3564.2604076862335
[14,  4000] loss: 2.556939
3575.1096782684326
[14,  4500] loss: 2.559443
3585.893344640732
[14,  5000] loss: 2.555695
3596.646961450577
[14,  5500] loss: 2.590832
3607.2686998844147
[14,  6000] loss: 2.554954
3618.02024102211
[14,  6500] loss: 2.567350
3628.8275768756866
[14,  7000] loss: 2.573425
3639.610062122345
[14,  7500] loss: 2.545974
3650.445211648941
[14,  8000] loss: 2.548616
3661.2538895606995
[14,  8500] loss: 2.556150
3672.016943216324
[14,  9000] loss: 2.566533
3682.6876583099365
[14,  9500] loss: 2.571998
3693.5516040325165
[14, 10000] loss: 2.576557
3704.198537826538
[14, 10500] loss: 2.557947
3715.1643528938293
[14, 11000] loss: 2.570465
3725.8615820407867
[14, 11500] loss: 2.551261
3736.4578609466553
[14, 12000] loss: 2.575424
3747.151032447815
[14, 12500] loss: 2.571589
3757.892907857895
Epoch [14] loss: 8029864.310573
[15,   500] loss: 2.589664
3768.8409235477448
[15,  1000] loss: 2.560786
3779.54616189003
[15,  1500] loss: 2.577087
3790.189895629883
[15,  2000] loss: 2.551454
3800.901109933853
[15,  2500] loss: 2.544782
3811.664319038391
[15,  3000] loss: 2.585051
3822.2954664230347
[15,  3500] loss: 2.552150
3833.0916788578033
[15,  4000] loss: 2.555937
3843.988345146179
[15,  4500] loss: 2.548582
3854.748262166977
[15,  5000] loss: 2.550454
3865.464184522629
[15,  5500] loss: 2.562947
3876.1942365169525
[15,  6000] loss: 2.566618
3888.4508259296417
[15,  6500] loss: 2.526053
3899.3604016304016
[15,  7000] loss: 2.545016
3919.4861030578613
[15,  7500] loss: 2.558471
3930.194953918457
[15,  8000] loss: 2.560143
3940.8531863689423
[15,  8500] loss: 2.559702
3951.732471227646
[15,  9000] loss: 2.563325
3962.3833544254303
[15,  9500] loss: 2.560655
3973.1843152046204
[15, 10000] loss: 2.544359
3984.2326555252075
[15, 10500] loss: 2.539185
3995.010185480118
[15, 11000] loss: 2.552295
4005.755502462387
[15, 11500] loss: 2.542066
4016.5311720371246
[15, 12000] loss: 2.545226
4027.2523379325867
[15, 12500] loss: 2.559966
4038.2011210918427
Epoch [15] loss: 8006243.067847
[16,   500] loss: 2.552198
4050.3216948509216
[16,  1000] loss: 2.595148
4061.032444000244
[16,  1500] loss: 2.554925
4071.782711982727
[16,  2000] loss: 2.543459
4082.6353385448456
[16,  2500] loss: 2.596789
4093.350175142288
[16,  3000] loss: 2.539989
4104.209161996841
[16,  3500] loss: 2.557838
4115.05694937706
[16,  4000] loss: 2.556146
4125.771465778351
[16,  4500] loss: 2.572428
4136.740540027618
[16,  5000] loss: 2.580200
4147.484502077103
[16,  5500] loss: 2.535356
4158.203550338745
[16,  6000] loss: 2.570996
4168.95631480217
[16,  6500] loss: 2.581174
4179.678030490875
[16,  7000] loss: 2.571725
4190.5168080329895
[16,  7500] loss: 2.563042
4201.352800130844
[16,  8000] loss: 2.565480
4212.325679063797
[16,  8500] loss: 2.560865
4223.119514226913
[16,  9000] loss: 2.541577
4233.791789770126
[16,  9500] loss: 2.548954
4244.476326942444
[16, 10000] loss: 2.562785
4255.359632730484
[16, 10500] loss: 2.555180
4266.0987293720245
[16, 11000] loss: 2.547913
4276.941703557968
[16, 11500] loss: 2.557624
4287.631674528122
[16, 12000] loss: 2.599483
4298.553241491318
[16, 12500] loss: 2.581266
4309.263699054718
Epoch [16] loss: 8016080.983439
[17,   500] loss: 2.571007
4320.293078660965
[17,  1000] loss: 2.543574
4331.04612326622
[17,  1500] loss: 2.547845
4341.76949262619
[17,  2000] loss: 2.562061
4352.5419771671295
[17,  2500] loss: 2.587277
4363.73103761673
[17,  3000] loss: 2.571563
4374.60372877121
[17,  3500] loss: 2.555205
4385.31657409668
[17,  4000] loss: 2.556969
4395.959366083145
[17,  4500] loss: 2.588113
4406.704306125641
[17,  5000] loss: 2.544752
4417.365087509155
[17,  5500] loss: 2.575364
4428.149707078934
[17,  6000] loss: 2.566076
4438.850242614746
[17,  6500] loss: 2.561936
4449.65607881546
[17,  7000] loss: 2.559488
4460.511606216431
[17,  7500] loss: 2.576497
4471.195011377335
[17,  8000] loss: 2.570562
4482.0065331459045
[17,  8500] loss: 2.557906
4492.785654067993
[17,  9000] loss: 2.560743
4503.657551050186
[17,  9500] loss: 2.544840
4514.4212872982025
[17, 10000] loss: 2.570443
4525.222725868225
[17, 10500] loss: 2.535405
4535.9167511463165
[17, 11000] loss: 2.548455
4546.940706729889
[17, 11500] loss: 2.576356
4557.875751495361
[17, 12000] loss: 2.546414
4568.868165969849
[17, 12500] loss: 2.544638
4579.613400697708
Epoch [17] loss: 8014976.047280
[18,   500] loss: 2.561847
4590.67773270607
[18,  1000] loss: 2.564718
4601.3962059021
[18,  1500] loss: 2.566256
4612.220184326172
[18,  2000] loss: 2.564759
4623.111042737961
[18,  2500] loss: 2.543879
4633.854268074036
[18,  3000] loss: 2.561563
4644.412799119949
[18,  3500] loss: 2.528422
4655.110238075256
[18,  4000] loss: 2.565313
4666.1121118068695
[18,  4500] loss: 2.570047
4676.948109865189
[18,  5000] loss: 2.545148
4687.758383989334
[18,  5500] loss: 2.552775
4698.647109508514
[18,  6000] loss: 2.558810
4709.38597369194
[18,  6500] loss: 2.563136
4720.123490095139
[18,  7000] loss: 2.589745
4730.7953560352325
[18,  7500] loss: 2.593868
4741.432384967804
[18,  8000] loss: 2.559835
4752.149397611618
[18,  8500] loss: 2.562076
4762.889126300812
[18,  9000] loss: 2.542023
4773.647165298462
[18,  9500] loss: 2.530008
4784.391302108765
[18, 10000] loss: 2.588866
4795.344557285309
[18, 10500] loss: 2.525885
4806.03298664093
[18, 11000] loss: 2.567306
4816.788012266159
[18, 11500] loss: 2.562216
4827.472865581512
[18, 12000] loss: 2.564193
4838.145646095276
[18, 12500] loss: 2.527390
4848.830325841904
Epoch [18] loss: 7998317.805260
[19,   500] loss: 2.560237
4859.743021249771
[19,  1000] loss: 2.522112
4870.27140712738
[19,  1500] loss: 2.555814
4880.906759977341
[19,  2000] loss: 2.587511
4891.682826042175
[19,  2500] loss: 2.555047
4902.250702857971
[19,  3000] loss: 2.535189
4912.908574819565
[19,  3500] loss: 2.573574
4923.6468851566315
[19,  4000] loss: 2.567305
4934.454898357391
[19,  4500] loss: 2.533906
4945.111569881439
[19,  5000] loss: 2.545296
4955.859905958176
[19,  5500] loss: 2.559358
4966.475149393082
[19,  6000] loss: 2.571876
4977.214640140533
[19,  6500] loss: 2.544344
4987.947137594223
[19,  7000] loss: 2.561227
4998.690207958221
[19,  7500] loss: 2.580198
5009.3012590408325
[19,  8000] loss: 2.562569
5020.073784351349
[19,  8500] loss: 2.586174
5030.816400766373
[19,  9000] loss: 2.561077
5041.419488191605
[19,  9500] loss: 2.567145
5052.090743303299
[19, 10000] loss: 2.567991
5062.825652837753
[19, 10500] loss: 2.528098
5073.595265865326
[19, 11000] loss: 2.573614
5084.35609292984
[19, 11500] loss: 2.560090
5095.134149074554
[19, 12000] loss: 2.576258
5105.842316389084
[19, 12500] loss: 2.566390
5116.522332906723
Epoch [19] loss: 8013973.527258
[20,   500] loss: 2.560838
5127.353492259979
[20,  1000] loss: 2.555182
5138.205466270447
[20,  1500] loss: 2.570409
5148.82692193985
[20,  2000] loss: 2.562056
5159.648081064224
[20,  2500] loss: 2.531584
5170.519698143005
[20,  3000] loss: 2.549902
5181.199036598206
[20,  3500] loss: 2.539316
5192.01592206955
[20,  4000] loss: 2.557300
5202.843221902847
[20,  4500] loss: 2.553425
5213.589978456497
[20,  5000] loss: 2.551253
5224.288973331451
[20,  5500] loss: 2.558082
5234.978948831558
[20,  6000] loss: 2.554531
5245.72188782692
[20,  6500] loss: 2.538643
5256.5402891635895
[20,  7000] loss: 2.577029
5267.362720489502
[20,  7500] loss: 2.588450
5278.217393875122
[20,  8000] loss: 2.542814
5289.009211301804
[20,  8500] loss: 2.551298
5299.721041440964
[20,  9000] loss: 2.577798
5310.438592433929
[20,  9500] loss: 2.545027
5321.229173898697
[20, 10000] loss: 2.555806
5331.888612508774
[20, 10500] loss: 2.576712
5342.627501487732
[20, 11000] loss: 2.577232
5353.304655790329
[20, 11500] loss: 2.574801
5363.999070882797
[20, 12000] loss: 2.561361
5374.5788242816925
[20, 12500] loss: 2.587974
5385.210939884186
Epoch [20] loss: 8011983.352682
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-26 06:05:16.960290
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[ 1.1739,  0.2836, -0.1353,  0.5266, -1.1312,  0.3171, -0.5708, -0.1344,
         -0.2043, -0.7374],
        [ 0.9755, -0.8587, -1.0520, -0.1545, -0.9189,  1.0755,  0.4094, -0.2856,
         -1.5235,  0.4443],
        [ 0.3656, -0.6900, -1.2489, -1.1919, -0.6382, -0.7499, -0.5489, -0.1647,
          0.3187, -0.6702],
        [ 0.2825, -0.9690, -1.2158, -0.5504, -1.1134,  0.4585, -0.6853, -0.0548,
         -0.1204,  0.2156]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:  plane   dog plane   dog
Accuracy of the network on the 4000.0 test images: 10 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 16, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 2}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=16, momentum=0, normalise=False, optimizer_choice=2, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 frog truck horse  bird
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model resnet50 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model resnet50 Reshaped
Number of parameters: 23528522 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.adam.Adam'> Optimizer
Starting Training at 1582693591.3691392
[1,   500] loss: 3.404192
18.759875774383545
[1,  1000] loss: 3.074695
37.06432628631592
[1,  1500] loss: 2.920412
55.331568479537964
[1,  2000] loss: 2.772718
73.39659333229065
[1,  2500] loss: 2.652475
91.56203556060791
[1,  3000] loss: 2.592603
109.86720323562622
[1,  3500] loss: 2.566179
128.0118384361267
[1,  4000] loss: 2.510046
146.14129638671875
[1,  4500] loss: 2.485988
164.50546312332153
[1,  5000] loss: 2.419586
182.6248733997345
[1,  5500] loss: 2.378153
200.88752436637878
[1,  6000] loss: 2.355889
218.98935627937317
[1,  6500] loss: 2.350221
236.88403868675232
[1,  7000] loss: 2.303934
254.8265974521637
[1,  7500] loss: 2.324335
272.9015245437622
[1,  8000] loss: 2.419805
290.95301842689514
[1,  8500] loss: 2.330899
308.91644978523254
[1,  9000] loss: 2.315514
326.89005303382874
[1,  9500] loss: 2.264166
344.761736869812
[1, 10000] loss: 2.294890
362.8220250606537
[1, 10500] loss: 2.263331
381.21463561058044
[1, 11000] loss: 2.332264
399.4522228240967
[1, 11500] loss: 2.285394
417.4182097911835
[1, 12000] loss: 2.289480
435.4432542324066
[1, 12500] loss: 2.269143
453.4572961330414
Epoch [1] loss: 7818905.634403
[2,   500] loss: 2.271905
471.60789942741394
[2,  1000] loss: 2.310518
489.54664945602417
[2,  1500] loss: 2.273807
507.5537340641022
[2,  2000] loss: 2.313761
525.5004382133484
[2,  2500] loss: 2.260200
543.4179329872131
[2,  3000] loss: 2.249185
561.4303221702576
[2,  3500] loss: 2.237909
579.5170691013336
[2,  4000] loss: 2.246803
597.5282487869263
[2,  4500] loss: 2.238441
615.6371576786041
[2,  5000] loss: 2.341857
633.8598461151123
[2,  5500] loss: 2.416020
651.8838093280792
[2,  6000] loss: 2.301731
669.8345499038696
[2,  6500] loss: 2.256718
687.8748345375061
[2,  7000] loss: 2.278406
705.8175051212311
[2,  7500] loss: 2.328858
723.8122673034668
[2,  8000] loss: 2.283638
741.812301158905
[2,  8500] loss: 2.285020
759.7034347057343
[2,  9000] loss: 2.270157
777.6169745922089
[2,  9500] loss: 2.252718
795.5694246292114
[2, 10000] loss: 2.252552
813.883437871933
[2, 10500] loss: 2.237768
831.9546103477478
[2, 11000] loss: 2.264174
849.9940717220306
[2, 11500] loss: 2.245153
867.9333505630493
[2, 12000] loss: 2.218443
885.8571479320526
[2, 12500] loss: 2.223812
903.9402046203613
Epoch [2] loss: 7129607.939885
[3,   500] loss: 2.229665
922.3128774166107
[3,  1000] loss: 2.216971
940.374630689621
[3,  1500] loss: 2.195405
958.4228837490082
[3,  2000] loss: 2.223374
976.2565631866455
[3,  2500] loss: 2.233118
994.2284369468689
[3,  3000] loss: 2.192595
1012.2316148281097
[3,  3500] loss: 2.261289
1030.2998361587524
[3,  4000] loss: 2.311826
1048.2147181034088
[3,  4500] loss: 2.232561
1066.093531370163
[3,  5000] loss: 2.198402
1083.987554550171
[3,  5500] loss: 2.202732
1101.8896961212158
[3,  6000] loss: 2.227158
1119.8259801864624
[3,  6500] loss: 2.200814
1137.8520469665527
[3,  7000] loss: 2.175688
1155.8989305496216
[3,  7500] loss: 2.178752
1174.0498011112213
[3,  8000] loss: 2.165859
1192.0897812843323
[3,  8500] loss: 2.173898
1210.160100698471
[3,  9000] loss: 2.170083
1228.5841934680939
[3,  9500] loss: 2.160401
1246.7396864891052
[3, 10000] loss: 2.128656
1264.7879176139832
[3, 10500] loss: 2.127416
1282.9747006893158
[3, 11000] loss: 2.255685
1301.0512549877167
[3, 11500] loss: 2.244556
1319.067251443863
[3, 12000] loss: 2.192130
1336.989265680313
[3, 12500] loss: 2.186132
1354.9557905197144
Epoch [3] loss: 6905568.840299
[4,   500] loss: 2.230836
1373.345092535019
[4,  1000] loss: 2.216141
1391.5312662124634
[4,  1500] loss: 2.195320
1409.612829208374
[4,  2000] loss: 2.193045
1427.5409669876099
[4,  2500] loss: 2.156464
1445.5064418315887
[4,  3000] loss: 2.174107
1463.382893562317
[4,  3500] loss: 2.189912
1481.2627594470978
[4,  4000] loss: 2.164803
1499.1609117984772
[4,  4500] loss: 2.180345
1517.133288383484
[4,  5000] loss: 2.163451
1535.210964679718
[4,  5500] loss: 2.155122
1553.3498759269714
[4,  6000] loss: 2.236684
1571.347067117691
[4,  6500] loss: 2.204517
1589.3549492359161
[4,  7000] loss: 2.191651
1607.4479112625122
[4,  7500] loss: 2.156589
1625.4548213481903
[4,  8000] loss: 2.230467
1643.3664319515228
[4,  8500] loss: 2.214743
1661.326878786087
[4,  9000] loss: 2.236268
1679.394183397293
[4,  9500] loss: 2.295266
1697.27929854393
[4, 10000] loss: 2.245356
1715.3003497123718
[4, 10500] loss: 2.216959
1733.2127346992493
[4, 11000] loss: 2.199081
1751.0932891368866
[4, 11500] loss: 2.224661
1769.0890922546387
[4, 12000] loss: 2.256357
1787.1452701091766
[4, 12500] loss: 2.220226
1805.0807447433472
Epoch [4] loss: 6900425.488604
[5,   500] loss: 2.188487
1823.2010915279388
[5,  1000] loss: 2.190229
1841.1870565414429
[5,  1500] loss: 2.212306
1859.3075869083405
[5,  2000] loss: 2.213696
1877.3692145347595
[5,  2500] loss: 2.194778
1895.307015657425
[5,  3000] loss: 2.188200
1913.269906282425
[5,  3500] loss: 2.178494
1931.1729125976562
[5,  4000] loss: 2.189391
1949.1059091091156
[5,  4500] loss: 2.188706
1967.2015676498413
[5,  5000] loss: 2.172330
1985.151623725891
[5,  5500] loss: 2.182992
2003.2735748291016
[5,  6000] loss: 2.164019
2021.3105652332306
[5,  6500] loss: 2.145449
2039.2447674274445
[5,  7000] loss: 2.139740
2057.2147607803345
[5,  7500] loss: 2.129716
2075.3185539245605
[5,  8000] loss: 2.139875
2093.1771416664124
[5,  8500] loss: 2.188933
2111.0937044620514
[5,  9000] loss: 2.158121
2128.969730615616
[5,  9500] loss: 2.168630
2146.773775577545
[5, 10000] loss: 2.165839
2164.50980424881
[5, 10500] loss: 2.143118
2182.257937192917
[5, 11000] loss: 2.130974
2200.020494222641
[5, 11500] loss: 2.261020
2217.9021611213684
[5, 12000] loss: 2.212742
2235.781517267227
[5, 12500] loss: 2.200943
2253.7659533023834
Epoch [5] loss: 6819370.027933
[6,   500] loss: 2.212792
2271.737331390381
[6,  1000] loss: 2.190988
2289.5960512161255
[6,  1500] loss: 2.195540
2307.606150150299
[6,  2000] loss: 2.206082
2325.583526134491
[6,  2500] loss: 2.187249
2343.627296924591
[6,  3000] loss: 2.179503
2361.5996856689453
[6,  3500] loss: 2.194085
2379.451346874237
[6,  4000] loss: 2.157202
2397.424020051956
[6,  4500] loss: 2.178595
2415.317608356476
[6,  5000] loss: 2.178553
2433.4176223278046
[6,  5500] loss: 2.130504
2451.340525150299
[6,  6000] loss: 2.214138
2469.3022286891937
[6,  6500] loss: 2.264826
2487.3164806365967
[6,  7000] loss: 2.201372
2505.2962453365326
[6,  7500] loss: 2.195761
2523.4234790802
[6,  8000] loss: 2.196907
2541.385423183441
[6,  8500] loss: 2.189883
2559.39684176445
[6,  9000] loss: 2.158172
2577.513004541397
[6,  9500] loss: 2.194980
2595.4508562088013
[6, 10000] loss: 2.166788
2613.4240753650665
[6, 10500] loss: 2.172400
2631.3765921592712
[6, 11000] loss: 2.146448
2649.347067832947
[6, 11500] loss: 2.160343
2667.2690007686615
[6, 12000] loss: 2.150332
2685.215847969055
[6, 12500] loss: 2.139575
2703.1473400592804
Epoch [6] loss: 6829621.686743
[7,   500] loss: 2.180002
2721.312439918518
[7,  1000] loss: 2.152492
2739.3274545669556
[7,  1500] loss: 2.143211
2757.3038556575775
[7,  2000] loss: 2.153089
2775.2756640911102
[7,  2500] loss: 2.137699
2793.235486984253
[7,  3000] loss: 2.161757
2811.190589427948
[7,  3500] loss: 2.142102
2829.2474887371063
[7,  4000] loss: 2.115189
2847.1477496623993
[7,  4500] loss: 2.114018
2865.1075072288513
[7,  5000] loss: 2.138055
2883.203780412674
[7,  5500] loss: 2.113392
2901.1820735931396
[7,  6000] loss: 2.139322
2919.173511505127
[7,  6500] loss: 2.112707
2937.2120509147644
[7,  7000] loss: 2.097688
2955.2962279319763
[7,  7500] loss: 2.129970
2973.3367998600006
[7,  8000] loss: 2.105934
2991.4169936180115
[7,  8500] loss: 2.101745
3009.380227804184
[7,  9000] loss: 2.104660
3027.3913159370422
[7,  9500] loss: 2.088002
3045.3453707695007
[7, 10000] loss: 2.082792
3063.358647584915
[7, 10500] loss: 2.100011
3081.3182933330536
[7, 11000] loss: 2.082970
3099.3573529720306
[7, 11500] loss: 2.087283
3117.4288082122803
[7, 12000] loss: 2.101421
3135.4164311885834
[7, 12500] loss: 2.058195
3153.4862418174744
Epoch [7] loss: 6618301.125980
[8,   500] loss: 2.089654
3171.656835079193
[8,  1000] loss: 2.109490
3189.6595499515533
[8,  1500] loss: 2.121736
3207.610743522644
[8,  2000] loss: 2.119112
3225.6671447753906
[8,  2500] loss: 2.104220
3243.653300523758
[8,  3000] loss: 2.086598
3261.8026027679443
[8,  3500] loss: 2.091538
3279.982243537903
[8,  4000] loss: 2.116538
3298.0437660217285
[8,  4500] loss: 2.119903
3316.168079853058
[8,  5000] loss: 2.054100
3334.5098280906677
[8,  5500] loss: 2.077236
3352.6940093040466
[8,  6000] loss: 2.048602
3371.0410573482513
[8,  6500] loss: 2.092933
3389.293266773224
[8,  7000] loss: 2.052047
3407.379202604294
[8,  7500] loss: 2.010529
3425.5006675720215
[8,  8000] loss: 2.041565
3443.72460269928
[8,  8500] loss: 2.001247
3461.915905237198
[8,  9000] loss: 2.049026
3480.320436477661
[8,  9500] loss: 2.047732
3498.5304963588715
[8, 10000] loss: 2.010127
3516.700091600418
[8, 10500] loss: 2.020410
3534.917979955673
[8, 11000] loss: 1.989594
3553.083146572113
[8, 11500] loss: 2.015664
3571.5419335365295
[8, 12000] loss: 1.997380
3589.766479730606
[8, 12500] loss: 2.005581
3608.073018312454
Epoch [8] loss: 6448482.668851
[9,   500] loss: 2.053301
3626.5708525180817
[9,  1000] loss: 2.038601
3644.804496526718
[9,  1500] loss: 1.996275
3662.955757379532
[9,  2000] loss: 2.021559
3680.914758205414
[9,  2500] loss: 1.952087
3699.0811138153076
[9,  3000] loss: 1.965427
3717.138813972473
[9,  3500] loss: 1.947964
3735.354999065399
[9,  4000] loss: 1.953348
3753.5372743606567
[9,  4500] loss: 1.969507
3771.7745718955994
[9,  5000] loss: 1.952756
3789.9791276454926
[9,  5500] loss: 1.985270
3808.1341881752014
[9,  6000] loss: 1.960976
3826.249913930893
[9,  6500] loss: 1.972119
3844.420746088028
[9,  7000] loss: 1.936620
3862.6490440368652
[9,  7500] loss: 1.946729
3880.8450498580933
[9,  8000] loss: 1.946583
3899.539294242859
[9,  8500] loss: 1.967833
3920.566380262375
[9,  9000] loss: 1.928158
3938.9504232406616
[9,  9500] loss: 1.910520
3957.2402780056
[9, 10000] loss: 1.902441
3975.4816720485687
[9, 10500] loss: 1.969951
3993.643546819687
[9, 11000] loss: 1.926201
4011.8866472244263
[9, 11500] loss: 1.917897
4030.0154638290405
[9, 12000] loss: 1.930746
4048.158858537674
[9, 12500] loss: 1.929853
4066.532986164093
Epoch [9] loss: 6130001.922248
[10,   500] loss: 1.925476
4088.508863925934
[10,  1000] loss: 1.936048
4106.636890888214
[10,  1500] loss: 1.993934
4124.744985580444
[10,  2000] loss: 2.001955
4142.968881368637
[10,  2500] loss: 2.000284
4161.107302188873
[10,  3000] loss: 1.944137
4179.231192350388
[10,  3500] loss: 1.967324
4197.399050712585
[10,  4000] loss: 1.998218
4215.2994821071625
[10,  4500] loss: 1.962305
4233.117824554443
[10,  5000] loss: 1.984017
4250.923562526703
[10,  5500] loss: 1.919911
4268.813991069794
[10,  6000] loss: 1.937804
4286.6245906353
[10,  6500] loss: 1.915338
4304.364736318588
[10,  7000] loss: 1.901213
4322.182145118713
[10,  7500] loss: 1.907894
4339.943888187408
[10,  8000] loss: 1.914958
4357.788852930069
[10,  8500] loss: 1.890215
4375.529591083527
[10,  9000] loss: 1.907578
4393.325788974762
[10,  9500] loss: 1.908648
4411.056557178497
[10, 10000] loss: 1.905819
4428.843073129654
[10, 10500] loss: 1.856830
4446.619970083237
[10, 11000] loss: 1.846429
4464.372607469559
[10, 11500] loss: 1.864125
4482.0733506679535
[10, 12000] loss: 1.844981
4499.85760641098
[10, 12500] loss: 1.857409
4517.612815618515
Epoch [10] loss: 6023856.768794
[11,   500] loss: 1.833169
4535.499153375626
[11,  1000] loss: 1.825965
4553.234133720398
[11,  1500] loss: 1.819785
4570.995840072632
[11,  2000] loss: 1.836746
4588.709782600403
[11,  2500] loss: 1.869801
4606.36635684967
[11,  3000] loss: 1.810842
4624.087608575821
[11,  3500] loss: 1.813631
4641.855714797974
[11,  4000] loss: 1.828143
4659.5914125442505
[11,  4500] loss: 1.863432
4677.398226261139
[11,  5000] loss: 1.805191
4695.081769227982
[11,  5500] loss: 1.889415
4712.849788427353
[11,  6000] loss: 1.863885
4730.6329934597015
[11,  6500] loss: 1.858117
4748.316028356552
[11,  7000] loss: 1.814057
4766.08985209465
[11,  7500] loss: 1.809754
4783.911270141602
[11,  8000] loss: 1.820643
4801.662310838699
[11,  8500] loss: 1.795199
4819.40149974823
[11,  9000] loss: 1.826573
4837.221937417984
[11,  9500] loss: 1.842960
4855.047979593277
[11, 10000] loss: 1.784058
4872.881077051163
[11, 10500] loss: 1.818850
4890.568458080292
[11, 11000] loss: 1.833798
4908.304895401001
[11, 11500] loss: 1.797490
4926.104784250259
[11, 12000] loss: 1.797445
4943.884896278381
[11, 12500] loss: 1.756532
4961.616546392441
Epoch [11] loss: 5724730.785567
[12,   500] loss: 1.783093
4979.506810665131
[12,  1000] loss: 1.755365
4997.384533882141
[12,  1500] loss: 1.790151
5015.162342071533
[12,  2000] loss: 1.709621
5032.896094799042
[12,  2500] loss: 1.720634
5050.645040035248
[12,  3000] loss: 1.771688
5068.396322965622
[12,  3500] loss: 1.740837
5086.147216081619
[12,  4000] loss: 1.734649
5103.918344497681
[12,  4500] loss: 1.752584
5121.764580011368
[12,  5000] loss: 1.742575
5139.400182962418
[12,  5500] loss: 1.713159
5157.053928852081
[12,  6000] loss: 1.800723
5174.735522270203
[12,  6500] loss: 1.799969
5192.539769411087
[12,  7000] loss: 1.788264
5210.37761759758
[12,  7500] loss: 1.765062
5228.19571185112
[12,  8000] loss: 1.804029
5245.909870147705
[12,  8500] loss: 1.850963
5263.717682361603
[12,  9000] loss: 1.763230
5281.567939758301
[12,  9500] loss: 1.811690
5299.281065702438
[12, 10000] loss: 1.898845
5317.134184360504
[12, 10500] loss: 1.864583
5334.912298679352
[12, 11000] loss: 1.794906
5352.633587598801
[12, 11500] loss: 1.817486
5370.434329509735
[12, 12000] loss: 1.811118
5388.23748922348
[12, 12500] loss: 1.817198
5406.106138467789
Epoch [12] loss: 5574098.566381
[13,   500] loss: 1.934211
5424.320434093475
[13,  1000] loss: 1.864726
5442.281546592712
[13,  1500] loss: 1.804429
5460.37663602829
[13,  2000] loss: 1.826542
5478.377712249756
[13,  2500] loss: 1.779512
5496.3594126701355
[13,  3000] loss: 1.806003
5514.3216280937195
[13,  3500] loss: 1.812509
5532.216803073883
[13,  4000] loss: 1.768422
5550.161057472229
[13,  4500] loss: 1.821917
5568.1417100429535
[13,  5000] loss: 1.795570
5586.06248831749
[13,  5500] loss: 1.765199
5604.021599769592
[13,  6000] loss: 1.764062
5622.047755718231
[13,  6500] loss: 1.747080
5640.045649528503
[13,  7000] loss: 1.721551
5658.067669868469
[13,  7500] loss: 1.763123
5676.10111951828
[13,  8000] loss: 1.746172
5694.177429676056
[13,  8500] loss: 1.683962
5712.164538383484
[13,  9000] loss: 1.730958
5730.192219734192
[13,  9500] loss: 1.692718
5748.252693414688
[13, 10000] loss: 1.664561
5766.217872619629
[13, 10500] loss: 1.675455
5784.221235752106
[13, 11000] loss: 1.707337
5802.299823760986
[13, 11500] loss: 1.683750
5820.324391126633
[13, 12000] loss: 1.721634
5838.185535907745
[13, 12500] loss: 1.696471
5856.20693564415
Epoch [13] loss: 5509743.286286
[14,   500] loss: 1.698187
5874.421061754227
[14,  1000] loss: 1.704407
5892.418952226639
[14,  1500] loss: 1.688131
5910.4133150577545
[14,  2000] loss: 1.693192
5928.4085302352905
[14,  2500] loss: 1.616580
5946.577111005783
[14,  3000] loss: 1.606184
5964.550164699554
[14,  3500] loss: 1.650741
5982.549671173096
[14,  4000] loss: 1.671636
6000.626775503159
[14,  4500] loss: 1.653573
6018.703864812851
[14,  5000] loss: 1.680382
6036.730054855347
[14,  5500] loss: 1.679662
6054.765021800995
[14,  6000] loss: 1.636162
6072.684438943863
[14,  6500] loss: 1.682442
6090.626770019531
[14,  7000] loss: 1.668366
6108.561460733414
[14,  7500] loss: 1.682724
6126.952661275864
[14,  8000] loss: 1.583538
6144.883260250092
[14,  8500] loss: 1.656705
6162.8849856853485
[14,  9000] loss: 1.663644
6180.776603460312
[14,  9500] loss: 1.638329
6198.857528924942
[14, 10000] loss: 1.641146
6216.946136474609
[14, 10500] loss: 1.669150
6235.0014724731445
[14, 11000] loss: 1.656041
6252.971380233765
[14, 11500] loss: 1.724687
6271.111482381821
[14, 12000] loss: 1.622359
6289.217350482941
[14, 12500] loss: 1.654463
6307.301155567169
Epoch [14] loss: 5219880.124558
[15,   500] loss: 1.601470
6325.487804412842
[15,  1000] loss: 1.621842
6343.541776895523
[15,  1500] loss: 1.643737
6361.604849815369
[15,  2000] loss: 1.624598
6379.4936509132385
[15,  2500] loss: 1.649117
6397.521963119507
[15,  3000] loss: 1.759820
6415.517978668213
[15,  3500] loss: 1.681123
6433.452895402908
[15,  4000] loss: 1.654576
6451.447060585022
[15,  4500] loss: 1.663770
6469.585156679153
[15,  5000] loss: 1.613308
6487.765994787216
[15,  5500] loss: 1.691198
6505.911627292633
[15,  6000] loss: 1.664470
6523.933622837067
[15,  6500] loss: 1.627289
6541.92498421669
[15,  7000] loss: 1.660911
6559.839748382568
[15,  7500] loss: 1.681369
6577.825807094574
[15,  8000] loss: 1.711928
6596.03614616394
[15,  8500] loss: 1.647168
6614.121517419815
[15,  9000] loss: 1.690420
6632.070224761963
[15,  9500] loss: 1.640776
6649.947373151779
[15, 10000] loss: 1.714051
6667.974187612534
[15, 10500] loss: 1.670078
6686.126238822937
[15, 11000] loss: 1.765125
6704.047535657883
[15, 11500] loss: 1.770078
6722.16064453125
[15, 12000] loss: 1.740714
6740.116745710373
[15, 12500] loss: 1.739940
6758.139918804169
Epoch [15] loss: 5255617.060101
[16,   500] loss: 1.767607
6776.303658723831
[16,  1000] loss: 1.736588
6794.349079370499
[16,  1500] loss: 1.720233
6812.451372385025
[16,  2000] loss: 1.713147
6830.327303647995
[16,  2500] loss: 1.680645
6848.244846820831
[16,  3000] loss: 1.708354
6866.1179366111755
[16,  3500] loss: 1.660084
6884.010849714279
[16,  4000] loss: 1.698192
6902.032644271851
[16,  4500] loss: 1.658240
6920.1222269535065
[16,  5000] loss: 1.641125
6938.014851570129
[16,  5500] loss: 1.626875
6956.0336492061615
[16,  6000] loss: 1.670021
6974.0412702560425
[16,  6500] loss: 1.674528
6992.002036809921
[16,  7000] loss: 1.675040
7009.841813087463
[16,  7500] loss: 1.666622
7027.6980521678925
[16,  8000] loss: 1.650748
7045.596114397049
[16,  8500] loss: 1.630927
7063.609660387039
[16,  9000] loss: 1.613844
7081.871729135513
[16,  9500] loss: 1.639755
7099.71179485321
[16, 10000] loss: 1.670321
7117.592099905014
[16, 10500] loss: 1.588283
7135.5804307460785
[16, 11000] loss: 1.634029
7153.492982625961
[16, 11500] loss: 1.687615
7171.505987882614
[16, 12000] loss: 1.660536
7189.492609977722
[16, 12500] loss: 1.655227
7207.489679098129
Epoch [16] loss: 5222895.588364
[17,   500] loss: 1.677031
7225.480922698975
[17,  1000] loss: 1.635019
7243.23860502243
[17,  1500] loss: 1.626813
7261.01065993309
[17,  2000] loss: 1.685960
7278.743669271469
[17,  2500] loss: 1.684973
7296.519968509674
[17,  3000] loss: 1.663478
7314.211069345474
[17,  3500] loss: 1.617889
7331.913872241974
[17,  4000] loss: 1.665593
7349.93864941597
[17,  4500] loss: 1.652257
7367.9564616680145
[17,  5000] loss: 1.573222
7385.931226968765
[17,  5500] loss: 1.686435
7404.06033539772
[17,  6000] loss: 1.630494
7421.99675154686
[17,  6500] loss: 1.618501
7440.014697790146
[17,  7000] loss: 1.625486
7458.001928806305
[17,  7500] loss: 1.626727
7476.044246435165
[17,  8000] loss: 1.637471
7493.888350248337
[17,  8500] loss: 1.617395
7511.894019126892
[17,  9000] loss: 1.563548
7529.878310918808
[17,  9500] loss: 1.607161
7547.781000137329
[17, 10000] loss: 1.597224
7565.778510808945
[17, 10500] loss: 1.584447
7583.633577108383
[17, 11000] loss: 1.567001
7601.64027762413
[17, 11500] loss: 1.600006
7619.673095703125
[17, 12000] loss: 1.593228
7637.534541845322
[17, 12500] loss: 1.589225
7655.42831325531
Epoch [17] loss: 5089304.692853
[18,   500] loss: 1.541527
7673.5020134449005
[18,  1000] loss: 1.586117
7691.407666444778
[18,  1500] loss: 1.535539
7709.343470096588
[18,  2000] loss: 1.577623
7727.45016002655
[18,  2500] loss: 1.553719
7745.361563682556
[18,  3000] loss: 1.592817
7763.294378519058
[18,  3500] loss: 1.651903
7781.174535512924
[18,  4000] loss: 1.641753
7799.410633563995
[18,  4500] loss: 1.625020
7817.732957363129
[18,  5000] loss: 1.628403
7835.769284248352
[18,  5500] loss: 1.575246
7853.753508806229
[18,  6000] loss: 1.555129
7871.8246693611145
[18,  6500] loss: 1.565317
7889.7940220832825
[18,  7000] loss: 1.590473
7907.7652752399445
[18,  7500] loss: 1.619807
7925.7945256233215
[18,  8000] loss: 1.633088
7943.63078379631
[18,  8500] loss: 1.653211
7961.608763217926
[18,  9000] loss: 1.613052
7986.022181749344
[18,  9500] loss: 1.649821
8004.016443967819
[18, 10000] loss: 1.617148
8022.079471349716
[18, 10500] loss: 1.638170
8040.022986650467
[18, 11000] loss: 1.600466
8057.986929178238
[18, 11500] loss: 1.583951
8075.976979732513
[18, 12000] loss: 1.579629
8093.957910299301
[18, 12500] loss: 1.535055
8111.878561735153
Epoch [18] loss: 5000187.909939
[19,   500] loss: 1.549119
8132.5911202430725
[19,  1000] loss: 1.522253
8150.665992259979
[19,  1500] loss: 1.543890
8168.7741050720215
[19,  2000] loss: 1.542271
8186.744187116623
[19,  2500] loss: 1.547550
8204.863279342651
[19,  3000] loss: 1.578067
8222.772159814835
[19,  3500] loss: 1.576191
8240.847020864487
[19,  4000] loss: 1.545612
8258.953275203705
[19,  4500] loss: 1.461675
8277.02618932724
[19,  5000] loss: 1.548673
8295.026553869247
[19,  5500] loss: 1.511661
8313.21337723732
[19,  6000] loss: 1.555060
8331.174536466599
[19,  6500] loss: 1.579012
8349.078040599823
[19,  7000] loss: 1.611027
8367.046278476715
[19,  7500] loss: 1.600173
8384.905994176865
[19,  8000] loss: 1.576568
8402.908739328384
[19,  8500] loss: 1.605965
8421.071635961533
[19,  9000] loss: 1.560951
8439.264537334442
[19,  9500] loss: 1.614821
8457.303977966309
[19, 10000] loss: 1.623838
8475.205958843231
[19, 10500] loss: 1.591017
8493.206743478775
[19, 11000] loss: 1.616625
8511.366290807724
[19, 11500] loss: 1.616434
8529.400556087494
[19, 12000] loss: 1.669823
8547.35797381401
[19, 12500] loss: 1.585916
8565.528886795044
Epoch [19] loss: 4942484.394789
[20,   500] loss: 1.560794
8584.009838104248
[20,  1000] loss: 1.570791
8602.090738296509
[20,  1500] loss: 1.587128
8620.02968287468
[20,  2000] loss: 1.541766
8637.902971744537
[20,  2500] loss: 1.540711
8655.875568628311
[20,  3000] loss: 1.627088
8673.990026712418
[20,  3500] loss: 1.572950
8691.915265321732
[20,  4000] loss: 1.548406
8709.917515039444
[20,  4500] loss: 1.541395
8727.834524393082
[20,  5000] loss: 1.568422
8745.944339513779
[20,  5500] loss: 1.532089
8764.136814594269
[20,  6000] loss: 1.561528
8782.06843495369
[20,  6500] loss: 1.572145
8800.159345149994
[20,  7000] loss: 1.587294
8818.164939880371
[20,  7500] loss: 1.506741
8836.221375465393
[20,  8000] loss: 1.532441
8854.494128465652
[20,  8500] loss: 1.565359
8872.416669607162
[20,  9000] loss: 1.533284
8890.366854667664
[20,  9500] loss: 1.683563
8908.377188920975
[20, 10000] loss: 1.600415
8926.368501663208
[20, 10500] loss: 1.562837
8944.408999919891
[20, 11000] loss: 1.593474
8962.438712358475
[20, 11500] loss: 1.588413
8980.440518140793
[20, 12000] loss: 1.565201
8998.161354541779
[20, 12500] loss: 1.616927
9015.902157306671
Epoch [20] loss: 4930096.543960
Finished Training
Saving model to /data/s4091221/trained-models/resnet502020-02-26 08:36:47.319766
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-3.9179, -4.1133, -2.8200, -1.3290, -2.9610, -1.5915, -2.4975, -2.7938,
         -2.9567, -3.8060],
        [-2.7150, -2.3736, -6.6384, -6.2765, -7.2140, -7.3539, -6.8525, -8.0575,
         -1.3408, -2.5933],
        [-1.8659, -2.2669, -4.9881, -5.6839, -5.6519, -6.2089, -6.0103, -6.2435,
         -0.5940, -2.0711],
        [-0.8387, -1.0921, -2.2716, -2.2822, -2.4053, -3.0042, -2.4899, -2.8698,
         -0.0483, -0.5538]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat  ship  ship  ship
Accuracy of the network on the 4000.0 test images: 40 %


###############################################################################
Peregrine Cluster
Job 9751126 for user 's4091221'
Finished at: Wed Feb 26 08:37:22 CET 2020

Job details:
============

Name                : resnet50.sh
User                : s4091221
Partition           : gpu
Nodes               : pg-gpu14
Cores               : 12
State               : COMPLETED
Submit              : 2020-02-25T16:13:48
Start               : 2020-02-25T21:18:03
End                 : 2020-02-26T08:37:22
Reserved walltime   : 20:00:00
Used walltime       : 11:19:19
Used CPU time       : 11:58:24 (efficiency:  8.81%)
% User (Computation): 97.34%
% System (I/O)      :  2.66%
Mem reserved        : 12000M/node
Max Mem used        : 2.96G (pg-gpu14)
Max Disk Write      : 1.57G (pg-gpu14)
Max Disk Read       : 6.40G (pg-gpu14)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
