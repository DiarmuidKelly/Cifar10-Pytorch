Requirement already satisfied: torchvision in ./.local/lib/python3.7/site-packages (0.5.0)
Requirement already satisfied: numpy in ./.local/lib/python3.7/site-packages (from torchvision) (1.18.1)
Requirement already satisfied: pillow>=4.1.1 in ./.local/lib/python3.7/site-packages (from torchvision) (7.0.0)
Requirement already satisfied: torch==1.4.0 in ./.local/lib/python3.7/site-packages (from torchvision) (1.4.0)
Requirement already satisfied: six in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from torchvision) (1.12.0)
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': True, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=34, momentum=0, normalise=True, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 frog  ship  deer   dog
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model wide_resnet50_2 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model wide_resnet50_2 Reshaped
Number of parameters: 66854730 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582663164.0176103
[1,   500] loss: 2.613784
14.110611915588379
[1,  1000] loss: 2.555169
26.511553287506104
[1,  1500] loss: 2.487091
38.95923471450806
[1,  2000] loss: 2.523861
51.749183177948
[1,  2500] loss: 2.480797
64.3999183177948
[1,  3000] loss: 2.483250
76.91545629501343
[1,  3500] loss: 2.436872
89.28029155731201
[1,  4000] loss: 2.399352
101.58955121040344
[1,  4500] loss: 2.429307
113.99612975120544
[1,  5000] loss: 2.359769
126.32676696777344
[1,  5500] loss: 2.372123
138.7574155330658
[1,  6000] loss: 2.377950
151.24543619155884
[1,  6500] loss: 2.386256
163.60763812065125
[1,  7000] loss: 2.343831
175.8993706703186
[1,  7500] loss: 2.371779
188.3543040752411
[1,  8000] loss: 2.317852
200.69056677818298
[1,  8500] loss: 2.294246
213.04879450798035
[1,  9000] loss: 2.289298
225.51274061203003
[1,  9500] loss: 2.290420
237.89206981658936
[1, 10000] loss: 2.258065
250.27379083633423
[1, 10500] loss: 2.311120
262.70372438430786
[1, 11000] loss: 2.227846
274.92743372917175
[1, 11500] loss: 2.215368
287.2124240398407
[1, 12000] loss: 2.225118
299.45731234550476
[1, 12500] loss: 2.221203
311.7834949493408
Epoch [1] loss: 7440482.306818
[2,   500] loss: 2.188691
324.3622672557831
[2,  1000] loss: 2.182175
336.7982997894287
[2,  1500] loss: 2.185289
349.2263071537018
[2,  2000] loss: 2.190143
361.7136149406433
[2,  2500] loss: 2.149225
374.21727323532104
[2,  3000] loss: 2.124801
386.5638711452484
[2,  3500] loss: 2.163587
398.9385278224945
[2,  4000] loss: 2.151174
411.2827847003937
[2,  4500] loss: 2.160609
423.81592655181885
[2,  5000] loss: 2.171741
436.50118136405945
[2,  5500] loss: 2.157071
449.0026113986969
[2,  6000] loss: 2.137351
461.39565920829773
[2,  6500] loss: 2.154691
473.8920693397522
[2,  7000] loss: 2.162080
486.1817705631256
[2,  7500] loss: 2.138972
498.5039372444153
[2,  8000] loss: 2.104021
510.8243420124054
[2,  8500] loss: 2.076713
523.1121315956116
[2,  9000] loss: 2.059197
535.4010572433472
[2,  9500] loss: 2.066997
547.8574326038361
[2, 10000] loss: 2.130393
560.3776106834412
[2, 10500] loss: 2.084049
572.8674385547638
[2, 11000] loss: 2.117542
585.2636370658875
[2, 11500] loss: 2.065146
597.6052484512329
[2, 12000] loss: 2.034706
610.033695936203
[2, 12500] loss: 2.030084
622.3971481323242
Epoch [2] loss: 6678863.922577
[3,   500] loss: 2.016897
634.9032967090607
[3,  1000] loss: 1.993618
647.4642655849457
[3,  1500] loss: 1.986158
660.2303085327148
[3,  2000] loss: 1.975731
672.805999994278
[3,  2500] loss: 1.965959
685.2070841789246
[3,  3000] loss: 1.989943
697.7015724182129
[3,  3500] loss: 2.000054
710.1756217479706
[3,  4000] loss: 1.957235
722.5669372081757
[3,  4500] loss: 1.960620
734.8926100730896
[3,  5000] loss: 1.952559
747.2249579429626
[3,  5500] loss: 1.947719
759.625301361084
[3,  6000] loss: 1.946300
772.1271352767944
[3,  6500] loss: 1.927031
784.4694752693176
[3,  7000] loss: 1.952257
796.846174955368
[3,  7500] loss: 1.950973
809.238853931427
[3,  8000] loss: 1.952283
821.6131110191345
[3,  8500] loss: 1.868657
834.1094398498535
[3,  9000] loss: 1.909058
846.4866180419922
[3,  9500] loss: 1.931154
858.7710852622986
[3, 10000] loss: 1.894520
871.069874048233
[3, 10500] loss: 1.898844
883.4212861061096
[3, 11000] loss: 1.882992
895.7497637271881
[3, 11500] loss: 1.842848
908.1296093463898
[3, 12000] loss: 1.876535
920.6151785850525
[3, 12500] loss: 1.878264
933.0584032535553
Epoch [3] loss: 6070777.899452
[4,   500] loss: 1.845207
945.6700720787048
[4,  1000] loss: 1.835663
958.0753381252289
[4,  1500] loss: 1.880850
970.4214315414429
[4,  2000] loss: 1.782146
983.0227754116058
[4,  2500] loss: 1.830839
995.4394860267639
[4,  3000] loss: 1.818687
1008.0117835998535
[4,  3500] loss: 1.818083
1020.5865821838379
[4,  4000] loss: 1.844547
1033.0887508392334
[4,  4500] loss: 1.826138
1045.491917848587
[4,  5000] loss: 1.808161
1057.8498692512512
[4,  5500] loss: 1.832220
1070.1816036701202
[4,  6000] loss: 1.809569
1082.4918117523193
[4,  6500] loss: 1.763165
1094.85595369339
[4,  7000] loss: 1.793477
1107.211107969284
[4,  7500] loss: 1.810581
1119.527048587799
[4,  8000] loss: 1.786721
1131.9435105323792
[4,  8500] loss: 1.767966
1144.3199801445007
[4,  9000] loss: 1.782247
1156.7577605247498
[4,  9500] loss: 1.751711
1169.1594064235687
[4, 10000] loss: 1.790558
1181.4628353118896
[4, 10500] loss: 1.750174
1193.8521676063538
[4, 11000] loss: 1.773444
1206.240365743637
[4, 11500] loss: 1.697365
1218.809728384018
[4, 12000] loss: 1.765749
1231.1909098625183
[4, 12500] loss: 1.730594
1243.807415485382
Epoch [4] loss: 5625623.546958
[5,   500] loss: 1.669749
1256.5074536800385
[5,  1000] loss: 1.743426
1269.0468146800995
[5,  1500] loss: 1.757613
1281.4046382904053
[5,  2000] loss: 1.728462
1293.7659072875977
[5,  2500] loss: 1.737268
1306.3692419528961
[5,  3000] loss: 1.666618
1318.7105557918549
[5,  3500] loss: 1.713893
1331.2795314788818
[5,  4000] loss: 1.738219
1343.913414478302
[5,  4500] loss: 1.705445
1356.4065518379211
[5,  5000] loss: 1.729909
1368.842349767685
[5,  5500] loss: 1.692263
1381.1857981681824
[5,  6000] loss: 1.713837
1393.5894153118134
[5,  6500] loss: 1.667968
1405.9874997138977
[5,  7000] loss: 1.652528
1418.3627166748047
[5,  7500] loss: 1.658882
1430.7841637134552
[5,  8000] loss: 1.682998
1443.1715126037598
[5,  8500] loss: 1.706388
1455.5392534732819
[5,  9000] loss: 1.705236
1467.8721175193787
[5,  9500] loss: 1.672045
1480.2357168197632
[5, 10000] loss: 1.597153
1492.7643325328827
[5, 10500] loss: 1.620621
1505.1930091381073
[5, 11000] loss: 1.641578
1517.495947599411
[5, 11500] loss: 1.584133
1529.9917137622833
[5, 12000] loss: 1.635854
1542.4589250087738
[5, 12500] loss: 1.648107
1554.8320882320404
Epoch [5] loss: 5255094.869677
[6,   500] loss: 1.589194
1567.3602974414825
[6,  1000] loss: 1.662367
1579.7791316509247
[6,  1500] loss: 1.603703
1592.2140142917633
[6,  2000] loss: 1.600195
1604.866153717041
[6,  2500] loss: 1.574734
1617.303782939911
[6,  3000] loss: 1.584447
1629.660388469696
[6,  3500] loss: 1.570236
1641.9889245033264
[6,  4000] loss: 1.611843
1654.5754253864288
[6,  4500] loss: 1.621844
1666.9316823482513
[6,  5000] loss: 1.614051
1679.29847407341
[6,  5500] loss: 1.587362
1691.6774306297302
[6,  6000] loss: 1.567949
1704.1284430027008
[6,  6500] loss: 1.609839
1716.5633234977722
[6,  7000] loss: 1.578994
1729.014446258545
[6,  7500] loss: 1.578265
1741.3938660621643
[6,  8000] loss: 1.607475
1753.809458732605
[6,  8500] loss: 1.630814
1766.1923911571503
[6,  9000] loss: 1.535149
1778.5509696006775
[6,  9500] loss: 1.581633
1791.0560183525085
[6, 10000] loss: 1.541954
1803.725947380066
[6, 10500] loss: 1.605191
1816.4492757320404
[6, 11000] loss: 1.595558
1829.2263371944427
[6, 11500] loss: 1.531700
1842.011744260788
[6, 12000] loss: 1.547245
1855.0886311531067
[6, 12500] loss: 1.558123
1868.1239693164825
Epoch [6] loss: 4968453.341165
[7,   500] loss: 1.530413
1880.9792964458466
[7,  1000] loss: 1.517391
1893.6998732089996
[7,  1500] loss: 1.593999
1906.5380313396454
[7,  2000] loss: 1.504309
1919.314210653305
[7,  2500] loss: 1.516875
1932.0508983135223
[7,  3000] loss: 1.512793
1944.8772158622742
[7,  3500] loss: 1.498408
1957.554648399353
[7,  4000] loss: 1.516316
1970.3289546966553
[7,  4500] loss: 1.530955
1983.230515241623
[7,  5000] loss: 1.496655
1996.0607919692993
[7,  5500] loss: 1.510421
2008.798035621643
[7,  6000] loss: 1.498065
2021.596003293991
[7,  6500] loss: 1.533856
2034.4055652618408
[7,  7000] loss: 1.511524
2047.2362430095673
[7,  7500] loss: 1.519782
2059.9848923683167
[7,  8000] loss: 1.512940
2072.688034057617
[7,  8500] loss: 1.503906
2085.5140092372894
[7,  9000] loss: 1.490418
2098.257782936096
[7,  9500] loss: 1.547737
2110.861423969269
[7, 10000] loss: 1.503917
2123.279646873474
[7, 10500] loss: 1.480217
2135.715993642807
[7, 11000] loss: 1.482654
2148.169266939163
[7, 11500] loss: 1.458867
2160.643614053726
[7, 12000] loss: 1.465003
2173.1766114234924
[7, 12500] loss: 1.485310
2185.5490033626556
Epoch [7] loss: 4728899.608675
[8,   500] loss: 1.410301
2198.0381515026093
[8,  1000] loss: 1.454126
2210.4801292419434
[8,  1500] loss: 1.501293
2223.164863586426
[8,  2000] loss: 1.443870
2236.0030579566956
[8,  2500] loss: 1.475075
2248.7604229450226
[8,  3000] loss: 1.455399
2261.4436144828796
[8,  3500] loss: 1.467636
2274.073645591736
[8,  4000] loss: 1.416917
2286.6821405887604
[8,  4500] loss: 1.483374
2299.472587585449
[8,  5000] loss: 1.468435
2312.245593070984
[8,  5500] loss: 1.409010
2324.905492782593
[8,  6000] loss: 1.477178
2337.337817668915
[8,  6500] loss: 1.420417
2349.798492670059
[8,  7000] loss: 1.465267
2362.2011382579803
[8,  7500] loss: 1.405884
2374.5978033542633
[8,  8000] loss: 1.379461
2386.957911491394
[8,  8500] loss: 1.442561
2399.27245926857
[8,  9000] loss: 1.438966
2411.572984933853
[8,  9500] loss: 1.375270
2423.7989156246185
[8, 10000] loss: 1.431169
2436.326343536377
[8, 10500] loss: 1.410406
2449.017277956009
[8, 11000] loss: 1.453118
2461.7164471149445
[8, 11500] loss: 1.396667
2474.4290895462036
[8, 12000] loss: 1.422097
2487.060888528824
[8, 12500] loss: 1.438716
2499.5226967334747
Epoch [8] loss: 4503680.734714
[9,   500] loss: 1.341686
2512.119532585144
[9,  1000] loss: 1.394331
2524.4340603351593
[9,  1500] loss: 1.366471
2536.791024684906
[9,  2000] loss: 1.357262
2549.154013156891
[9,  2500] loss: 1.402290
2561.5438866615295
[9,  3000] loss: 1.369236
2573.9534759521484
[9,  3500] loss: 1.426020
2586.2620978355408
[9,  4000] loss: 1.370005
2598.7063477039337
[9,  4500] loss: 1.385858
2611.174175977707
[9,  5000] loss: 1.366642
2623.8272914886475
[9,  5500] loss: 1.394810
2636.258369445801
[9,  6000] loss: 1.358403
2648.6702296733856
[9,  6500] loss: 1.380986
2661.0250198841095
[9,  7000] loss: 1.403527
2673.461967229843
[9,  7500] loss: 1.371393
2686.0207307338715
[9,  8000] loss: 1.396529
2698.3524515628815
[9,  8500] loss: 1.396397
2710.7679121494293
[9,  9000] loss: 1.385355
2723.2272651195526
[9,  9500] loss: 1.366299
2735.600728750229
[9, 10000] loss: 1.362173
2748.0711753368378
[9, 10500] loss: 1.321829
2760.4036135673523
[9, 11000] loss: 1.358248
2772.617010831833
[9, 11500] loss: 1.342130
2784.842056751251
[9, 12000] loss: 1.389734
2797.236389398575
[9, 12500] loss: 1.354710
2809.6826677322388
Epoch [9] loss: 4297149.128952
[10,   500] loss: 1.287334
2822.214799642563
[10,  1000] loss: 1.278257
2834.769833087921
[10,  1500] loss: 1.305585
2847.1830475330353
[10,  2000] loss: 1.294506
2859.5151534080505
[10,  2500] loss: 1.297599
2871.8241078853607
[10,  3000] loss: 1.356089
2883.992306947708
[10,  3500] loss: 1.358866
2896.1714980602264
[10,  4000] loss: 1.311243
2908.473842382431
[10,  4500] loss: 1.283864
2920.7231669425964
[10,  5000] loss: 1.336435
2933.200868844986
[10,  5500] loss: 1.331661
2945.755021095276
[10,  6000] loss: 1.304300
2958.321329832077
[10,  6500] loss: 1.300070
2970.7699291706085
[10,  7000] loss: 1.329170
2983.2588844299316
[10,  7500] loss: 1.277295
2995.670330286026
[10,  8000] loss: 1.308584
3008.0755977630615
[10,  8500] loss: 1.272707
3020.4562878608704
[10,  9000] loss: 1.310495
3032.760508298874
[10,  9500] loss: 1.300753
3045.3826110363007
[10, 10000] loss: 1.314654
3057.805121898651
[10, 10500] loss: 1.244278
3070.2811603546143
[10, 11000] loss: 1.323954
3082.829852104187
[10, 11500] loss: 1.350649
3095.1762070655823
[10, 12000] loss: 1.309557
3107.580587863922
[10, 12500] loss: 1.304299
3119.8750178813934
Epoch [10] loss: 4110371.989941
[11,   500] loss: 1.309215
3132.4058344364166
[11,  1000] loss: 1.281522
3144.9308185577393
[11,  1500] loss: 1.288919
3157.4156584739685
[11,  2000] loss: 1.260361
3169.8969116210938
[11,  2500] loss: 1.274116
3182.48708820343
[11,  3000] loss: 1.287277
3195.081016778946
[11,  3500] loss: 1.279204
3207.602602005005
[11,  4000] loss: 1.208415
3220.073394060135
[11,  4500] loss: 1.278758
3232.545346260071
[11,  5000] loss: 1.231386
3244.8548889160156
[11,  5500] loss: 1.237589
3257.5593571662903
[11,  6000] loss: 1.267002
3270.16379904747
[11,  6500] loss: 1.266842
3282.583622932434
[11,  7000] loss: 1.215543
3295.141762971878
[11,  7500] loss: 1.208981
3307.4776661396027
[11,  8000] loss: 1.231839
3319.7666325569153
[11,  8500] loss: 1.234730
3332.090057373047
[11,  9000] loss: 1.252979
3344.4768679142
[11,  9500] loss: 1.202333
3356.8737688064575
[11, 10000] loss: 1.232698
3369.1895706653595
[11, 10500] loss: 1.216776
3381.565281867981
[11, 11000] loss: 1.300038
3393.91242313385
[11, 11500] loss: 1.259710
3406.3812527656555
[11, 12000] loss: 1.247130
3418.8933687210083
[11, 12500] loss: 1.297351
3431.169998407364
Epoch [11] loss: 3949330.315940
[12,   500] loss: 1.194546
3443.5916604995728
[12,  1000] loss: 1.200855
3455.868384361267
[12,  1500] loss: 1.217540
3468.3223645687103
[12,  2000] loss: 1.210902
3480.7041957378387
[12,  2500] loss: 1.232901
3493.0698232650757
[12,  3000] loss: 1.169544
3505.4898312091827
[12,  3500] loss: 1.159720
3517.9895668029785
[12,  4000] loss: 1.221983
3530.5072202682495
[12,  4500] loss: 1.236640
3542.983357667923
[12,  5000] loss: 1.208525
3555.6427693367004
[12,  5500] loss: 1.204547
3568.230175256729
[12,  6000] loss: 1.261930
3580.858113527298
[12,  6500] loss: 1.155153
3593.4413969516754
[12,  7000] loss: 1.286780
3605.763350009918
[12,  7500] loss: 1.200535
3618.030424594879
[12,  8000] loss: 1.166895
3630.5614471435547
[12,  8500] loss: 1.189990
3643.124037504196
[12,  9000] loss: 1.178439
3655.6333017349243
[12,  9500] loss: 1.212944
3668.167072534561
[12, 10000] loss: 1.187221
3680.5326619148254
[12, 10500] loss: 1.249537
3692.949795484543
[12, 11000] loss: 1.213327
3705.2838790416718
[12, 11500] loss: 1.150347
3717.64386177063
[12, 12000] loss: 1.254246
3730.0613498687744
[12, 12500] loss: 1.209368
3742.4856474399567
Epoch [12] loss: 3796471.211514
[13,   500] loss: 1.114838
3755.021415710449
[13,  1000] loss: 1.161988
3767.464535713196
[13,  1500] loss: 1.177168
3779.7843692302704
[13,  2000] loss: 1.152619
3792.2421929836273
[13,  2500] loss: 1.176183
3804.6843614578247
[13,  3000] loss: 1.220583
3817.169775247574
[13,  3500] loss: 1.191455
3829.557416677475
[13,  4000] loss: 1.182754
3842.234223842621
[13,  4500] loss: 1.192880
3854.873901605606
[13,  5000] loss: 1.164356
3867.4452016353607
[13,  5500] loss: 1.135502
3880.051480770111
[13,  6000] loss: 1.130122
3892.5087661743164
[13,  6500] loss: 1.186145
3904.918040037155
[13,  7000] loss: 1.140751
3918.1858098506927
[13,  7500] loss: 1.158543
3930.5465874671936
[13,  8000] loss: 1.178172
3942.829996585846
[13,  8500] loss: 1.143451
3955.0937588214874
[13,  9000] loss: 1.132392
3967.4972314834595
[13,  9500] loss: 1.144837
3979.884379386902
[13, 10000] loss: 1.163976
3992.178088903427
[13, 10500] loss: 1.104944
4004.589104413986
[13, 11000] loss: 1.162488
4016.8516533374786
[13, 11500] loss: 1.170150
4029.1540966033936
[13, 12000] loss: 1.095098
4041.4309117794037
[13, 12500] loss: 1.141285
4053.732642889023
Epoch [13] loss: 3634385.618395
[14,   500] loss: 1.089710
4067.8299627304077
[14,  1000] loss: 1.088401
4080.1844420433044
[14,  1500] loss: 1.100094
4092.410732984543
[14,  2000] loss: 1.090232
4104.6451587677
[14,  2500] loss: 1.109085
4116.924058198929
[14,  3000] loss: 1.137954
4129.182606458664
[14,  3500] loss: 1.093494
4141.504070281982
[14,  4000] loss: 1.103952
4153.810470342636
[14,  4500] loss: 1.116313
4166.254990816116
[14,  5000] loss: 1.110438
4178.500024318695
[14,  5500] loss: 1.076419
4190.853787899017
[14,  6000] loss: 1.095139
4203.2505939006805
[14,  6500] loss: 1.094946
4215.7737193107605
[14,  7000] loss: 1.095421
4228.457136392593
[14,  7500] loss: 1.146809
4241.075705528259
[14,  8000] loss: 1.107415
4253.848786115646
[14,  8500] loss: 1.102769
4266.607397556305
[14,  9000] loss: 1.098549
4279.0941252708435
[14,  9500] loss: 1.151016
4291.437240362167
[14, 10000] loss: 1.069021
4303.854031324387
[14, 10500] loss: 1.084050
4316.279172182083
[14, 11000] loss: 1.107631
4328.642730474472
[14, 11500] loss: 1.080645
4341.023323535919
[14, 12000] loss: 1.134705
4353.37229180336
[14, 12500] loss: 1.100456
4365.639742612839
Epoch [14] loss: 3449772.698295
[15,   500] loss: 1.013392
4378.809730529785
[15,  1000] loss: 1.023688
4391.142584323883
[15,  1500] loss: 1.062677
4403.335963010788
[15,  2000] loss: 1.007322
4415.611194372177
[15,  2500] loss: 1.034498
4427.93474817276
[15,  3000] loss: 1.034995
4440.288734436035
[15,  3500] loss: 1.032800
4452.542840242386
[15,  4000] loss: 1.034157
4464.872493982315
[15,  4500] loss: 1.079281
4477.086905002594
[15,  5000] loss: 1.074279
4489.319269418716
[15,  5500] loss: 1.079954
4501.593548536301
[15,  6000] loss: 1.007336
4513.8402807712555
[15,  6500] loss: 1.008633
4526.182948589325
[15,  7000] loss: 1.054103
4538.831312894821
[15,  7500] loss: 1.044914
4551.307466506958
[15,  8000] loss: 1.050189
4563.705104827881
[15,  8500] loss: 1.037221
4576.177130699158
[15,  9000] loss: 1.067496
4588.57768535614
[15,  9500] loss: 1.062287
4600.893141269684
[15, 10000] loss: 1.023916
4613.4520790576935
[15, 10500] loss: 1.062086
4625.826359033585
[15, 11000] loss: 1.023588
4638.240960359573
[15, 11500] loss: 1.060052
4650.602511882782
[15, 12000] loss: 1.078886
4663.0361568927765
[15, 12500] loss: 1.057858
4675.360515356064
Epoch [15] loss: 3277925.490789
[16,   500] loss: 0.964486
4687.846635341644
[16,  1000] loss: 0.994964
4700.232135057449
[16,  1500] loss: 0.987408
4712.618735790253
[16,  2000] loss: 0.993926
4725.026964664459
[16,  2500] loss: 0.978252
4737.383914470673
[16,  3000] loss: 0.993509
4749.773144721985
[16,  3500] loss: 0.975493
4762.136768341064
[16,  4000] loss: 1.009044
4774.47874212265
[16,  4500] loss: 0.992792
4786.728180408478
[16,  5000] loss: 0.999871
4799.107394933701
[16,  5500] loss: 1.027492
4811.4493243694305
[16,  6000] loss: 0.989259
4823.789930343628
[16,  6500] loss: 1.003436
4836.156872749329
[16,  7000] loss: 0.995837
4848.727455377579
[16,  7500] loss: 0.972430
4861.383269309998
[16,  8000] loss: 0.974067
4874.044215917587
[16,  8500] loss: 0.998109
4886.473562717438
[16,  9000] loss: 0.953202
4898.805881023407
[16,  9500] loss: 0.957062
4911.254686355591
[16, 10000] loss: 1.024620
4923.521630048752
[16, 10500] loss: 0.961034
4935.863083600998
[16, 11000] loss: 1.043217
4948.269967556
[16, 11500] loss: 1.009250
4960.61448597908
[16, 12000] loss: 1.062718
4972.9575934410095
[16, 12500] loss: 1.024881
4985.307598352432
Epoch [16] loss: 3115789.936823
[17,   500] loss: 0.958871
4997.9830865859985
[17,  1000] loss: 0.939817
5010.408017873764
[17,  1500] loss: 0.916979
5022.858195543289
[17,  2000] loss: 0.942806
5035.198610305786
[17,  2500] loss: 0.981859
5047.414510726929
[17,  3000] loss: 0.931482
5059.739365577698
[17,  3500] loss: 0.954668
5072.130461215973
[17,  4000] loss: 0.914888
5084.508236169815
[17,  4500] loss: 0.963485
5096.864986181259
[17,  5000] loss: 0.936036
5109.244836568832
[17,  5500] loss: 0.917200
5121.585726737976
[17,  6000] loss: 0.938299
5133.961038827896
[17,  6500] loss: 0.949698
5146.37841963768
[17,  7000] loss: 0.981322
5158.810206651688
[17,  7500] loss: 0.925749
5171.156847476959
[17,  8000] loss: 0.948540
5183.774364233017
[17,  8500] loss: 0.963338
5196.252409696579
[17,  9000] loss: 0.959927
5208.5790848731995
[17,  9500] loss: 0.967616
5221.044743537903
[17, 10000] loss: 0.976505
5233.450692653656
[17, 10500] loss: 0.972383
5245.753141880035
[17, 11000] loss: 1.001468
5258.043949604034
[17, 11500] loss: 0.972292
5270.4611394405365
[17, 12000] loss: 1.000834
5282.799316644669
[17, 12500] loss: 0.918185
5295.337697029114
Epoch [17] loss: 2986893.318717
[18,   500] loss: 0.879314
5307.872421503067
[18,  1000] loss: 0.862494
5320.292690753937
[18,  1500] loss: 0.939984
5332.874335765839
[18,  2000] loss: 0.916427
5345.263573884964
[18,  2500] loss: 0.881920
5357.579170227051
[18,  3000] loss: 0.916142
5369.88350892067
[18,  3500] loss: 0.935054
5382.309418678284
[18,  4000] loss: 0.906382
5394.7079038619995
[18,  4500] loss: 0.892875
5407.035443544388
[18,  5000] loss: 0.942096
5419.437155246735
[18,  5500] loss: 0.951881
5431.913228750229
[18,  6000] loss: 0.920018
5444.571792125702
[18,  6500] loss: 0.938033
5457.202958345413
[18,  7000] loss: 0.908095
5469.839037656784
[18,  7500] loss: 0.922571
5482.253229856491
[18,  8000] loss: 0.930430
5494.6657774448395
[18,  8500] loss: 0.960374
5507.184515237808
[18,  9000] loss: 0.959823
5519.595489025116
[18,  9500] loss: 0.939994
5531.959786891937
[18, 10000] loss: 0.955141
5544.394384622574
[18, 10500] loss: 0.911667
5556.747527837753
[18, 11000] loss: 0.931103
5568.996418237686
[18, 11500] loss: 0.972597
5581.289146900177
[18, 12000] loss: 0.972737
5593.583832502365
[18, 12500] loss: 0.964604
5605.99662065506
Epoch [18] loss: 2904239.700306
[19,   500] loss: 0.867299
5618.415886163712
[19,  1000] loss: 0.839775
5630.692222833633
[19,  1500] loss: 0.857712
5643.168889760971
[19,  2000] loss: 0.837072
5655.661736011505
[19,  2500] loss: 0.864603
5668.134979009628
[19,  3000] loss: 0.854343
5680.446304321289
[19,  3500] loss: 0.858703
5692.684606552124
[19,  4000] loss: 0.869339
5704.869238853455
[19,  4500] loss: 0.870418
5717.300751686096
[19,  5000] loss: 0.842392
5729.678965806961
[19,  5500] loss: 0.880491
5742.043344974518
[19,  6000] loss: 0.917708
5754.380680322647
[19,  6500] loss: 0.881964
5766.62805390358
[19,  7000] loss: 0.897631
5778.9537851810455
[19,  7500] loss: 0.846761
5791.272203207016
[19,  8000] loss: 0.874562
5803.8005928993225
[19,  8500] loss: 0.875632
5816.15985918045
[19,  9000] loss: 0.890431
5828.638099193573
[19,  9500] loss: 0.882051
5840.927761554718
[19, 10000] loss: 0.886134
5853.206856966019
[19, 10500] loss: 0.867470
5865.492244243622
[19, 11000] loss: 0.880568
5877.8375415802
[19, 11500] loss: 0.850149
5890.125949621201
[19, 12000] loss: 0.839111
5902.453804731369
[19, 12500] loss: 0.923083
5914.760720014572
Epoch [19] loss: 2724148.364876
[20,   500] loss: 0.844294
5927.1782875061035
[20,  1000] loss: 0.783231
5939.548738002777
[20,  1500] loss: 0.848648
5952.101007461548
[20,  2000] loss: 0.794113
5964.6084420681
[20,  2500] loss: 0.779684
5976.9065935611725
[20,  3000] loss: 0.816353
5989.260970830917
[20,  3500] loss: 0.810831
6001.667277574539
[20,  4000] loss: 0.847092
6014.0276210308075
[20,  4500] loss: 0.787894
6026.465354204178
[20,  5000] loss: 0.791060
6038.946683168411
[20,  5500] loss: 0.830375
6051.677716493607
[20,  6000] loss: 0.854923
6064.209738969803
[20,  6500] loss: 0.827506
6076.811672449112
[20,  7000] loss: 0.863624
6089.346694469452
[20,  7500] loss: 0.838212
6101.790279150009
[20,  8000] loss: 0.824898
6114.24374461174
[20,  8500] loss: 0.850605
6126.666380167007
[20,  9000] loss: 0.828056
6139.18411898613
[20,  9500] loss: 0.862382
6151.685344696045
[20, 10000] loss: 0.839835
6164.151637792587
[20, 10500] loss: 0.856503
6176.515702486038
[20, 11000] loss: 0.824589
6188.920773506165
[20, 11500] loss: 0.839857
6201.347767591476
[20, 12000] loss: 0.813537
6213.797778844833
[20, 12500] loss: 0.865834
6226.189195156097
Epoch [20] loss: 2604884.763557
Finished Training
Saving model to /data/s4091221/trained-models/wide_resnet50_22020-02-25 23:23:10.253879
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-0.1078, -1.2173,  1.8658,  2.1151,  1.9141,  0.6852,  1.7448, -3.3070,
         -1.7612, -2.7180],
        [-2.6933,  7.9059,  1.6827, -4.7425, -2.9102, -1.9050, -8.8620, -7.8328,
          4.6223,  0.0606],
        [-0.0109,  1.7202,  0.8303, -2.7019, -0.4856, -0.3369, -5.2282, -2.6615,
          0.9764,  0.6148],
        [-0.2666, -0.4568,  3.1439, -2.5925,  3.1401,  0.4164, -5.6014, -0.8826,
         -1.3740, -2.4209]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat   car   car  bird
Accuracy of the network on the 4000.0 test images: 60 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': True, 'load_from_memory': False, 'pretrain': True, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=34, momentum=0, normalise=True, optimizer_choice=1, peregrine=True, pretrain=True, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 frog  frog  bird plane
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model wide_resnet50_2 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model wide_resnet50_2 Reshaped
Number of parameters: 66854730 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582669480.1046915
[1,   500] loss: 2.215087
12.606019258499146
[1,  1000] loss: 2.159277
25.111785650253296
[1,  1500] loss: 2.129179
37.601847410202026
[1,  2000] loss: 2.060917
50.006614208221436
[1,  2500] loss: 1.997755
62.4795982837677
[1,  3000] loss: 1.983993
74.88230323791504
[1,  3500] loss: 1.951969
87.37475872039795
[1,  4000] loss: 1.953550
99.85542702674866
[1,  4500] loss: 1.925613
112.292809009552
[1,  5000] loss: 1.895619
124.70288348197937
[1,  5500] loss: 1.873194
137.12949109077454
[1,  6000] loss: 1.859543
149.70321559906006
[1,  6500] loss: 1.844868
162.33650279045105
[1,  7000] loss: 1.775030
174.83883690834045
[1,  7500] loss: 1.730240
187.3467743396759
[1,  8000] loss: 1.800024
199.7925443649292
[1,  8500] loss: 1.785740
212.34445214271545
[1,  9000] loss: 1.750796
224.8604052066803
[1,  9500] loss: 1.700188
237.47468972206116
[1, 10000] loss: 1.685225
250.0628538131714
[1, 10500] loss: 1.723709
262.52609515190125
[1, 11000] loss: 1.691598
275.1571123600006
[1, 11500] loss: 1.668397
287.52668046951294
[1, 12000] loss: 1.669812
300.0677983760834
[1, 12500] loss: 1.599126
312.5597321987152
Epoch [1] loss: 5827277.840939
[2,   500] loss: 1.620995
325.3340833187103
[2,  1000] loss: 1.656208
337.9057593345642
[2,  1500] loss: 1.584240
350.4455397129059
[2,  2000] loss: 1.566824
362.87987780570984
[2,  2500] loss: 1.568932
375.2457230091095
[2,  3000] loss: 1.557061
387.5483374595642
[2,  3500] loss: 1.484393
400.1376931667328
[2,  4000] loss: 1.526994
412.58681750297546
[2,  4500] loss: 1.534897
425.02499198913574
[2,  5000] loss: 1.508743
437.4787881374359
[2,  5500] loss: 1.453133
450.05109167099
[2,  6000] loss: 1.459187
462.6890308856964
[2,  6500] loss: 1.449523
475.2344810962677
[2,  7000] loss: 1.443177
487.70304918289185
[2,  7500] loss: 1.424185
500.1676297187805
[2,  8000] loss: 1.421326
512.6011946201324
[2,  8500] loss: 1.467327
525.0587558746338
[2,  9000] loss: 1.382815
537.4406912326813
[2,  9500] loss: 1.365575
549.7262234687805
[2, 10000] loss: 1.413162
562.2194917201996
[2, 10500] loss: 1.357738
574.6782147884369
[2, 11000] loss: 1.368187
587.1374728679657
[2, 11500] loss: 1.351886
599.4779899120331
[2, 12000] loss: 1.354019
611.8905963897705
[2, 12500] loss: 1.365577
624.2636909484863
Epoch [2] loss: 4597974.103700
[3,   500] loss: 1.298202
636.83202958107
[3,  1000] loss: 1.317541
649.2427825927734
[3,  1500] loss: 1.268210
661.6029386520386
[3,  2000] loss: 1.251642
673.9265923500061
[3,  2500] loss: 1.240996
686.4655165672302
[3,  3000] loss: 1.231358
699.0722417831421
[3,  3500] loss: 1.253787
711.5005261898041
[3,  4000] loss: 1.267380
724.11279463768
[3,  4500] loss: 1.250972
736.5790045261383
[3,  5000] loss: 1.269109
749.0140354633331
[3,  5500] loss: 1.209040
761.4856352806091
[3,  6000] loss: 1.217260
773.924084186554
[3,  6500] loss: 1.206225
786.3069729804993
[3,  7000] loss: 1.230271
798.8452618122101
[3,  7500] loss: 1.171564
811.2664129734039
[3,  8000] loss: 1.216795
823.6762204170227
[3,  8500] loss: 1.178459
836.1080300807953
[3,  9000] loss: 1.154448
848.5223243236542
[3,  9500] loss: 1.101399
860.921546459198
[3, 10000] loss: 1.153744
873.3925545215607
[3, 10500] loss: 1.184238
885.8403251171112
[3, 11000] loss: 1.142368
898.5359883308411
[3, 11500] loss: 1.104649
910.9878814220428
[3, 12000] loss: 1.160481
923.3921821117401
[3, 12500] loss: 1.133933
936.0246465206146
Epoch [3] loss: 3796811.423383
[4,   500] loss: 1.076187
948.7429745197296
[4,  1000] loss: 1.080255
961.3044798374176
[4,  1500] loss: 1.073159
973.7384006977081
[4,  2000] loss: 1.078660
986.126302242279
[4,  2500] loss: 1.071066
998.5750184059143
[4,  3000] loss: 1.096710
1010.8966081142426
[4,  3500] loss: 1.037999
1023.2931289672852
[4,  4000] loss: 1.079769
1035.5434226989746
[4,  4500] loss: 1.017337
1047.8313913345337
[4,  5000] loss: 1.078419
1060.2464694976807
[4,  5500] loss: 1.053068
1072.6766967773438
[4,  6000] loss: 1.040581
1085.0609431266785
[4,  6500] loss: 1.017789
1097.4189648628235
[4,  7000] loss: 1.075626
1109.9318587779999
[4,  7500] loss: 0.997663
1122.3749718666077
[4,  8000] loss: 1.039835
1134.8325283527374
[4,  8500] loss: 0.976077
1147.3492028713226
[4,  9000] loss: 1.063705
1159.9194934368134
[4,  9500] loss: 1.051880
1172.4943640232086
[4, 10000] loss: 1.044897
1184.9808502197266
[4, 10500] loss: 1.035710
1197.3896458148956
[4, 11000] loss: 1.038147
1209.7746813297272
[4, 11500] loss: 1.005034
1222.2595796585083
[4, 12000] loss: 0.960637
1234.6597588062286
[4, 12500] loss: 0.967382
1247.0538165569305
Epoch [4] loss: 3268988.795017
[5,   500] loss: 0.968617
1259.5472462177277
[5,  1000] loss: 0.898589
1271.8640086650848
[5,  1500] loss: 0.917248
1284.2056369781494
[5,  2000] loss: 0.925495
1296.5370531082153
[5,  2500] loss: 0.956977
1308.9188573360443
[5,  3000] loss: 0.940187
1321.3431231975555
[5,  3500] loss: 0.962985
1333.6913924217224
[5,  4000] loss: 0.943453
1346.115389585495
[5,  4500] loss: 0.971756
1358.6208877563477
[5,  5000] loss: 0.953202
1371.1814939975739
[5,  5500] loss: 0.913259
1383.8162467479706
[5,  6000] loss: 0.869316
1396.3401412963867
[5,  6500] loss: 0.846920
1408.763701915741
[5,  7000] loss: 0.876119
1421.149185180664
[5,  7500] loss: 0.927512
1433.5715036392212
[5,  8000] loss: 0.932410
1445.9889590740204
[5,  8500] loss: 0.931786
1458.411907672882
[5,  9000] loss: 0.890117
1470.8230969905853
[5,  9500] loss: 0.926558
1483.190173625946
[5, 10000] loss: 0.939940
1495.6322634220123
[5, 10500] loss: 0.903722
1508.0370836257935
[5, 11000] loss: 0.848133
1520.4651992321014
[5, 11500] loss: 0.898990
1533.1315925121307
[5, 12000] loss: 0.900191
1545.7233140468597
[5, 12500] loss: 0.922730
1558.2293791770935
Epoch [5] loss: 2880549.429891
[6,   500] loss: 0.859117
1570.7801518440247
[6,  1000] loss: 0.820541
1583.1665816307068
[6,  1500] loss: 0.812257
1595.7458698749542
[6,  2000] loss: 0.830979
1608.167737007141
[6,  2500] loss: 0.800863
1620.5467238426208
[6,  3000] loss: 0.819571
1632.9551875591278
[6,  3500] loss: 0.807885
1645.295431137085
[6,  4000] loss: 0.853784
1657.566742181778
[6,  4500] loss: 0.852980
1669.9483978748322
[6,  5000] loss: 0.827050
1682.2721722126007
[6,  5500] loss: 0.858057
1694.6598138809204
[6,  6000] loss: 0.855280
1707.3505682945251
[6,  6500] loss: 0.871938
1719.6258628368378
[6,  7000] loss: 0.799385
1732.0051944255829
[6,  7500] loss: 0.801880
1744.425743341446
[6,  8000] loss: 0.826902
1756.8611035346985
[6,  8500] loss: 0.823863
1769.2859582901
[6,  9000] loss: 0.811213
1781.657535791397
[6,  9500] loss: 0.805170
1794.0694222450256
[6, 10000] loss: 0.793550
1806.49618601799
[6, 10500] loss: 0.780290
1818.8545203208923
[6, 11000] loss: 0.788273
1831.4494466781616
[6, 11500] loss: 0.805366
1843.918954372406
[6, 12000] loss: 0.797161
1856.6060819625854
[6, 12500] loss: 0.798032
1869.0312161445618
Epoch [6] loss: 2573006.297297
[7,   500] loss: 0.752362
1881.5564410686493
[7,  1000] loss: 0.774962
1893.9522304534912
[7,  1500] loss: 0.719528
1906.3134725093842
[7,  2000] loss: 0.733040
1918.6889290809631
[7,  2500] loss: 0.706132
1931.114905834198
[7,  3000] loss: 0.807508
1943.5122861862183
[7,  3500] loss: 0.752879
1955.868482351303
[7,  4000] loss: 0.679720
1968.267652273178
[7,  4500] loss: 0.738114
1980.7900006771088
[7,  5000] loss: 0.739768
1993.298581123352
[7,  5500] loss: 0.720342
2005.6576011180878
[7,  6000] loss: 0.709624
2018.1759946346283
[7,  6500] loss: 0.745482
2030.6558191776276
[7,  7000] loss: 0.740869
2043.008940935135
[7,  7500] loss: 0.713513
2055.407141685486
[7,  8000] loss: 0.697466
2067.856561899185
[7,  8500] loss: 0.734224
2080.170188188553
[7,  9000] loss: 0.747325
2092.565401315689
[7,  9500] loss: 0.741149
2104.9384937286377
[7, 10000] loss: 0.697314
2117.3724236488342
[7, 10500] loss: 0.700728
2130.1758041381836
[7, 11000] loss: 0.758284
2142.8016397953033
[7, 11500] loss: 0.709227
2155.2470965385437
[7, 12000] loss: 0.691075
2167.7403061389923
[7, 12500] loss: 0.716245
2180.0589559078217
Epoch [7] loss: 2292973.876126
[8,   500] loss: 0.628034
2192.6456427574158
[8,  1000] loss: 0.696546
2205.087952852249
[8,  1500] loss: 0.647804
2217.508514881134
[8,  2000] loss: 0.659929
2230.0292615890503
[8,  2500] loss: 0.704165
2242.5498492717743
[8,  3000] loss: 0.659647
2255.0318155288696
[8,  3500] loss: 0.702055
2267.3300487995148
[8,  4000] loss: 0.683787
2279.6881840229034
[8,  4500] loss: 0.688887
2292.116670370102
[8,  5000] loss: 0.624409
2304.416350841522
[8,  5500] loss: 0.682739
2316.8220043182373
[8,  6000] loss: 0.696390
2329.206635951996
[8,  6500] loss: 0.669149
2341.6745405197144
[8,  7000] loss: 0.621155
2354.0647296905518
[8,  7500] loss: 0.607541
2366.360164642334
[8,  8000] loss: 0.652556
2378.755835533142
[8,  8500] loss: 0.646988
2391.041852712631
[8,  9000] loss: 0.671538
2403.554148197174
[8,  9500] loss: 0.673866
2416.0552654266357
[8, 10000] loss: 0.644653
2428.7180116176605
[8, 10500] loss: 0.637995
2441.2377076148987
[8, 11000] loss: 0.668856
2453.648593187332
[8, 11500] loss: 0.693833
2466.241237640381
[8, 12000] loss: 0.669737
2478.739140033722
[8, 12500] loss: 0.635671
2491.1833579540253
Epoch [8] loss: 2074788.131970
[9,   500] loss: 0.584936
2503.781997203827
[9,  1000] loss: 0.561427
2516.227201461792
[9,  1500] loss: 0.562537
2528.7098882198334
[9,  2000] loss: 0.582383
2541.2185821533203
[9,  2500] loss: 0.552385
2553.653525829315
[9,  3000] loss: 0.596622
2566.1540627479553
[9,  3500] loss: 0.579607
2578.559850215912
[9,  4000] loss: 0.558738
2591.0472662448883
[9,  4500] loss: 0.634826
2603.533892393112
[9,  5000] loss: 0.579581
2616.043018102646
[9,  5500] loss: 0.591620
2628.4660363197327
[9,  6000] loss: 0.666870
2640.9271264076233
[9,  6500] loss: 0.619993
2653.3750076293945
[9,  7000] loss: 0.578026
2665.8010149002075
[9,  7500] loss: 0.569427
2678.2204117774963
[9,  8000] loss: 0.597113
2690.5389308929443
[9,  8500] loss: 0.565696
2703.003687143326
[9,  9000] loss: 0.614729
2715.4135386943817
[9,  9500] loss: 0.621809
2727.8184974193573
[9, 10000] loss: 0.585360
2740.193693637848
[9, 10500] loss: 0.578955
2752.5829050540924
[9, 11000] loss: 0.617910
2765.0192670822144
[9, 11500] loss: 0.607815
2777.4625215530396
[9, 12000] loss: 0.619985
2789.993189573288
[9, 12500] loss: 0.576300
2802.4948256015778
Epoch [9] loss: 1860201.830875
[10,   500] loss: 0.523607
2815.0778896808624
[10,  1000] loss: 0.481051
2827.509773015976
[10,  1500] loss: 0.530399
2839.8354523181915
[10,  2000] loss: 0.535745
2852.2056772708893
[10,  2500] loss: 0.533687
2864.597335577011
[10,  3000] loss: 0.545666
2876.981256723404
[10,  3500] loss: 0.537220
2889.3665432929993
[10,  4000] loss: 0.547270
2901.688618659973
[10,  4500] loss: 0.548131
2914.092870950699
[10,  5000] loss: 0.542577
2926.4756832122803
[10,  5500] loss: 0.552302
2938.9121096134186
[10,  6000] loss: 0.539054
2951.4227554798126
[10,  6500] loss: 0.511514
2963.804536819458
[10,  7000] loss: 0.530374
2976.1612689495087
[10,  7500] loss: 0.499920
2988.488048553467
[10,  8000] loss: 0.557038
3000.908284664154
[10,  8500] loss: 0.533723
3013.3992733955383
[10,  9000] loss: 0.573152
3025.840550661087
[10,  9500] loss: 0.557775
3038.21533036232
[10, 10000] loss: 0.538582
3050.5750443935394
[10, 10500] loss: 0.542039
3062.9482436180115
[10, 11000] loss: 0.514290
3075.3213720321655
[10, 11500] loss: 0.560802
3087.8112585544586
[10, 12000] loss: 0.503430
3100.1883022785187
[10, 12500] loss: 0.559374
3112.564877271652
Epoch [10] loss: 1667037.616560
[11,   500] loss: 0.480573
3125.130112171173
[11,  1000] loss: 0.444262
3137.4521350860596
[11,  1500] loss: 0.455674
3149.838710784912
[11,  2000] loss: 0.498834
3162.1098017692566
[11,  2500] loss: 0.423280
3174.5007753372192
[11,  3000] loss: 0.525070
3186.926464319229
[11,  3500] loss: 0.509001
3199.328834295273
[11,  4000] loss: 0.475147
3211.856870651245
[11,  4500] loss: 0.528984
3224.3389959335327
[11,  5000] loss: 0.477869
3236.938928604126
[11,  5500] loss: 0.497115
3249.3433105945587
[11,  6000] loss: 0.477105
3261.835038661957
[11,  6500] loss: 0.483807
3274.2136812210083
[11,  7000] loss: 0.484498
3286.5856754779816
[11,  7500] loss: 0.510036
3299.006134033203
[11,  8000] loss: 0.504661
3311.389546394348
[11,  8500] loss: 0.483234
3323.8325741291046
[11,  9000] loss: 0.510458
3336.404151201248
[11,  9500] loss: 0.513416
3348.931901693344
[11, 10000] loss: 0.480774
3361.397605419159
[11, 10500] loss: 0.512204
3373.6945548057556
[11, 11000] loss: 0.476246
3386.0587832927704
[11, 11500] loss: 0.434619
3398.4108576774597
[11, 12000] loss: 0.451562
3410.8953619003296
[11, 12500] loss: 0.511764
3423.4050085544586
Epoch [11] loss: 1519377.969867
[12,   500] loss: 0.399074
3435.8955664634705
[12,  1000] loss: 0.416118
3448.345878839493
[12,  1500] loss: 0.422792
3460.656226873398
[12,  2000] loss: 0.416172
3473.0519540309906
[12,  2500] loss: 0.395986
3485.456523180008
[12,  3000] loss: 0.445445
3497.820078611374
[12,  3500] loss: 0.482271
3510.2420947551727
[12,  4000] loss: 0.472738
3522.662689447403
[12,  4500] loss: 0.466454
3535.0906126499176
[12,  5000] loss: 0.421756
3547.6866431236267
[12,  5500] loss: 0.455891
3560.185527086258
[12,  6000] loss: 0.419047
3572.7639541625977
[12,  6500] loss: 0.474060
3585.24658370018
[12,  7000] loss: 0.475008
3597.666944026947
[12,  7500] loss: 0.414615
3610.0529301166534
[12,  8000] loss: 0.439489
3622.3452899456024
[12,  8500] loss: 0.459359
3634.776752471924
[12,  9000] loss: 0.408258
3647.1359601020813
[12,  9500] loss: 0.447538
3659.707506418228
[12, 10000] loss: 0.460108
3672.1995174884796
[12, 10500] loss: 0.418666
3684.5481123924255
[12, 11000] loss: 0.448360
3696.940120458603
[12, 11500] loss: 0.465223
3709.3204510211945
[12, 12000] loss: 0.427407
3721.7165830135345
[12, 12500] loss: 0.451818
3734.465976715088
Epoch [12] loss: 1390770.749489
[13,   500] loss: 0.374042
3747.043694257736
[13,  1000] loss: 0.379723
3759.3528287410736
[13,  1500] loss: 0.383966
3771.723189353943
[13,  2000] loss: 0.391925
3784.1010150909424
[13,  2500] loss: 0.395292
3796.4396727085114
[13,  3000] loss: 0.415835
3808.7711997032166
[13,  3500] loss: 0.389445
3821.1880960464478
[13,  4000] loss: 0.430237
3833.593658924103
[13,  4500] loss: 0.402430
3846.000479698181
[13,  5000] loss: 0.399593
3858.487404823303
[13,  5500] loss: 0.412362
3871.281954050064
[13,  6000] loss: 0.399057
3891.0230870246887
[13,  6500] loss: 0.350365
3903.6058473587036
[13,  7000] loss: 0.414455
3916.245162010193
[13,  7500] loss: 0.379908
3928.912968158722
[13,  8000] loss: 0.399355
3941.375180721283
[13,  8500] loss: 0.386318
3953.8938171863556
[13,  9000] loss: 0.408114
3966.3763496875763
[13,  9500] loss: 0.394911
3978.740077972412
[13, 10000] loss: 0.452271
3991.2283687591553
[13, 10500] loss: 0.388768
4003.6255877017975
[13, 11000] loss: 0.434739
4015.9983315467834
[13, 11500] loss: 0.406501
4028.322516679764
[13, 12000] loss: 0.419331
4040.7572100162506
[13, 12500] loss: 0.421734
4053.5152411460876
Epoch [13] loss: 1252643.689647
[14,   500] loss: 0.357657
4068.3561742305756
[14,  1000] loss: 0.376332
4080.889608860016
[14,  1500] loss: 0.334447
4093.4073061943054
[14,  2000] loss: 0.352110
4105.847963094711
[14,  2500] loss: 0.361473
4118.395894050598
[14,  3000] loss: 0.378982
4130.921879768372
[14,  3500] loss: 0.374040
4143.42666721344
[14,  4000] loss: 0.381546
4155.956070899963
[14,  4500] loss: 0.389202
4168.439985513687
[14,  5000] loss: 0.340195
4181.016873836517
[14,  5500] loss: 0.334266
4193.51059794426
[14,  6000] loss: 0.366305
4206.025349378586
[14,  6500] loss: 0.379442
4218.396013259888
[14,  7000] loss: 0.372048
4230.938467502594
[14,  7500] loss: 0.392851
4243.360423326492
[14,  8000] loss: 0.350818
4255.835098266602
[14,  8500] loss: 0.371794
4268.215598106384
[14,  9000] loss: 0.386019
4280.644233465195
[14,  9500] loss: 0.391750
4293.180047512054
[14, 10000] loss: 0.389499
4305.616625547409
[14, 10500] loss: 0.415529
4318.118802785873
[14, 11000] loss: 0.387578
4330.603074550629
[14, 11500] loss: 0.389592
4343.068078994751
[14, 12000] loss: 0.355753
4355.536755800247
[14, 12500] loss: 0.384374
4367.999773740768
Epoch [14] loss: 1174979.431155
[15,   500] loss: 0.318960
4380.663773059845
[15,  1000] loss: 0.314665
4393.178605556488
[15,  1500] loss: 0.293215
4405.436822652817
[15,  2000] loss: 0.354140
4417.81113243103
[15,  2500] loss: 0.353456
4430.36456489563
[15,  3000] loss: 0.321354
4442.79526591301
[15,  3500] loss: 0.342911
4455.198951005936
[15,  4000] loss: 0.297149
4467.644926786423
[15,  4500] loss: 0.371773
4480.165269613266
[15,  5000] loss: 0.315583
4492.7153997421265
[15,  5500] loss: 0.333911
4505.320019006729
[15,  6000] loss: 0.346036
4517.709843635559
[15,  6500] loss: 0.342694
4530.352992534637
[15,  7000] loss: 0.377846
4543.02121424675
[15,  7500] loss: 0.344051
4555.623480558395
[15,  8000] loss: 0.315864
4568.285210847855
[15,  8500] loss: 0.375383
4580.821628570557
[15,  9000] loss: 0.379477
4593.353549957275
[15,  9500] loss: 0.364469
4605.602938890457
[15, 10000] loss: 0.362665
4617.979149580002
[15, 10500] loss: 0.356862
4630.428729057312
[15, 11000] loss: 0.337741
4642.86367893219
[15, 11500] loss: 0.318439
4655.2908046245575
[15, 12000] loss: 0.372688
4667.781413078308
[15, 12500] loss: 0.350276
4680.295514822006
Epoch [15] loss: 1063954.153858
[16,   500] loss: 0.294486
4693.052042961121
[16,  1000] loss: 0.258384
4705.511408090591
[16,  1500] loss: 0.242335
4717.991958141327
[16,  2000] loss: 0.264906
4730.439073085785
[16,  2500] loss: 0.300485
4742.886237382889
[16,  3000] loss: 0.261098
4755.335079908371
[16,  3500] loss: 0.336040
4767.727049589157
[16,  4000] loss: 0.333401
4780.192147731781
[16,  4500] loss: 0.260527
4792.61110830307
[16,  5000] loss: 0.293150
4804.869815349579
[16,  5500] loss: 0.277897
4817.220114946365
[16,  6000] loss: 0.283117
4829.5856757164
[16,  6500] loss: 0.290179
4841.889560222626
[16,  7000] loss: 0.312718
4854.278025150299
[16,  7500] loss: 0.309695
4866.620354652405
[16,  8000] loss: 0.299611
4879.035229921341
[16,  8500] loss: 0.301636
4891.445101737976
[16,  9000] loss: 0.319025
4903.978370189667
[16,  9500] loss: 0.281097
4916.47978925705
[16, 10000] loss: 0.283876
4928.888222694397
[16, 10500] loss: 0.286303
4941.293592453003
[16, 11000] loss: 0.286204
4953.68733048439
[16, 11500] loss: 0.333708
4966.0170793533325
[16, 12000] loss: 0.297148
4978.495358705521
[16, 12500] loss: 0.321606
4990.888913393021
Epoch [16] loss: 911340.458571
[17,   500] loss: 0.238091
5003.43462395668
[17,  1000] loss: 0.217377
5016.052053928375
[17,  1500] loss: 0.296917
5028.496800661087
[17,  2000] loss: 0.274870
5040.997165441513
[17,  2500] loss: 0.270001
5053.370667934418
[17,  3000] loss: 0.250767
5065.749515533447
[17,  3500] loss: 0.246231
5078.243230819702
[17,  4000] loss: 0.261403
5090.782546520233
[17,  4500] loss: 0.216822
5103.347779989243
[17,  5000] loss: 0.255326
5115.852593898773
[17,  5500] loss: 0.269024
5128.636090993881
[17,  6000] loss: 0.301658
5141.467652320862
[17,  6500] loss: 0.243328
5154.287131071091
[17,  7000] loss: 0.250037
5166.667738676071
[17,  7500] loss: 0.257221
5179.076376438141
[17,  8000] loss: 0.238232
5191.472936868668
[17,  8500] loss: 0.271929
5203.938601732254
[17,  9000] loss: 0.254750
5216.334235429764
[17,  9500] loss: 0.342224
5228.784126520157
[17, 10000] loss: 0.291424
5241.30791425705
[17, 10500] loss: 0.300676
5253.707614660263
[17, 11000] loss: 0.307619
5266.142415761948
[17, 11500] loss: 0.296813
5278.576371431351
[17, 12000] loss: 0.310389
5290.938934803009
[17, 12500] loss: 0.328909
5303.354598522186
Epoch [17] loss: 834906.464636
[18,   500] loss: 0.240538
5315.864106416702
[18,  1000] loss: 0.221789
5328.36190533638
[18,  1500] loss: 0.246484
5340.945855140686
[18,  2000] loss: 0.195680
5353.442514181137
[18,  2500] loss: 0.258066
5365.88156080246
[18,  3000] loss: 0.263548
5378.3184106349945
[18,  3500] loss: 0.244277
5390.756605148315
[18,  4000] loss: 0.260684
5403.184544801712
[18,  4500] loss: 0.236502
5415.590389251709
[18,  5000] loss: 0.266801
5428.039438486099
[18,  5500] loss: 0.236950
5440.413266658783
[18,  6000] loss: 0.227943
5452.819159269333
[18,  6500] loss: 0.280870
5465.196151018143
[18,  7000] loss: 0.279204
5477.633932590485
[18,  7500] loss: 0.257110
5490.06512594223
[18,  8000] loss: 0.234682
5502.535768032074
[18,  8500] loss: 0.247352
5514.97288274765
[18,  9000] loss: 0.267522
5527.7850778102875
[18,  9500] loss: 0.273449
5540.182225704193
[18, 10000] loss: 0.225854
5552.60745716095
[18, 10500] loss: 0.269254
5565.064055919647
[18, 11000] loss: 0.244435
5577.5538449287415
[18, 11500] loss: 0.268563
5590.144618272781
[18, 12000] loss: 0.242869
5602.59889125824
[18, 12500] loss: 0.255573
5615.098536729813
Epoch [18] loss: 780316.388712
[19,   500] loss: 0.214614
5627.630990743637
[19,  1000] loss: 0.191487
5639.984216213226
[19,  1500] loss: 0.204712
5652.527044534683
[19,  2000] loss: 0.231803
5665.060715675354
[19,  2500] loss: 0.197559
5677.43611240387
[19,  3000] loss: 0.236841
5689.786721467972
[19,  3500] loss: 0.166710
5702.173346042633
[19,  4000] loss: 0.204862
5714.605948925018
[19,  4500] loss: 0.260992
5727.375507354736
[19,  5000] loss: 0.245069
5740.102911233902
[19,  5500] loss: 0.213746
5752.890239953995
[19,  6000] loss: 0.212240
5765.644150018692
[19,  6500] loss: 0.224356
5778.245985984802
[19,  7000] loss: 0.262904
5790.850624322891
[19,  7500] loss: 0.243590
5803.4086718559265
[19,  8000] loss: 0.266175
5815.905392408371
[19,  8500] loss: 0.253934
5828.266361713409
[19,  9000] loss: 0.207582
5840.681549072266
[19,  9500] loss: 0.232265
5853.107037067413
[19, 10000] loss: 0.262648
5865.50582408905
[19, 10500] loss: 0.214244
5877.992383718491
[19, 11000] loss: 0.211329
5890.391813993454
[19, 11500] loss: 0.231400
5902.958665847778
[19, 12000] loss: 0.206487
5915.476180076599
[19, 12500] loss: 0.237768
5927.874124765396
Epoch [19] loss: 692567.301360
[20,   500] loss: 0.235642
5940.613533735275
[20,  1000] loss: 0.190050
5953.0988755226135
[20,  1500] loss: 0.176737
5965.589505672455
[20,  2000] loss: 0.205998
5978.246933698654
[20,  2500] loss: 0.177032
5990.705019950867
[20,  3000] loss: 0.192498
6003.157596111298
[20,  3500] loss: 0.188736
6015.602627277374
[20,  4000] loss: 0.190499
6028.086008548737
[20,  4500] loss: 0.197463
6040.414235830307
[20,  5000] loss: 0.208837
6052.818017959595
[20,  5500] loss: 0.209102
6065.261482477188
[20,  6000] loss: 0.197580
6077.700587749481
[20,  6500] loss: 0.213961
6090.15988278389
[20,  7000] loss: 0.249972
6102.6444664001465
[20,  7500] loss: 0.184888
6115.070879459381
[20,  8000] loss: 0.179798
6127.482759475708
[20,  8500] loss: 0.192801
6139.881203651428
[20,  9000] loss: 0.234336
6152.596398830414
[20,  9500] loss: 0.224376
6165.266243219376
[20, 10000] loss: 0.196869
6177.991987228394
[20, 10500] loss: 0.197680
6190.536581516266
[20, 11000] loss: 0.235045
6203.029849529266
[20, 11500] loss: 0.250388
6215.461874723434
[20, 12000] loss: 0.223504
6227.89172744751
[20, 12500] loss: 0.228240
6240.252954483032
Epoch [20] loss: 644072.271951
Finished Training
Saving model to /data/s4091221/trained-models/wide_resnet50_22020-02-26 01:08:40.414113
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[ 2.1272, -1.4489, -0.4887,  3.6962,  0.6368,  0.3404, -0.3791, -2.4230,
          0.6879, -1.8370],
        [ 1.6034,  1.4321, -2.3658,  0.7340, -0.9552,  0.2778, -2.3515, -2.4539,
          3.1159,  1.0496],
        [ 2.4604,  2.0962, -0.3248, -0.2685, -0.1667, -0.6954, -4.4488, -2.8642,
          4.4282,  0.4844],
        [ 2.2212, -0.4224, -0.6027,  0.3508,  0.9991, -0.1050, -2.5829, -0.5597,
          0.6171,  0.7662]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat  ship  ship plane
Accuracy of the network on the 4000.0 test images: 70 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=34, momentum=0.9, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
horse truck horse  ship
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model wide_resnet50_2 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model wide_resnet50_2 Reshaped
Number of parameters: 66854730 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0.9
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582675807.9258392
[1,   500] loss: 6.299857
14.636083602905273
[1,  1000] loss: 6.254936
28.783858060836792
[1,  1500] loss: 5.844868
42.87012314796448
[1,  2000] loss: 5.874697
56.95069456100464
[1,  2500] loss: 5.712161
71.10400867462158
[1,  3000] loss: 5.344429
85.09127283096313
[1,  3500] loss: 5.048914
99.31502985954285
[1,  4000] loss: 4.699404
113.3058111667633
[1,  4500] loss: 4.855092
127.3223671913147
[1,  5000] loss: 4.376641
141.4120647907257
[1,  5500] loss: 4.291752
155.45224595069885
[1,  6000] loss: 3.966031
169.40140104293823
[1,  6500] loss: 3.860155
183.24118304252625
[1,  7000] loss: 3.989418
197.17959928512573
[1,  7500] loss: 3.623809
211.1879541873932
[1,  8000] loss: 3.656018
225.10455226898193
[1,  8500] loss: 3.530707
239.1380078792572
[1,  9000] loss: 3.447608
253.01671409606934
[1,  9500] loss: 3.421243
266.91104435920715
[1, 10000] loss: 3.469647
281.0523986816406
[1, 10500] loss: 3.322598
295.4331018924713
[1, 11000] loss: 3.278145
309.5204610824585
[1, 11500] loss: 3.212471
323.63079619407654
[1, 12000] loss: 3.109902
337.4721040725708
[1, 12500] loss: 3.119370
351.462571144104
Epoch [1] loss: 13572197.171409
[2,   500] loss: 3.187844
365.66349172592163
[2,  1000] loss: 3.037505
379.6779787540436
[2,  1500] loss: 3.027939
393.75045108795166
[2,  2000] loss: 3.106648
407.58647322654724
[2,  2500] loss: 3.089058
421.4717116355896
[2,  3000] loss: 2.995403
435.47870445251465
[2,  3500] loss: 3.001681
449.5689640045166
[2,  4000] loss: 3.003267
463.63787055015564
[2,  4500] loss: 2.886751
477.8790204524994
[2,  5000] loss: 2.774238
491.85043239593506
[2,  5500] loss: 2.891942
505.8555519580841
[2,  6000] loss: 2.898812
519.8983881473541
[2,  6500] loss: 2.830154
533.9688720703125
[2,  7000] loss: 2.770719
548.0531687736511
[2,  7500] loss: 2.788576
562.0430686473846
[2,  8000] loss: 2.728972
575.9639792442322
[2,  8500] loss: 2.703372
590.0150814056396
[2,  9000] loss: 2.697211
604.4276144504547
[2,  9500] loss: 2.701160
618.9155418872833
[2, 10000] loss: 2.661654
633.449375629425
[2, 10500] loss: 2.675395
647.3872585296631
[2, 11000] loss: 2.649180
661.445944070816
[2, 11500] loss: 2.596957
675.3992507457733
[2, 12000] loss: 2.589136
689.1678078174591
[2, 12500] loss: 2.641048
703.0528223514557
Epoch [2] loss: 8903797.926296
[3,   500] loss: 2.622674
716.9370810985565
[3,  1000] loss: 2.596702
730.811984539032
[3,  1500] loss: 2.526707
744.9362032413483
[3,  2000] loss: 2.532952
758.9849209785461
[3,  2500] loss: 2.504644
773.0006945133209
[3,  3000] loss: 2.534195
786.943008184433
[3,  3500] loss: 2.456132
801.128091096878
[3,  4000] loss: 2.529665
815.1984870433807
[3,  4500] loss: 2.521953
829.0559296607971
[3,  5000] loss: 2.404860
843.1382868289948
[3,  5500] loss: 2.452157
857.2231130599976
[3,  6000] loss: 2.468617
871.24680519104
[3,  6500] loss: 2.521811
885.2373313903809
[3,  7000] loss: 2.417557
899.2481338977814
[3,  7500] loss: 2.404162
913.3249485492706
[3,  8000] loss: 2.421379
927.3904132843018
[3,  8500] loss: 2.427234
941.8662869930267
[3,  9000] loss: 2.395770
956.110940694809
[3,  9500] loss: 2.418219
970.2849941253662
[3, 10000] loss: 2.381562
984.43368268013
[3, 10500] loss: 2.404353
998.5043773651123
[3, 11000] loss: 2.370699
1012.4728684425354
[3, 11500] loss: 2.367964
1026.6667838096619
[3, 12000] loss: 2.349674
1040.5840919017792
[3, 12500] loss: 2.350808
1054.7265455722809
Epoch [3] loss: 7691904.657804
[4,   500] loss: 2.511690
1068.7615180015564
[4,  1000] loss: 2.380351
1082.7380113601685
[4,  1500] loss: 2.383448
1096.7652008533478
[4,  2000] loss: 2.397955
1110.5508584976196
[4,  2500] loss: 2.371459
1124.5517146587372
[4,  3000] loss: 2.315722
1138.5705802440643
[4,  3500] loss: 2.307147
1152.6511952877045
[4,  4000] loss: 2.355743
1166.5160298347473
[4,  4500] loss: 2.324853
1180.5906426906586
[4,  5000] loss: 2.389116
1194.8188035488129
[4,  5500] loss: 2.337537
1208.936451435089
[4,  6000] loss: 2.345641
1223.2749872207642
[4,  6500] loss: 2.490001
1237.187804698944
[4,  7000] loss: 2.406411
1251.3503518104553
[4,  7500] loss: 2.401561
1265.6530947685242
[4,  8000] loss: 2.350759
1279.8426406383514
[4,  8500] loss: 2.343333
1294.0026795864105
[4,  9000] loss: 2.305446
1308.169023990631
[4,  9500] loss: 2.274408
1322.5495817661285
[4, 10000] loss: 2.286483
1336.629157781601
[4, 10500] loss: 2.275189
1350.5332503318787
[4, 11000] loss: 2.270799
1364.4953155517578
[4, 11500] loss: 2.267593
1378.5359835624695
[4, 12000] loss: 2.242441
1392.5695917606354
[4, 12500] loss: 2.247758
1406.5462067127228
Epoch [4] loss: 7347062.400655
[5,   500] loss: 2.294840
1420.6726138591766
[5,  1000] loss: 2.254339
1434.649646282196
[5,  1500] loss: 2.252038
1448.6580936908722
[5,  2000] loss: 2.268250
1462.6337871551514
[5,  2500] loss: 2.233122
1476.5883433818817
[5,  3000] loss: 2.189062
1490.7769753932953
[5,  3500] loss: 2.245198
1505.0098595619202
[5,  4000] loss: 2.232817
1518.9918406009674
[5,  4500] loss: 2.198547
1533.0385904312134
[5,  5000] loss: 2.202615
1546.9030578136444
[5,  5500] loss: 2.169063
1560.9499816894531
[5,  6000] loss: 2.176927
1574.9719052314758
[5,  6500] loss: 2.165245
1588.9379591941833
[5,  7000] loss: 2.173968
1602.8591842651367
[5,  7500] loss: 2.147543
1616.7226617336273
[5,  8000] loss: 2.147025
1630.6162843704224
[5,  8500] loss: 2.137043
1644.8357841968536
[5,  9000] loss: 2.173531
1658.8097643852234
[5,  9500] loss: 2.149937
1672.9039678573608
[5, 10000] loss: 2.132299
1686.8590016365051
[5, 10500] loss: 2.122472
1700.8579785823822
[5, 11000] loss: 2.135972
1714.7779378890991
[5, 11500] loss: 2.135368
1728.7092249393463
[5, 12000] loss: 2.104202
1742.7051892280579
[5, 12500] loss: 2.101103
1756.8638298511505
Epoch [5] loss: 6832167.263277
[6,   500] loss: 2.127396
1771.2291190624237
[6,  1000] loss: 2.151207
1785.455484867096
[6,  1500] loss: 2.107311
1799.972519159317
[6,  2000] loss: 2.097278
1814.0923244953156
[6,  2500] loss: 2.132544
1827.8669197559357
[6,  3000] loss: 2.082469
1841.5629172325134
[6,  3500] loss: 2.098039
1855.53777384758
[6,  4000] loss: 2.065922
1869.364956855774
[6,  4500] loss: 2.090187
1883.430150270462
[6,  5000] loss: 2.056237
1897.5106437206268
[6,  5500] loss: 2.076367
1911.5246131420135
[6,  6000] loss: 2.069672
1925.5718355178833
[6,  6500] loss: 2.027718
1939.528179883957
[6,  7000] loss: 2.083535
1953.421536207199
[6,  7500] loss: 2.086910
1967.2943692207336
[6,  8000] loss: 2.048275
1981.2833695411682
[6,  8500] loss: 2.040527
1995.2016344070435
[6,  9000] loss: 2.034430
2009.1525394916534
[6,  9500] loss: 2.016611
2023.1320896148682
[6, 10000] loss: 2.049667
2036.964334487915
[6, 10500] loss: 2.043019
2050.8453714847565
[6, 11000] loss: 2.027381
2064.826122045517
[6, 11500] loss: 2.046943
2078.864750146866
[6, 12000] loss: 2.088938
2092.841620206833
[6, 12500] loss: 2.073385
2106.756437063217
Epoch [6] loss: 6485944.730688
[7,   500] loss: 2.050177
2120.735004425049
[7,  1000] loss: 2.056539
2134.9050772190094
[7,  1500] loss: 2.045157
2148.860098361969
[7,  2000] loss: 2.003207
2163.014883041382
[7,  2500] loss: 2.020293
2177.0059027671814
[7,  3000] loss: 2.002061
2191.1118795871735
[7,  3500] loss: 2.024384
2205.306067943573
[7,  4000] loss: 1.981509
2219.5128021240234
[7,  4500] loss: 1.996824
2233.5654513835907
[7,  5000] loss: 2.004250
2247.447906970978
[7,  5500] loss: 1.995207
2261.51570892334
[7,  6000] loss: 1.992711
2275.46311879158
[7,  6500] loss: 1.974402
2289.5333309173584
[7,  7000] loss: 1.979054
2303.5530643463135
[7,  7500] loss: 1.990761
2317.3586864471436
[7,  8000] loss: 1.977350
2331.2106320858
[7,  8500] loss: 1.974874
2345.146196126938
[7,  9000] loss: 1.933326
2359.1310448646545
[7,  9500] loss: 1.952623
2373.139935731888
[7, 10000] loss: 1.975297
2387.059304714203
[7, 10500] loss: 1.915339
2401.2115755081177
[7, 11000] loss: 1.938649
2415.1593737602234
[7, 11500] loss: 1.947429
2428.8474457263947
[7, 12000] loss: 1.973363
2442.657767534256
[7, 12500] loss: 1.935244
2456.576012134552
Epoch [7] loss: 6229285.416433
[8,   500] loss: 1.911810
2470.7687826156616
[8,  1000] loss: 1.912089
2484.914758205414
[8,  1500] loss: 1.945673
2499.0315477848053
[8,  2000] loss: 1.904504
2513.1233830451965
[8,  2500] loss: 1.928137
2527.087963104248
[8,  3000] loss: 1.924367
2541.259756088257
[8,  3500] loss: 1.887685
2555.5545632839203
[8,  4000] loss: 1.924275
2569.4616599082947
[8,  4500] loss: 1.867950
2583.5025470256805
[8,  5000] loss: 1.893255
2597.60106921196
[8,  5500] loss: 1.898500
2611.7818706035614
[8,  6000] loss: 1.887371
2625.8256158828735
[8,  6500] loss: 1.908907
2639.8958928585052
[8,  7000] loss: 1.928610
2654.0831546783447
[8,  7500] loss: 1.923553
2668.159456729889
[8,  8000] loss: 1.892368
2682.0265691280365
[8,  8500] loss: 1.899721
2695.995564699173
[8,  9000] loss: 1.872782
2709.9772787094116
[8,  9500] loss: 1.863587
2724.064232110977
[8, 10000] loss: 1.863464
2738.1559450626373
[8, 10500] loss: 1.912169
2752.278406381607
[8, 11000] loss: 1.916990
2766.274304628372
[8, 11500] loss: 1.876284
2780.438134431839
[8, 12000] loss: 1.905725
2794.3214676380157
[8, 12500] loss: 1.920449
2808.4176790714264
Epoch [8] loss: 5967035.884643
[9,   500] loss: 1.900707
2822.6405386924744
[9,  1000] loss: 1.878830
2836.721873521805
[9,  1500] loss: 1.919300
2850.750864505768
[9,  2000] loss: 1.908314
2864.8605711460114
[9,  2500] loss: 1.861849
2879.125452041626
[9,  3000] loss: 1.911270
2892.956736803055
[9,  3500] loss: 1.914358
2907.018461704254
[9,  4000] loss: 1.860408
2921.1861941814423
[9,  4500] loss: 1.896234
2935.233954668045
[9,  5000] loss: 1.890780
2949.367294073105
[9,  5500] loss: 1.867063
2963.2574665546417
[9,  6000] loss: 1.849053
2977.122824907303
[9,  6500] loss: 1.822930
2991.1575577259064
[9,  7000] loss: 1.867695
3005.5133430957794
[9,  7500] loss: 1.882226
3019.601551055908
[9,  8000] loss: 1.859282
3033.780784368515
[9,  8500] loss: 1.884376
3047.8853335380554
[9,  9000] loss: 1.930635
3061.9755585193634
[9,  9500] loss: 1.900939
3076.098166704178
[9, 10000] loss: 1.905583
3090.128001689911
[9, 10500] loss: 1.894227
3104.087494134903
[9, 11000] loss: 1.865151
3118.160324573517
[9, 11500] loss: 1.882895
3132.4578738212585
[9, 12000] loss: 1.832690
3146.2984142303467
[9, 12500] loss: 1.858150
3160.1268804073334
Epoch [9] loss: 5904217.135097
[10,   500] loss: 1.830776
3174.3076412677765
[10,  1000] loss: 1.826983
3188.256604909897
[10,  1500] loss: 1.868014
3202.2683901786804
[10,  2000] loss: 1.852170
3216.1315627098083
[10,  2500] loss: 1.877146
3229.997618198395
[10,  3000] loss: 1.922542
3244.0398938655853
[10,  3500] loss: 1.943078
3257.9936616420746
[10,  4000] loss: 2.009645
3271.9947216510773
[10,  4500] loss: 1.940572
3286.0763988494873
[10,  5000] loss: 1.960716
3300.2205555438995
[10,  5500] loss: 1.940075
3314.0796525478363
[10,  6000] loss: 1.898293
3328.1124823093414
[10,  6500] loss: 1.905851
3342.2045509815216
[10,  7000] loss: 1.879802
3356.4625928401947
[10,  7500] loss: 1.881625
3370.522304058075
[10,  8000] loss: 1.875643
3384.57772731781
[10,  8500] loss: 1.832970
3398.8134138584137
[10,  9000] loss: 1.906794
3412.8877046108246
[10,  9500] loss: 1.889311
3426.9672689437866
[10, 10000] loss: 1.849426
3440.886731863022
[10, 10500] loss: 1.876195
3454.873333454132
[10, 11000] loss: 1.826689
3468.8658363819122
[10, 11500] loss: 1.846648
3482.82900762558
[10, 12000] loss: 1.853681
3496.8422780036926
[10, 12500] loss: 1.805213
3510.898368835449
Epoch [10] loss: 5889177.797590
[11,   500] loss: 1.828564
3525.0587389469147
[11,  1000] loss: 1.847478
3539.019670724869
[11,  1500] loss: 1.832142
3553.055287837982
[11,  2000] loss: 1.802273
3567.0417244434357
[11,  2500] loss: 1.805728
3580.9474036693573
[11,  3000] loss: 1.806328
3594.880103826523
[11,  3500] loss: 1.834103
3609.093210220337
[11,  4000] loss: 1.814099
3623.2402317523956
[11,  4500] loss: 1.776965
3637.2673461437225
[11,  5000] loss: 1.815422
3651.169816970825
[11,  5500] loss: 1.832064
3665.2225391864777
[11,  6000] loss: 1.823784
3679.1730303764343
[11,  6500] loss: 1.799763
3693.265177488327
[11,  7000] loss: 1.819583
3707.2770369052887
[11,  7500] loss: 1.804531
3721.362542629242
[11,  8000] loss: 1.833327
3735.376266002655
[11,  8500] loss: 1.791789
3749.500096321106
[11,  9000] loss: 1.818952
3763.5748813152313
[11,  9500] loss: 1.801878
3777.559134721756
[11, 10000] loss: 1.792745
3791.709046125412
[11, 10500] loss: 1.802576
3805.739401578903
[11, 11000] loss: 1.827303
3819.919439792633
[11, 11500] loss: 1.803035
3833.8256437778473
[11, 12000] loss: 1.871446
3847.9721229076385
[11, 12500] loss: 1.877521
3862.0849900245667
Epoch [11] loss: 5696559.779825
[12,   500] loss: 1.902911
3876.9209039211273
[12,  1000] loss: 1.861315
3891.045686483383
[12,  1500] loss: 1.853379
3905.581517934799
[12,  2000] loss: 1.851393
3930.128131866455
[12,  2500] loss: 1.845220
3944.2063748836517
[12,  3000] loss: 1.818816
3958.342426776886
[12,  3500] loss: 1.792996
3972.228592634201
[12,  4000] loss: 1.836247
3986.232335090637
[12,  4500] loss: 1.809753
4000.201686143875
[12,  5000] loss: 1.792143
4014.2191574573517
[12,  5500] loss: 1.813411
4028.2667739391327
[12,  6000] loss: 1.771733
4042.269213438034
[12,  6500] loss: 1.763326
4056.317750453949
[12,  7000] loss: 1.793859
4070.5158290863037
[12,  7500] loss: 1.801376
4084.683025598526
[12,  8000] loss: 1.774431
4098.732619524002
[12,  8500] loss: 1.807050
4112.705958604813
[12,  9000] loss: 1.761655
4126.8278341293335
[12,  9500] loss: 1.755041
4140.96976852417
[12, 10000] loss: 1.765164
4154.974196910858
[12, 10500] loss: 1.796425
4169.051077842712
[12, 11000] loss: 1.806518
4183.130388736725
[12, 11500] loss: 1.782896
4197.363290071487
[12, 12000] loss: 1.753467
4211.65731883049
[12, 12500] loss: 1.747691
4225.847739934921
Epoch [12] loss: 5645694.517741
[13,   500] loss: 1.741795
4242.093962669373
[13,  1000] loss: 1.780570
4256.10076880455
[13,  1500] loss: 1.730536
4270.1503257751465
[13,  2000] loss: 1.810975
4284.06552195549
[13,  2500] loss: 1.777799
4298.0023872852325
[13,  3000] loss: 1.739542
4311.913496494293
[13,  3500] loss: 1.700294
4325.919613838196
[13,  4000] loss: 1.739885
4339.924695253372
[13,  4500] loss: 1.722783
4354.105346441269
[13,  5000] loss: 1.738241
4368.233550071716
[13,  5500] loss: 1.738188
4382.519197940826
[13,  6000] loss: 1.735882
4396.550064325333
[13,  6500] loss: 1.693674
4410.623673915863
[13,  7000] loss: 1.782213
4424.70031785965
[13,  7500] loss: 1.712689
4438.933162927628
[13,  8000] loss: 1.721673
4453.20651102066
[13,  8500] loss: 1.715905
4467.351706504822
[13,  9000] loss: 1.687292
4481.414882898331
[13,  9500] loss: 1.695282
4495.42276763916
[13, 10000] loss: 2.152886
4509.260128259659
[13, 10500] loss: 2.126975
4523.255864143372
[13, 11000] loss: 2.093505
4537.262962341309
[13, 11500] loss: 2.011800
4551.1018035411835
[13, 12000] loss: 2.022998
4565.131615400314
[13, 12500] loss: 1.972967
4579.11523938179
Epoch [13] loss: 5676988.903848
[14,   500] loss: 1.948398
4593.212815999985
[14,  1000] loss: 1.899547
4607.152687072754
[14,  1500] loss: 1.904064
4621.269437074661
[14,  2000] loss: 1.906371
4635.322845220566
[14,  2500] loss: 1.917020
4649.4666659832
[14,  3000] loss: 1.897565
4663.76172876358
[14,  3500] loss: 1.873641
4678.039121389389
[14,  4000] loss: 1.891204
4692.0795974731445
[14,  4500] loss: 1.891300
4706.039083242416
[14,  5000] loss: 1.861539
4720.003208637238
[14,  5500] loss: 1.868544
4734.093074083328
[14,  6000] loss: 1.826681
4748.097516775131
[14,  6500] loss: 1.860933
4761.971203327179
[14,  7000] loss: 1.841225
4775.8652555942535
[14,  7500] loss: 1.820787
4790.034772872925
[14,  8000] loss: 1.782732
4804.458205938339
[14,  8500] loss: 1.833257
4818.931052446365
[14,  9000] loss: 1.896470
4833.297646284103
[14,  9500] loss: 1.873250
4847.234428882599
[14, 10000] loss: 1.836886
4861.226184606552
[14, 10500] loss: 1.841761
4875.2170395851135
[14, 11000] loss: 1.834122
4889.2423486709595
[14, 11500] loss: 1.824741
4903.269887208939
[14, 12000] loss: 1.816846
4917.132292032242
[14, 12500] loss: 1.790421
4930.9846222400665
Epoch [14] loss: 5835811.130698
[15,   500] loss: 1.825489
4945.021965742111
[15,  1000] loss: 1.851135
4958.937940359116
[15,  1500] loss: 1.795794
4972.858676195145
[15,  2000] loss: 1.779148
4986.845621347427
[15,  2500] loss: 1.792462
5000.783376455307
[15,  3000] loss: 1.812044
5014.864106893539
[15,  3500] loss: 1.784938
5028.8329067230225
[15,  4000] loss: 1.768116
5042.800179719925
[15,  4500] loss: 1.730045
5056.738399982452
[15,  5000] loss: 1.760591
5070.651883602142
[15,  5500] loss: 1.791551
5084.652832746506
[15,  6000] loss: 1.754933
5098.806885480881
[15,  6500] loss: 1.721941
5112.93795633316
[15,  7000] loss: 1.706488
5126.9171760082245
[15,  7500] loss: 1.696941
5140.872614622116
[15,  8000] loss: 1.700995
5154.850295305252
[15,  8500] loss: 1.714828
5169.177522182465
[15,  9000] loss: 1.716364
5183.454788684845
[15,  9500] loss: 1.754017
5197.630562543869
[15, 10000] loss: 1.686313
5211.556167840958
[15, 10500] loss: 1.703901
5225.472248077393
[15, 11000] loss: 1.727597
5239.356961250305
[15, 11500] loss: 1.684628
5253.227334737778
[15, 12000] loss: 1.722507
5267.10383939743
[15, 12500] loss: 1.710998
5281.040097236633
Epoch [15] loss: 5481466.014677
[16,   500] loss: 1.686721
5295.127380371094
[16,  1000] loss: 1.699563
5309.074602365494
[16,  1500] loss: 1.666268
5323.004590034485
[16,  2000] loss: 1.673907
5336.816760778427
[16,  2500] loss: 1.663392
5350.9611122608185
[16,  3000] loss: 1.675861
5364.9123067855835
[16,  3500] loss: 1.638965
5379.032747507095
[16,  4000] loss: 1.670340
5393.1325850486755
[16,  4500] loss: 1.684345
5407.692897796631
[16,  5000] loss: 1.641036
5422.019878864288
[16,  5500] loss: 1.662217
5436.308997631073
[16,  6000] loss: 1.643729
5450.481840610504
[16,  6500] loss: 1.686901
5464.613341093063
[16,  7000] loss: 1.655886
5478.5455849170685
[16,  7500] loss: 1.630724
5492.4714415073395
[16,  8000] loss: 1.686270
5506.599173784256
[16,  8500] loss: 1.633393
5520.669257879257
[16,  9000] loss: 1.600519
5534.526213645935
[16,  9500] loss: 1.654706
5548.669281959534
[16, 10000] loss: 1.642447
5562.6601095199585
[16, 10500] loss: 1.639488
5576.700610637665
[16, 11000] loss: 1.667223
5590.7206218242645
[16, 11500] loss: 1.627153
5604.812540769577
[16, 12000] loss: 1.628868
5618.912119626999
[16, 12500] loss: 1.599177
5632.947759628296
Epoch [16] loss: 5180921.500926
[17,   500] loss: 1.590219
5647.550729036331
[17,  1000] loss: 1.640408
5661.574549674988
[17,  1500] loss: 1.619955
5675.6196002960205
[17,  2000] loss: 1.621376
5689.79763507843
[17,  2500] loss: 1.626442
5704.068465471268
[17,  3000] loss: 1.641949
5718.209012508392
[17,  3500] loss: 1.592127
5732.636281967163
[17,  4000] loss: 1.711016
5746.768645763397
[17,  4500] loss: 1.650133
5760.987704753876
[17,  5000] loss: 1.695938
5775.141384124756
[17,  5500] loss: 1.713660
5788.943790197372
[17,  6000] loss: 1.823675
5802.829452991486
[17,  6500] loss: 1.740110
5816.7640209198
[17,  7000] loss: 1.740143
5830.594995975494
[17,  7500] loss: 1.661755
5844.583254814148
[17,  8000] loss: 1.656916
5858.784962177277
[17,  8500] loss: 1.661662
5872.909718990326
[17,  9000] loss: 1.966942
5887.078315734863
[17,  9500] loss: 1.851478
5901.087592601776
[17, 10000] loss: 1.852305
5915.261849403381
[17, 10500] loss: 1.820795
5929.539591550827
[17, 11000] loss: 1.769415
5943.574164152145
[17, 11500] loss: 1.760318
5957.656175851822
[17, 12000] loss: 1.759954
5971.593468427658
[17, 12500] loss: 1.748893
5985.624512195587
Epoch [17] loss: 5369753.533893
[18,   500] loss: 1.751106
6000.003509283066
[18,  1000] loss: 1.726848
6014.685101032257
[18,  1500] loss: 1.738435
6028.924757480621
[18,  2000] loss: 1.685041
6043.074688673019
[18,  2500] loss: 1.697294
6057.033184528351
[18,  3000] loss: 1.716637
6071.123769521713
[18,  3500] loss: 1.683025
6085.223794698715
[18,  4000] loss: 1.687707
6099.222288608551
[18,  4500] loss: 1.696167
6113.064204692841
[18,  5000] loss: 1.672335
6126.966083526611
[18,  5500] loss: 1.690577
6141.001816272736
[18,  6000] loss: 1.688713
6154.9649794101715
[18,  6500] loss: 1.669151
6168.865810871124
[18,  7000] loss: 1.677317
6182.895849466324
[18,  7500] loss: 1.700929
6196.8737823963165
[18,  8000] loss: 1.652860
6210.89714050293
[18,  8500] loss: 1.688257
6225.065300226212
[18,  9000] loss: 1.641512
6238.980185747147
[18,  9500] loss: 1.700911
6252.893565416336
[18, 10000] loss: 1.673057
6266.996224403381
[18, 10500] loss: 1.682494
6281.020866632462
[18, 11000] loss: 1.597759
6295.124516963959
[18, 11500] loss: 1.643412
6309.212204217911
[18, 12000] loss: 1.626268
6323.257984876633
[18, 12500] loss: 1.652837
6337.288872718811
Epoch [18] loss: 5273567.959635
[19,   500] loss: 1.649018
6351.497073888779
[19,  1000] loss: 1.610869
6365.589758872986
[19,  1500] loss: 1.649975
6379.510773181915
[19,  2000] loss: 1.646189
6393.796251773834
[19,  2500] loss: 1.619171
6408.021171808243
[19,  3000] loss: 1.613768
6422.23365855217
[19,  3500] loss: 1.615701
6436.347012042999
[19,  4000] loss: 1.628986
6450.252806663513
[19,  4500] loss: 1.623041
6464.169073343277
[19,  5000] loss: 1.569869
6478.299402952194
[19,  5500] loss: 1.583540
6492.275891542435
[19,  6000] loss: 1.592899
6506.223310947418
[19,  6500] loss: 1.614892
6520.287946462631
[19,  7000] loss: 1.621784
6534.248878955841
[19,  7500] loss: 1.614892
6548.183521270752
[19,  8000] loss: 1.567622
6561.9896149635315
[19,  8500] loss: 1.575817
6575.955174922943
[19,  9000] loss: 1.606531
6590.037521362305
[19,  9500] loss: 1.609491
6604.44517326355
[19, 10000] loss: 1.587435
6618.67508482933
[19, 10500] loss: 1.572796
6632.916444778442
[19, 11000] loss: 1.573204
6647.231957435608
[19, 11500] loss: 1.576037
6661.423979759216
[19, 12000] loss: 1.583308
6675.54890704155
[19, 12500] loss: 1.593497
6689.757701873779
Epoch [19] loss: 5013943.444975
[20,   500] loss: 1.561473
6703.966772079468
[20,  1000] loss: 1.619710
6718.191564798355
[20,  1500] loss: 1.573365
6732.301453828812
[20,  2000] loss: 1.563491
6746.531764745712
[20,  2500] loss: 1.533306
6760.61475110054
[20,  3000] loss: 1.569597
6774.660452127457
[20,  3500] loss: 1.575534
6788.798197984695
[20,  4000] loss: 1.570025
6802.778459787369
[20,  4500] loss: 1.586946
6817.0500831604
[20,  5000] loss: 1.549572
6831.316165924072
[20,  5500] loss: 1.543556
6845.435055971146
[20,  6000] loss: 1.581560
6859.260303735733
[20,  6500] loss: 1.597913
6873.23731136322
[20,  7000] loss: 1.740434
6887.285169124603
[20,  7500] loss: 1.740796
6901.474858522415
[20,  8000] loss: 1.675335
6915.592430591583
[20,  8500] loss: 1.655412
6929.50065946579
[20,  9000] loss: 1.638610
6943.476555109024
[20,  9500] loss: 1.635591
6957.347862243652
[20, 10000] loss: 1.625134
6971.253018856049
[20, 10500] loss: 1.659120
6985.318413734436
[20, 11000] loss: 1.607813
6999.131009817123
[20, 11500] loss: 1.652588
7012.893771886826
[20, 12000] loss: 1.755371
7026.841087341309
[20, 12500] loss: 1.768911
7041.0639362335205
Epoch [20] loss: 5078680.940190
Finished Training
Saving model to /data/s4091221/trained-models/wide_resnet50_22020-02-26 03:07:29.027280
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-1.9786e-01,  1.5717e-01,  1.6491e-01,  1.4952e-01,  1.4991e-01,
         -5.3375e-02, -1.4929e-03, -3.3034e-02, -9.4267e-02, -1.8825e-01],
        [ 3.2706e+00,  4.3708e+00, -1.6098e+00, -2.3490e+00, -2.0688e+00,
         -3.7555e+00, -2.8297e+00, -1.9917e+00,  3.5844e+00,  3.3754e+00],
        [ 1.4167e+00,  5.0294e-01, -1.6122e-01, -9.1292e-01, -1.7270e-01,
         -1.6150e+00, -1.1243e+00, -6.7136e-01,  1.9643e+00,  7.7991e-01],
        [ 1.9578e+00,  1.1222e+00, -6.0260e-01, -1.2043e+00, -4.8729e-01,
         -2.0907e+00, -1.2906e+00, -1.1988e+00,  2.7746e+00,  1.0108e+00]],
       device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:   bird   car  ship  ship
Accuracy of the network on the 4000.0 test images: 38 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 16, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=16, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=34, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
3125
  cat  ship  bird   car
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model wide_resnet50_2 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model wide_resnet50_2 Reshaped
Number of parameters: 66854730 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582682923.1260426
[1,   500] loss: 2.397252
15.067156314849854
[1,  1000] loss: 2.250293
30.05516743659973
[1,  1500] loss: 2.158594
45.153210163116455
[1,  2000] loss: 2.082177
60.137601375579834
[1,  2500] loss: 2.034154
75.14336824417114
[1,  3000] loss: 1.993944
90.2235541343689
Epoch [1] loss: 1642328.008217
[2,   500] loss: 1.962268
109.15684771537781
[2,  1000] loss: 1.917284
124.29405546188354
[2,  1500] loss: 1.910135
139.28002858161926
[2,  2000] loss: 1.884962
154.30475759506226
[2,  2500] loss: 1.840237
169.3558690547943
[2,  3000] loss: 1.814112
184.27065801620483
Epoch [2] loss: 1437784.158668
[3,   500] loss: 1.781356
202.99078178405762
[3,  1000] loss: 1.750760
217.92482042312622
[3,  1500] loss: 1.751940
232.85715055465698
[3,  2000] loss: 1.745973
247.93429946899414
[3,  2500] loss: 1.738453
262.9066553115845
[3,  3000] loss: 1.711710
277.8682074546814
Epoch [3] loss: 1328138.995899
[4,   500] loss: 1.665973
296.71445870399475
[4,  1000] loss: 1.638505
311.7058951854706
[4,  1500] loss: 1.658624
326.7875666618347
[4,  2000] loss: 1.621557
341.7273361682892
[4,  2500] loss: 1.618485
356.66805481910706
[4,  3000] loss: 1.587796
371.6425793170929
Epoch [4] loss: 1240542.582279
[5,   500] loss: 1.574166
390.629430770874
[5,  1000] loss: 1.559007
405.5804121494293
[5,  1500] loss: 1.571013
420.5993118286133
[5,  2000] loss: 1.544461
435.58299493789673
[5,  2500] loss: 1.557200
450.56666016578674
[5,  3000] loss: 1.524730
465.5580027103424
Epoch [5] loss: 1181835.753338
[6,   500] loss: 1.499459
484.4937205314636
[6,  1000] loss: 1.498616
499.56573939323425
[6,  1500] loss: 1.507868
514.6128852367401
[6,  2000] loss: 1.499327
529.5649440288544
[6,  2500] loss: 1.476505
544.5376689434052
[6,  3000] loss: 1.457921
559.4914762973785
Epoch [6] loss: 1129211.193894
[7,   500] loss: 1.432426
578.32297539711
[7,  1000] loss: 1.432550
593.3468019962311
[7,  1500] loss: 1.411059
608.3407766819
[7,  2000] loss: 1.427980
623.4493041038513
[7,  2500] loss: 1.420503
638.5401620864868
[7,  3000] loss: 1.398858
653.5715248584747
Epoch [7] loss: 1079568.780024
[8,   500] loss: 1.358231
672.6631708145142
[8,  1000] loss: 1.376699
688.0202782154083
[8,  1500] loss: 1.367609
703.191064119339
[8,  2000] loss: 1.375773
718.3711602687836
[8,  2500] loss: 1.359293
733.4012706279755
[8,  3000] loss: 1.349704
748.5545010566711
Epoch [8] loss: 1035164.498910
[9,   500] loss: 1.293072
767.618424654007
[9,  1000] loss: 1.304960
782.6865139007568
[9,  1500] loss: 1.302672
797.701372385025
[9,  2000] loss: 1.320207
812.7028963565826
[9,  2500] loss: 1.319370
827.6961667537689
[9,  3000] loss: 1.305543
842.9385454654694
Epoch [9] loss: 992442.669017
[10,   500] loss: 1.264593
862.0797302722931
[10,  1000] loss: 1.255069
877.339848279953
[10,  1500] loss: 1.256644
892.4636616706848
[10,  2000] loss: 1.227509
907.5025069713593
[10,  2500] loss: 1.284641
922.630072593689
[10,  3000] loss: 1.270555
937.79776263237
Epoch [10] loss: 953449.045827
[11,   500] loss: 1.189386
956.7832353115082
[11,  1000] loss: 1.198185
971.8512444496155
[11,  1500] loss: 1.247528
986.9780166149139
[11,  2000] loss: 1.187092
1001.8966679573059
[11,  2500] loss: 1.222965
1016.9447495937347
[11,  3000] loss: 1.198810
1032.0961010456085
Epoch [11] loss: 915195.230758
[12,   500] loss: 1.155363
1051.1762373447418
[12,  1000] loss: 1.152248
1066.2423601150513
[12,  1500] loss: 1.171023
1081.2749078273773
[12,  2000] loss: 1.153786
1096.190582036972
[12,  2500] loss: 1.127885
1111.2054755687714
[12,  3000] loss: 1.165872
1126.3027703762054
Epoch [12] loss: 876047.158014
[13,   500] loss: 1.078346
1145.1634542942047
[13,  1000] loss: 1.099648
1160.215829372406
[13,  1500] loss: 1.113413
1175.2298686504364
[13,  2000] loss: 1.091266
1190.1652846336365
[13,  2500] loss: 1.152402
1205.2753539085388
[13,  3000] loss: 1.118072
1220.4022285938263
Epoch [13] loss: 842694.415067
[14,   500] loss: 1.055751
1239.375975370407
[14,  1000] loss: 1.028020
1254.3878409862518
[14,  1500] loss: 1.053474
1269.378494501114
[14,  2000] loss: 1.069612
1284.593409538269
[14,  2500] loss: 1.066861
1299.7810671329498
[14,  3000] loss: 1.059038
1315.0397033691406
Epoch [14] loss: 800370.839465
[15,   500] loss: 0.974400
1333.9315042495728
[15,  1000] loss: 0.979441
1348.8908927440643
[15,  1500] loss: 1.010758
1363.927149772644
[15,  2000] loss: 1.007320
1378.8849890232086
[15,  2500] loss: 1.028301
1393.8125405311584
[15,  3000] loss: 1.014988
1408.6913907527924
Epoch [15] loss: 760820.832257
[16,   500] loss: 0.938607
1427.5591008663177
[16,  1000] loss: 0.942418
1442.3893580436707
[16,  1500] loss: 0.950695
1457.3114416599274
[16,  2000] loss: 0.963910
1472.2669870853424
[16,  2500] loss: 0.965001
1487.3373363018036
[16,  3000] loss: 0.934181
1502.4814739227295
Epoch [16] loss: 719800.370594
[17,   500] loss: 0.873214
1521.5495238304138
[17,  1000] loss: 0.880749
1536.5969610214233
[17,  1500] loss: 0.907740
1551.596672296524
[17,  2000] loss: 0.895261
1566.5615711212158
[17,  2500] loss: 0.914261
1581.5854623317719
[17,  3000] loss: 0.929118
1596.6022651195526
Epoch [17] loss: 682729.732560
[18,   500] loss: 0.841000
1615.4019289016724
[18,  1000] loss: 0.853731
1630.4376759529114
[18,  1500] loss: 0.840282
1645.3876938819885
[18,  2000] loss: 0.852476
1660.3353037834167
[18,  2500] loss: 0.859920
1675.2908880710602
[18,  3000] loss: 0.871939
1690.3007230758667
Epoch [18] loss: 645197.614107
[19,   500] loss: 0.770931
1709.1877131462097
[19,  1000] loss: 0.777331
1724.1723847389221
[19,  1500] loss: 0.783771
1739.1312775611877
[19,  2000] loss: 0.809795
1754.1896896362305
[19,  2500] loss: 0.832596
1769.348982334137
[19,  3000] loss: 0.811360
1784.3942070007324
Epoch [19] loss: 606865.037778
[20,   500] loss: 0.716184
1803.3731200695038
[20,  1000] loss: 0.721611
1818.354956626892
[20,  1500] loss: 0.763776
1833.5412545204163
[20,  2000] loss: 0.767352
1848.7428193092346
[20,  2500] loss: 0.743503
1863.8728349208832
[20,  3000] loss: 0.784870
1878.9576983451843
Epoch [20] loss: 574790.870796
Finished Training
Saving model to /data/s4091221/trained-models/wide_resnet50_22020-02-26 03:40:05.949223
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-3.5944e-01, -9.5896e-01,  1.6476e+00,  2.7462e+00,  4.1826e-01,
          8.9224e-01, -1.0183e+00, -1.9269e+00,  7.7356e-01, -1.3806e+00],
        [ 3.0392e+00,  7.2203e+00, -7.5655e-03, -3.5987e+00, -1.5512e+00,
         -4.0074e+00, -4.8315e+00, -5.3224e+00,  6.4528e+00,  4.6540e+00],
        [ 4.1503e+00,  8.6454e-01, -2.0106e+00, -1.9950e+00, -9.0833e-01,
         -1.3727e+00, -4.3506e+00, -2.1897e+00,  5.0234e+00,  3.4855e+00],
        [ 7.7082e+00, -2.4736e+00,  1.5538e+00, -1.3125e+00,  4.3345e+00,
         -3.3346e+00, -4.7277e+00, -1.1924e+00,  3.1306e+00, -1.7000e+00],
        [-4.7333e+00, -7.9146e+00,  5.9347e+00,  5.5906e+00,  6.3164e+00,
          2.5287e+00,  4.2736e+00, -3.8258e-01, -7.2782e+00, -3.3566e+00],
        [-1.6454e+00, -3.1574e+00,  3.3531e+00,  2.0867e+00, -2.8269e-02,
          1.2621e+00,  3.7040e+00, -1.5575e+00, -2.0163e+00, -2.0496e+00],
        [ 1.7118e+00,  4.2266e+00, -1.1612e+00, -2.8798e-01, -3.1859e+00,
         -1.4020e+00, -2.6203e+00,  2.3104e+00, -1.7695e+00,  3.5827e+00],
        [-2.3943e+00, -6.9386e+00,  4.5059e+00,  2.0713e+00,  5.5441e+00,
          2.9585e+00,  4.2197e+00, -4.3937e-01, -3.1316e+00, -4.1469e+00],
        [ 1.0848e-01, -1.2431e+00,  9.0287e-01,  4.2031e+00,  1.2097e+00,
          2.0766e+00, -1.2730e+00, -3.0541e-01, -3.0836e+00, -7.1787e-01],
        [-3.3386e+00,  6.7869e+00, -2.7235e+00, -8.2453e-01, -1.8816e+00,
         -2.2145e+00,  1.6563e+00, -5.9849e+00,  8.5735e-02,  6.7351e+00],
        [ 3.6697e+00, -6.2872e+00,  9.4999e-01,  2.1583e+00,  1.1770e+00,
          1.7987e+00, -1.1404e+00,  4.8831e-01,  1.9583e+00, -4.5001e+00],
        [-2.3759e+00,  2.7812e+00, -7.3467e-01, -2.1698e+00, -1.8276e+00,
         -1.7979e-01, -1.7837e+00, -1.0023e+00, -2.9463e+00,  8.5075e+00],
        [-3.5935e+00, -1.3562e+00, -3.8015e-01,  3.0202e+00,  7.6464e-01,
          3.1842e+00,  3.7773e+00, -6.5790e-01, -4.0843e+00, -1.8705e+00],
        [-3.2325e+00, -6.8622e+00,  2.4804e+00,  3.8259e+00,  2.4481e+00,
          4.9959e+00,  1.5141e-01,  7.5000e+00, -8.3943e+00, -4.4826e-01],
        [ 2.8477e-01,  2.5466e+00,  4.1318e-01, -1.9771e-01, -3.2197e+00,
         -2.8300e-01, -2.8470e+00, -2.8597e-01, -2.7408e+00,  6.9648e+00],
        [ 1.3518e+00, -8.6591e-01,  1.9989e-01,  2.1921e-01,  6.8061e-02,
         -1.0844e+00,  7.8249e-01, -4.4959e+00,  4.4270e+00, -6.7590e-01]],
       device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    cat   car  ship plane
Accuracy of the network on the 4000.0 test images: 60 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0.9, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=34, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0.9, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 ship plane truck horse
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model wide_resnet50_2 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model wide_resnet50_2 Reshaped
Number of parameters: 66854730 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0.9, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582684822.4167972
[1,   500] loss: 2.446987
13.366297483444214
[1,  1000] loss: 2.284347
26.375353574752808
[1,  1500] loss: 2.234572
39.2953565120697
[1,  2000] loss: 2.195118
52.2780487537384
[1,  2500] loss: 2.190041
65.27977323532104
[1,  3000] loss: 2.187788
78.23573708534241
[1,  3500] loss: 2.231905
91.19948172569275
[1,  4000] loss: 2.299600
104.15060496330261
[1,  4500] loss: 2.302647
117.07919216156006
[1,  5000] loss: 2.302586
130.05683493614197
[1,  5500] loss: 2.302547
143.03489780426025
[1,  6000] loss: 2.302559
156.0155735015869
[1,  6500] loss: 2.302603
168.98424649238586
[1,  7000] loss: 2.302503
182.12847304344177
[1,  7500] loss: 2.302654
195.19657444953918
[1,  8000] loss: 2.302592
208.2937035560608
[1,  8500] loss: 2.302552
221.49404096603394
[1,  9000] loss: 2.302715
234.50972318649292
[1,  9500] loss: 2.302599
247.51572227478027
[1, 10000] loss: 2.302611
260.5111594200134
[1, 10500] loss: 2.302558
273.5315330028534
[1, 11000] loss: 2.302610
286.8399438858032
[1, 11500] loss: 2.302537
299.7956631183624
[1, 12000] loss: 2.302560
312.877783536911
[1, 12500] loss: 2.302666
325.91094851493835
Epoch [1] loss: 7170793.973080
[2,   500] loss: 2.302582
339.02019333839417
[2,  1000] loss: 2.302599
352.03592443466187
[2,  1500] loss: 2.302639
365.07869720458984
[2,  2000] loss: 2.302626
378.14824295043945
[2,  2500] loss: 2.302549
391.19673585891724
[2,  3000] loss: 2.302649
404.2343113422394
[2,  3500] loss: 2.302542
417.17617440223694
[2,  4000] loss: 2.302696
430.1017904281616
[2,  4500] loss: 2.302636
443.21152329444885
[2,  5000] loss: 2.302572
456.31898379325867
[2,  5500] loss: 2.302671
469.47650146484375
[2,  6000] loss: 2.302597
482.4991247653961
[2,  6500] loss: 2.302542
495.52252173423767
[2,  7000] loss: 2.302507
508.60778427124023
[2,  7500] loss: 2.302652
521.6308605670929
[2,  8000] loss: 2.302606
534.6325213909149
[2,  8500] loss: 2.302638
547.6001586914062
[2,  9000] loss: 2.302555
560.5979034900665
[2,  9500] loss: 2.302658
573.5148205757141
[2, 10000] loss: 2.302638
587.0551209449768
[2, 10500] loss: 2.302494
600.602343082428
[2, 11000] loss: 2.302720
613.7419419288635
[2, 11500] loss: 2.302613
626.8213467597961
[2, 12000] loss: 2.302604
639.8782403469086
[2, 12500] loss: 2.302624
652.8277690410614
Epoch [2] loss: 7210058.845269
[3,   500] loss: 2.302638
665.9485869407654
[3,  1000] loss: 2.302507
678.9210715293884
[3,  1500] loss: 2.302628
691.9226608276367
[3,  2000] loss: 2.302501
704.9128661155701
[3,  2500] loss: 2.302738
717.8964574337006
[3,  3000] loss: 2.302641
730.8762545585632
[3,  3500] loss: 2.302501
743.7965579032898
[3,  4000] loss: 2.302672
756.5726432800293
[3,  4500] loss: 2.302693
769.594979763031
[3,  5000] loss: 2.302675
782.5973844528198
[3,  5500] loss: 2.302623
795.5935475826263
[3,  6000] loss: 2.302631
808.4840013980865
[3,  6500] loss: 2.302560
821.4978339672089
[3,  7000] loss: 2.302639
834.4929502010345
[3,  7500] loss: 2.302659
847.5755107402802
[3,  8000] loss: 2.302613
860.5756776332855
[3,  8500] loss: 2.302564
873.4944174289703
[3,  9000] loss: 2.302656
886.5555219650269
[3,  9500] loss: 2.302532
899.5545191764832
[3, 10000] loss: 2.302528
912.7137627601624
[3, 10500] loss: 2.302743
925.783061504364
[3, 11000] loss: 2.302634
938.7235724925995
[3, 11500] loss: 2.302609
951.6897656917572
[3, 12000] loss: 2.302581
964.6749305725098
[3, 12500] loss: 2.302668
977.767519235611
Epoch [3] loss: 7210072.145004
[4,   500] loss: 2.302657
990.9464800357819
[4,  1000] loss: 2.302618
1003.9742333889008
[4,  1500] loss: 2.302628
1016.8988664150238
[4,  2000] loss: 2.302649
1029.871962070465
[4,  2500] loss: 2.302672
1042.8061051368713
[4,  3000] loss: 2.302592
1055.785546541214
[4,  3500] loss: 2.302478
1068.786512851715
[4,  4000] loss: 2.302584
1081.6434695720673
[4,  4500] loss: 2.302729
1094.5548651218414
[4,  5000] loss: 2.302610
1107.489840745926
[4,  5500] loss: 2.302568
1120.4370329380035
[4,  6000] loss: 2.302606
1133.3524525165558
[4,  6500] loss: 2.302661
1146.2301461696625
[4,  7000] loss: 2.302637
1159.2433984279633
[4,  7500] loss: 2.302579
1172.1999695301056
[4,  8000] loss: 2.302700
1185.7877576351166
[4,  8500] loss: 2.302581
1199.0943846702576
[4,  9000] loss: 2.302592
1212.408732175827
[4,  9500] loss: 2.302622
1225.6982686519623
[4, 10000] loss: 2.302612
1239.164873600006
[4, 10500] loss: 2.302533
1252.6694810390472
[4, 11000] loss: 2.302592
1265.9592308998108
[4, 11500] loss: 2.302664
1279.187838792801
[4, 12000] loss: 2.302598
1292.3288834095001
[4, 12500] loss: 2.302687
1305.543860912323
Epoch [4] loss: 7210058.966875
[5,   500] loss: 2.302580
1318.9257020950317
[5,  1000] loss: 2.302377
1332.1729147434235
[5,  1500] loss: 2.302693
1345.6042618751526
[5,  2000] loss: 2.302613
1359.1186048984528
[5,  2500] loss: 2.302590
1372.4626970291138
[5,  3000] loss: 2.302583
1385.84561085701
[5,  3500] loss: 2.302580
1399.2484679222107
[5,  4000] loss: 2.302717
1412.5022740364075
[5,  4500] loss: 2.302642
1425.9741320610046
[5,  5000] loss: 2.302497
1439.2275204658508
[5,  5500] loss: 2.302649
1452.6479926109314
[5,  6000] loss: 2.302674
1465.9344618320465
[5,  6500] loss: 2.302592
1480.033706665039
[5,  7000] loss: 2.302664
1493.5057451725006
[5,  7500] loss: 2.302640
1506.883902311325
[5,  8000] loss: 2.302594
1520.301278591156
[5,  8500] loss: 2.302651
1533.533493757248
[5,  9000] loss: 2.302600
1546.8920984268188
[5,  9500] loss: 2.302522
1560.383543252945
[5, 10000] loss: 2.302647
1573.7537677288055
[5, 10500] loss: 2.302677
1587.080221414566
[5, 11000] loss: 2.302611
1600.5243632793427
[5, 11500] loss: 2.302657
1614.0149173736572
[5, 12000] loss: 2.302583
1627.4863517284393
[5, 12500] loss: 2.302521
1641.2755665779114
Epoch [5] loss: 7210045.164797
[6,   500] loss: 2.302632
1654.9385600090027
[6,  1000] loss: 2.302620
1668.652780532837
[6,  1500] loss: 2.302571
1682.094143629074
[6,  2000] loss: 2.302676
1695.3366751670837
[6,  2500] loss: 2.302658
1708.6285150051117
[6,  3000] loss: 2.302532
1721.8700497150421
[6,  3500] loss: 2.302623
1735.0657687187195
[6,  4000] loss: 2.302558
1748.4328451156616
[6,  4500] loss: 2.302693
1761.8742127418518
[6,  5000] loss: 2.302617
1775.3309924602509
[6,  5500] loss: 2.302612
1788.828153848648
[6,  6000] loss: 2.302657
1802.2309012413025
[6,  6500] loss: 2.302599
1815.5555942058563
[6,  7000] loss: 2.302626
1828.9233384132385
[6,  7500] loss: 2.302604
1842.3634305000305
[6,  8000] loss: 2.302611
1855.5345196723938
[6,  8500] loss: 2.302612
1868.8285155296326
[6,  9000] loss: 2.302555
1882.1089596748352
[6,  9500] loss: 2.302607
1895.2853202819824
[6, 10000] loss: 2.302540
1908.4147288799286
[6, 10500] loss: 2.302731
1921.5233538150787
[6, 11000] loss: 2.302641
1934.587907075882
[6, 11500] loss: 2.302526
1947.7540037631989
[6, 12000] loss: 2.302591
1960.9447915554047
[6, 12500] loss: 2.302632
1974.1310641765594
Epoch [6] loss: 7210048.158392
[7,   500] loss: 2.302632
1987.3242406845093
[7,  1000] loss: 2.302548
2000.310795545578
[7,  1500] loss: 2.302728
2013.5231764316559
[7,  2000] loss: 2.302632
2026.703631401062
[7,  2500] loss: 2.302655
2039.9041714668274
[7,  3000] loss: 2.302626
2053.0529475212097
[7,  3500] loss: 2.302640
2066.266523361206
[7,  4000] loss: 2.302570
2079.5401108264923
[7,  4500] loss: 2.302575
2092.8490312099457
[7,  5000] loss: 2.302517
2106.0164680480957
[7,  5500] loss: 2.302555
2119.2912797927856
[7,  6000] loss: 2.302565
2132.5061242580414
[7,  6500] loss: 2.302576
2145.7228429317474
[7,  7000] loss: 2.302575
2158.88799905777
[7,  7500] loss: 2.302640
2172.071356534958
[7,  8000] loss: 2.302417
2185.2018637657166
[7,  8500] loss: 2.302688
2198.445492506027
[7,  9000] loss: 2.302577
2211.550798892975
[7,  9500] loss: 2.302674
2224.599531888962
[7, 10000] loss: 2.302591
2237.6989850997925
[7, 10500] loss: 2.302600
2250.8040409088135
[7, 11000] loss: 2.302678
2264.0039851665497
[7, 11500] loss: 2.302565
2277.0949635505676
[7, 12000] loss: 2.302694
2290.079607486725
[7, 12500] loss: 2.302511
2303.121385574341
Epoch [7] loss: 7209994.871009
[8,   500] loss: 2.302582
2316.305797815323
[8,  1000] loss: 2.302663
2329.469853401184
[8,  1500] loss: 2.302653
2342.4572422504425
[8,  2000] loss: 2.302651
2355.5763370990753
[8,  2500] loss: 2.302611
2368.675866127014
[8,  3000] loss: 2.302599
2381.8448905944824
[8,  3500] loss: 2.302604
2395.067347049713
[8,  4000] loss: 2.302666
2408.1260137557983
[8,  4500] loss: 2.302642
2421.1529223918915
[8,  5000] loss: 2.302654
2434.23885846138
[8,  5500] loss: 2.302496
2447.331706762314
[8,  6000] loss: 2.302597
2460.347028493881
[8,  6500] loss: 2.302636
2473.3657734394073
[8,  7000] loss: 2.302683
2486.357651233673
[8,  7500] loss: 2.302672
2499.438131570816
[8,  8000] loss: 2.302592
2512.585428953171
[8,  8500] loss: 2.302616
2525.687866449356
[8,  9000] loss: 2.302664
2538.761662006378
[8,  9500] loss: 2.302594
2551.6425676345825
[8, 10000] loss: 2.302542
2564.751680612564
[8, 10500] loss: 2.302644
2577.877896308899
[8, 11000] loss: 2.302599
2591.0257427692413
[8, 11500] loss: 2.302552
2604.3433322906494
[8, 12000] loss: 2.302617
2617.524537086487
[8, 12500] loss: 2.302591
2630.6633837223053
Epoch [8] loss: 7210076.602699
[9,   500] loss: 2.302620
2643.8674054145813
[9,  1000] loss: 2.302560
2656.872225522995
[9,  1500] loss: 2.302571
2669.771003961563
[9,  2000] loss: 2.302611
2682.8444170951843
[9,  2500] loss: 2.302410
2695.814933538437
[9,  3000] loss: 2.302745
2708.790227651596
[9,  3500] loss: 2.302585
2721.852210998535
[9,  4000] loss: 2.302573
2735.0012505054474
[9,  4500] loss: 2.302686
2748.205302476883
[9,  5000] loss: 2.302667
2761.334493637085
[9,  5500] loss: 2.302540
2774.4192357063293
[9,  6000] loss: 2.302581
2787.559344768524
[9,  6500] loss: 2.302515
2800.660652399063
[9,  7000] loss: 2.302588
2813.783504962921
[9,  7500] loss: 2.302673
2826.8786177635193
[9,  8000] loss: 2.302648
2840.1182732582092
[9,  8500] loss: 2.302509
2853.212249517441
[9,  9000] loss: 2.302578
2866.287393808365
[9,  9500] loss: 2.302668
2879.3008000850677
[9, 10000] loss: 2.302647
2892.2403843402863
[9, 10500] loss: 2.302451
2905.2288086414337
[9, 11000] loss: 2.302638
2918.228939294815
[9, 11500] loss: 2.302665
2931.260682106018
[9, 12000] loss: 2.302572
2944.342980861664
[9, 12500] loss: 2.302720
2957.372857570648
Epoch [9] loss: 7210035.501244
[10,   500] loss: 2.302604
2970.506234884262
[10,  1000] loss: 2.302580
2983.65630030632
[10,  1500] loss: 2.302625
2996.8404099941254
[10,  2000] loss: 2.302647
3010.070684194565
[10,  2500] loss: 2.302646
3023.1337592601776
[10,  3000] loss: 2.302536
3036.0995099544525
[10,  3500] loss: 2.302665
3048.9980595111847
[10,  4000] loss: 2.302514
3061.969779253006
[10,  4500] loss: 2.302401
3074.971190214157
[10,  5000] loss: 2.302525
3087.9176812171936
[10,  5500] loss: 2.302460
3100.932493686676
[10,  6000] loss: 2.302654
3113.901228904724
[10,  6500] loss: 2.302518
3126.8354077339172
[10,  7000] loss: 2.302639
3139.8281128406525
[10,  7500] loss: 2.302655
3152.8840143680573
[10,  8000] loss: 2.302619
3165.9002737998962
[10,  8500] loss: 2.302662
3178.9615545272827
[10,  9000] loss: 2.302614
3191.9760222434998
[10,  9500] loss: 2.302601
3204.9684159755707
[10, 10000] loss: 2.302699
3217.99453663826
[10, 10500] loss: 2.302642
3230.976927280426
[10, 11000] loss: 2.302574
3244.027079820633
[10, 11500] loss: 2.302563
3257.104870080948
[10, 12000] loss: 2.302596
3270.2326180934906
[10, 12500] loss: 2.302283
3283.6282572746277
Epoch [10] loss: 7209982.838327
[11,   500] loss: 2.302546
3296.9037165641785
[11,  1000] loss: 2.302414
3310.0312898159027
[11,  1500] loss: 2.302738
3323.2062814235687
[11,  2000] loss: 2.302657
3336.3178522586823
[11,  2500] loss: 2.302502
3349.3453755378723
[11,  3000] loss: 2.302667
3362.3259468078613
[11,  3500] loss: 2.302629
3375.359807252884
[11,  4000] loss: 2.302638
3388.4108035564423
[11,  4500] loss: 2.302594
3401.4609155654907
[11,  5000] loss: 2.302567
3414.6032338142395
[11,  5500] loss: 2.302583
3427.60919713974
[11,  6000] loss: 2.302565
3440.6648066043854
[11,  6500] loss: 2.302646
3453.615038394928
[11,  7000] loss: 2.302676
3466.685346364975
[11,  7500] loss: 2.302656
3479.83113861084
[11,  8000] loss: 2.302528
3493.006678342819
[11,  8500] loss: 2.302664
3506.0802142620087
[11,  9000] loss: 2.302611
3519.1191568374634
[11,  9500] loss: 2.302629
3532.065306186676
[11, 10000] loss: 2.302561
3545.1112530231476
[11, 10500] loss: 2.302550
3558.12268781662
[11, 11000] loss: 2.302605
3571.078952550888
[11, 11500] loss: 2.302566
3584.222606420517
[11, 12000] loss: 2.302692
3597.4213614463806
[11, 12500] loss: 2.302590
3610.640285015106
Epoch [11] loss: 7210032.704973
[12,   500] loss: 2.302531
3623.9012274742126
[12,  1000] loss: 2.302525
3636.907459497452
[12,  1500] loss: 2.302640
3650.042412519455
[12,  2000] loss: 2.302658
3663.192579269409
[12,  2500] loss: 2.302603
3676.2246549129486
[12,  3000] loss: 2.302494
3689.4415204524994
[12,  3500] loss: 2.302623
3702.5484778881073
[12,  4000] loss: 2.302566
3715.6310937404633
[12,  4500] loss: 2.302551
3728.728088617325
[12,  5000] loss: 2.302613
3741.771000623703
[12,  5500] loss: 2.302587
3754.7497680187225
[12,  6000] loss: 2.302600
3767.842945575714
[12,  6500] loss: 2.302674
3780.959869146347
[12,  7000] loss: 2.302623
3794.1159155368805
[12,  7500] loss: 2.302521
3807.175683259964
[12,  8000] loss: 2.302601
3820.1103615760803
[12,  8500] loss: 2.302611
3833.020553588867
[12,  9000] loss: 2.302558
3845.9990792274475
[12,  9500] loss: 2.302649
3859.120352268219
[12, 10000] loss: 2.302525
3872.278359889984
[12, 10500] loss: 2.302572
3885.418837070465
[12, 11000] loss: 2.302712
3898.4629209041595
[12, 11500] loss: 2.302648
3926.6871967315674
[12, 12000] loss: 2.302649
3939.758490562439
[12, 12500] loss: 2.302663
3952.792251586914
Epoch [12] loss: 7210002.630882
[13,   500] loss: 2.302516
3968.81005859375
[13,  1000] loss: 2.302603
3981.960826396942
[13,  1500] loss: 2.302538
3994.950763940811
[13,  2000] loss: 2.302557
4007.99768614769
[13,  2500] loss: 2.302693
4021.0166087150574
[13,  3000] loss: 2.302657
4034.058357477188
[13,  3500] loss: 2.302374
4047.0783858299255
[13,  4000] loss: 2.302545
4060.041419506073
[13,  4500] loss: 2.302760
4073.0330035686493
[13,  5000] loss: 2.302662
4086.002034664154
[13,  5500] loss: 2.302578
4099.018822193146
[13,  6000] loss: 2.302689
4112.140560388565
[13,  6500] loss: 2.302553
4125.289698600769
[13,  7000] loss: 2.302505
4138.358619213104
[13,  7500] loss: 2.302622
4151.505194425583
[13,  8000] loss: 2.302724
4164.697061061859
[13,  8500] loss: 2.302550
4177.664127588272
[13,  9000] loss: 2.302590
4191.288998365402
[13,  9500] loss: 2.302591
4204.638962507248
[13, 10000] loss: 2.302696
4217.7202179431915
[13, 10500] loss: 2.302598
4230.657743215561
[13, 11000] loss: 2.302582
4243.625364542007
[13, 11500] loss: 2.302621
4256.652946472168
[13, 12000] loss: 2.302644
4269.609679937363
[13, 12500] loss: 2.302589
4282.594110488892
Epoch [13] loss: 7210039.065485
[14,   500] loss: 2.302376
4295.733821392059
[14,  1000] loss: 2.302738
4308.86757850647
[14,  1500] loss: 2.302613
4321.9371609687805
[14,  2000] loss: 2.302701
4335.061503410339
[14,  2500] loss: 2.302636
4348.224410533905
[14,  3000] loss: 2.302624
4361.297659158707
[14,  3500] loss: 2.302552
4374.460789680481
[14,  4000] loss: 2.302730
4387.487127780914
[14,  4500] loss: 2.302589
4400.722284078598
[14,  5000] loss: 2.302644
4413.770329713821
[14,  5500] loss: 2.302641
4426.91845703125
[14,  6000] loss: 2.302544
4440.165278196335
[14,  6500] loss: 2.302657
4453.317213535309
[14,  7000] loss: 2.302570
4466.427976369858
[14,  7500] loss: 2.302644
4479.513530492783
[14,  8000] loss: 2.302614
4492.6260867118835
[14,  8500] loss: 2.302669
4505.626127243042
[14,  9000] loss: 2.302576
4518.656203985214
[14,  9500] loss: 2.302705
4531.688923358917
[14, 10000] loss: 2.302604
4544.85040307045
[14, 10500] loss: 2.302643
4557.916299581528
[14, 11000] loss: 2.302522
4570.908072948456
[14, 11500] loss: 2.302630
4583.858012676239
[14, 12000] loss: 2.302618
4596.898534536362
[14, 12500] loss: 2.302644
4609.881194114685
Epoch [14] loss: 7210120.282985
[15,   500] loss: 2.302531
4623.001102209091
[15,  1000] loss: 2.302658
4635.926475286484
[15,  1500] loss: 2.302713
4648.9000725746155
[15,  2000] loss: 2.302559
4661.902804136276
[15,  2500] loss: 2.302657
4674.920624732971
[15,  3000] loss: 2.302681
4687.874182462692
[15,  3500] loss: 2.302625
4700.8829889297485
[15,  4000] loss: 2.302633
4713.913646936417
[15,  4500] loss: 2.302623
4726.908272743225
[15,  5000] loss: 2.302650
4740.02942442894
[15,  5500] loss: 2.302594
4753.116163492203
[15,  6000] loss: 2.302471
4766.2977414131165
[15,  6500] loss: 2.302688
4779.442432165146
[15,  7000] loss: 2.302643
4792.997958183289
[15,  7500] loss: 2.302552
4806.203738689423
[15,  8000] loss: 2.302692
4819.670909166336
[15,  8500] loss: 2.302567
4832.689915895462
[15,  9000] loss: 2.302670
4845.9315984249115
[15,  9500] loss: 2.302635
4859.021023035049
[15, 10000] loss: 2.302605
4872.0838532447815
[15, 10500] loss: 2.302568
4885.147514343262
[15, 11000] loss: 2.302669
4898.1900935173035
[15, 11500] loss: 2.302631
4911.193573236465
[15, 12000] loss: 2.302575
4924.152914524078
[15, 12500] loss: 2.302687
4937.143342733383
Epoch [15] loss: 7210109.234709
[16,   500] loss: 2.302516
4950.395389556885
[16,  1000] loss: 2.302633
4963.340623855591
[16,  1500] loss: 2.302711
4976.407482624054
[16,  2000] loss: 2.302618
4989.364520072937
[16,  2500] loss: 2.302513
5002.455533742905
[16,  3000] loss: 2.302540
5015.417544364929
[16,  3500] loss: 2.302621
5028.491909503937
[16,  4000] loss: 2.302610
5041.426038265228
[16,  4500] loss: 2.302558
5054.319016218185
[16,  5000] loss: 2.302510
5067.292473554611
[16,  5500] loss: 2.302499
5080.509998559952
[16,  6000] loss: 2.302642
5093.560013532639
[16,  6500] loss: 2.302440
5106.652317762375
[16,  7000] loss: 2.302665
5119.679423332214
[16,  7500] loss: 2.302687
5132.7872087955475
[16,  8000] loss: 2.302643
5145.882165431976
[16,  8500] loss: 2.302521
5158.8679621219635
[16,  9000] loss: 2.302585
5171.972330093384
[16,  9500] loss: 2.302553
5185.263036727905
[16, 10000] loss: 2.302648
5198.385593175888
[16, 10500] loss: 2.302657
5211.55454158783
[16, 11000] loss: 2.302579
5224.619000196457
[16, 11500] loss: 2.302642
5237.619285821915
[16, 12000] loss: 2.302594
5250.62640786171
[16, 12500] loss: 2.302614
5263.60046339035
Epoch [16] loss: 7209993.261476
[17,   500] loss: 2.302640
5276.641308546066
[17,  1000] loss: 2.302562
5289.563360214233
[17,  1500] loss: 2.302728
5302.474558830261
[17,  2000] loss: 2.302617
5315.543855190277
[17,  2500] loss: 2.302655
5328.548657417297
[17,  3000] loss: 2.302555
5341.548004150391
[17,  3500] loss: 2.302532
5354.517378807068
[17,  4000] loss: 2.302648
5367.490425348282
[17,  4500] loss: 2.302629
5380.638009786606
[17,  5000] loss: 2.302611
5393.784505605698
[17,  5500] loss: 2.302511
5407.001670837402
[17,  6000] loss: 2.302588
5420.191223621368
[17,  6500] loss: 2.302731
5433.114781856537
[17,  7000] loss: 2.302603
5446.1683077812195
[17,  7500] loss: 2.302542
5459.235770702362
[17,  8000] loss: 2.302548
5472.284205913544
[17,  8500] loss: 2.302569
5485.399813175201
[17,  9000] loss: 2.302618
5498.519389629364
[17,  9500] loss: 2.302574
5511.440774440765
[17, 10000] loss: 2.302611
5524.492263555527
[17, 10500] loss: 2.302556
5537.565908908844
[17, 11000] loss: 2.302613
5550.618403196335
[17, 11500] loss: 2.302626
5563.650567770004
[17, 12000] loss: 2.302626
5576.650587320328
[17, 12500] loss: 2.302541
5589.711819648743
Epoch [17] loss: 7210065.079197
[18,   500] loss: 2.302668
5602.875388145447
[18,  1000] loss: 2.302608
5615.897509098053
[18,  1500] loss: 2.302632
5628.876342058182
[18,  2000] loss: 2.302622
5641.863512039185
[18,  2500] loss: 2.302575
5654.809821605682
[18,  3000] loss: 2.302625
5667.829770565033
[18,  3500] loss: 2.302620
5680.892461299896
[18,  4000] loss: 2.302556
5693.873737335205
[18,  4500] loss: 2.302627
5706.900914907455
[18,  5000] loss: 2.302489
5719.980971336365
[18,  5500] loss: 2.302532
5733.083251237869
[18,  6000] loss: 2.302676
5746.198717355728
[18,  6500] loss: 2.302606
5759.227607727051
[18,  7000] loss: 2.302611
5772.34704875946
[18,  7500] loss: 2.302617
5785.522745370865
[18,  8000] loss: 2.302664
5798.661406993866
[18,  8500] loss: 2.302623
5811.675466775894
[18,  9000] loss: 2.302495
5824.615087270737
[18,  9500] loss: 2.302462
5837.549183607101
[18, 10000] loss: 2.302563
5850.461426734924
[18, 10500] loss: 2.302729
5863.481176137924
[18, 11000] loss: 2.302522
5876.599944353104
[18, 11500] loss: 2.302674
5889.688649177551
[18, 12000] loss: 2.302641
5902.720384597778
[18, 12500] loss: 2.302539
5915.6348032951355
Epoch [18] loss: 7210036.276763
[19,   500] loss: 2.302704
5928.773150682449
[19,  1000] loss: 2.302632
5941.782117605209
[19,  1500] loss: 2.302538
5954.769599437714
[19,  2000] loss: 2.302609
5967.752743005753
[19,  2500] loss: 2.302562
5980.709578514099
[19,  3000] loss: 2.302679
5993.975916862488
[19,  3500] loss: 2.302612
6007.072315692902
[19,  4000] loss: 2.302709
6020.123802661896
[19,  4500] loss: 2.302576
6033.164886474609
[19,  5000] loss: 2.302666
6046.432571649551
[19,  5500] loss: 2.302530
6059.677846908569
[19,  6000] loss: 2.302633
6072.907166719437
[19,  6500] loss: 2.302529
6086.005488157272
[19,  7000] loss: 2.302620
6099.015232801437
[19,  7500] loss: 2.302673
6112.054079294205
[19,  8000] loss: 2.302661
6124.992150306702
[19,  8500] loss: 2.302533
6137.809067964554
[19,  9000] loss: 2.302588
6150.73388838768
[19,  9500] loss: 2.302637
6163.871937513351
[19, 10000] loss: 2.302626
6176.937670707703
[19, 10500] loss: 2.302620
6190.045616149902
[19, 11000] loss: 2.302660
6203.12452173233
[19, 11500] loss: 2.302534
6216.244973659515
[19, 12000] loss: 2.302693
6229.198604106903
[19, 12500] loss: 2.302632
6242.254975318909
Epoch [19] loss: 7210043.533432
[20,   500] loss: 2.302556
6255.349654197693
[20,  1000] loss: 2.302590
6268.389701366425
[20,  1500] loss: 2.302629
6281.41524720192
[20,  2000] loss: 2.302527
6294.4185473918915
[20,  2500] loss: 2.302537
6307.391849517822
[20,  3000] loss: 2.302608
6320.499533176422
[20,  3500] loss: 2.302659
6333.560257673264
[20,  4000] loss: 2.302702
6346.661925792694
[20,  4500] loss: 2.302565
6359.730374097824
[20,  5000] loss: 2.302540
6372.855744361877
[20,  5500] loss: 2.302596
6385.855371236801
[20,  6000] loss: 2.302622
6398.811996221542
[20,  6500] loss: 2.302651
6411.841150522232
[20,  7000] loss: 2.302621
6424.747820854187
[20,  7500] loss: 2.302569
6437.595588922501
[20,  8000] loss: 2.302610
6450.517194271088
[20,  8500] loss: 2.302381
6463.624172449112
[20,  9000] loss: 2.302689
6476.672130584717
[20,  9500] loss: 2.302585
6489.713011264801
[20, 10000] loss: 2.302616
6502.823390245438
[20, 10500] loss: 2.302584
6515.923745393753
[20, 11000] loss: 2.302687
6528.868701934814
[20, 11500] loss: 2.302558
6541.874640703201
[20, 12000] loss: 2.302581
6554.865568637848
[20, 12500] loss: 2.302566
6567.847647428513
Epoch [20] loss: 7209999.903927
Finished Training
Saving model to /data/s4091221/trained-models/wide_resnet50_22020-02-26 05:29:50.313111
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[-5.1413e-03, -1.3109e-04,  4.7738e-05, -3.7908e-04,  7.2888e-04,
          6.2172e-03,  3.6866e-03,  2.4605e-03, -7.5372e-03,  4.7651e-05],
        [-5.1413e-03, -1.3109e-04,  4.7738e-05, -3.7908e-04,  7.2888e-04,
          6.2172e-03,  3.6866e-03,  2.4605e-03, -7.5372e-03,  4.7651e-05],
        [-5.1413e-03, -1.3109e-04,  4.7738e-05, -3.7908e-04,  7.2888e-04,
          6.2172e-03,  3.6866e-03,  2.4605e-03, -7.5372e-03,  4.7651e-05],
        [-5.1413e-03, -1.3109e-04,  4.7738e-05, -3.7908e-04,  7.2888e-04,
          6.2172e-03,  3.6866e-03,  2.4605e-03, -7.5372e-03,  4.7651e-05]],
       device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:    dog   dog   dog   dog
Accuracy of the network on the 4000.0 test images: 10 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': True, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.05, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 1}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.05, learning_rate_scheduler=True, load_from_memory=False, model_archi=34, momentum=0, normalise=False, optimizer_choice=1, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
 deer truck  ship   cat
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model wide_resnet50_2 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model wide_resnet50_2 Reshaped
Number of parameters: 66854730 
Sending model to GPU
Learning Rate: 0.05, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.sgd.SGD'> Optimizer
Starting Training at 1582691474.2388484
[1,   500] loss: 2.634602
11.569801330566406
[1,  1000] loss: 2.596387
22.9928457736969
[1,  1500] loss: 2.600156
34.574541091918945
[1,  2000] loss: 2.645836
46.112311363220215
[1,  2500] loss: 2.621962
57.63683772087097
[1,  3000] loss: 2.647573
69.16935729980469
[1,  3500] loss: 2.597454
80.64934134483337
[1,  4000] loss: 2.634244
92.23925733566284
[1,  4500] loss: 2.598974
103.89102387428284
[1,  5000] loss: 2.628247
115.41615700721741
[1,  5500] loss: 2.614876
126.94877004623413
[1,  6000] loss: 2.642421
138.4891893863678
[1,  6500] loss: 2.607371
149.99378752708435
[1,  7000] loss: 2.616455
161.46849703788757
[1,  7500] loss: 2.641335
172.99395656585693
[1,  8000] loss: 2.609448
184.57840585708618
[1,  8500] loss: 2.612869
196.12145900726318
[1,  9000] loss: 2.617069
207.73674201965332
[1,  9500] loss: 2.599452
219.34793615341187
[1, 10000] loss: 2.643554
230.78361296653748
[1, 10500] loss: 2.624217
242.25642466545105
[1, 11000] loss: 2.578856
253.88792490959167
[1, 11500] loss: 2.648878
265.4137177467346
[1, 12000] loss: 2.624298
276.9385278224945
[1, 12500] loss: 2.641644
288.42599630355835
Epoch [1] loss: 8197559.377097
/home/s4091221/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[2,   500] loss: 2.620801
300.0790185928345
[2,  1000] loss: 2.628235
311.5433158874512
[2,  1500] loss: 2.617086
323.152649641037
[2,  2000] loss: 2.619157
334.58787775039673
[2,  2500] loss: 2.615350
346.10082507133484
[2,  3000] loss: 2.637185
357.70655131340027
[2,  3500] loss: 2.632870
369.3564941883087
[2,  4000] loss: 2.612878
380.9392640590668
[2,  4500] loss: 2.630491
392.371750831604
[2,  5000] loss: 2.627486
403.8205225467682
[2,  5500] loss: 2.628430
415.3744697570801
[2,  6000] loss: 2.602437
426.8431077003479
[2,  6500] loss: 2.621339
438.4325590133667
[2,  7000] loss: 2.624843
449.8880605697632
[2,  7500] loss: 2.598299
461.38684010505676
[2,  8000] loss: 2.603309
472.9543089866638
[2,  8500] loss: 2.619319
484.5132555961609
[2,  9000] loss: 2.619836
496.0459394454956
[2,  9500] loss: 2.608542
507.54629492759705
[2, 10000] loss: 2.645677
518.9554200172424
[2, 10500] loss: 2.606135
530.5046303272247
[2, 11000] loss: 2.640355
542.2467021942139
[2, 11500] loss: 2.649871
553.9935224056244
[2, 12000] loss: 2.628422
565.5679731369019
[2, 12500] loss: 2.616283
577.147433757782
Epoch [2] loss: 8216948.721428
[3,   500] loss: 2.611681
588.8254601955414
[3,  1000] loss: 2.629997
600.3280355930328
[3,  1500] loss: 2.634065
611.7879807949066
[3,  2000] loss: 2.634072
623.3772611618042
[3,  2500] loss: 2.639334
634.9431893825531
[3,  3000] loss: 2.626679
646.5457549095154
[3,  3500] loss: 2.621908
658.1664795875549
[3,  4000] loss: 2.637221
669.7270176410675
[3,  4500] loss: 2.638340
681.3565516471863
[3,  5000] loss: 2.632821
693.0364162921906
[3,  5500] loss: 2.590662
704.6002902984619
[3,  6000] loss: 2.588285
716.1391158103943
[3,  6500] loss: 2.634287
727.7404329776764
[3,  7000] loss: 2.609619
739.2941637039185
[3,  7500] loss: 2.617927
750.8185405731201
[3,  8000] loss: 2.641157
762.3314023017883
[3,  8500] loss: 2.647601
773.8483965396881
[3,  9000] loss: 2.634027
785.2422208786011
[3,  9500] loss: 2.597077
796.7359793186188
[3, 10000] loss: 2.621532
808.2767262458801
[3, 10500] loss: 2.585851
819.8014359474182
[3, 11000] loss: 2.626220
831.5709414482117
[3, 11500] loss: 2.624391
843.1039869785309
[3, 12000] loss: 2.615276
854.6173026561737
[3, 12500] loss: 2.648409
866.2689144611359
Epoch [3] loss: 8205866.357664
[4,   500] loss: 2.606180
877.9557709693909
[4,  1000] loss: 2.614888
889.5162460803986
[4,  1500] loss: 2.644905
901.0863325595856
[4,  2000] loss: 2.624801
912.5993688106537
[4,  2500] loss: 2.618439
924.1306369304657
[4,  3000] loss: 2.621975
935.5979890823364
[4,  3500] loss: 2.633260
947.160103559494
[4,  4000] loss: 2.664515
958.7497684955597
[4,  4500] loss: 2.628693
970.3072469234467
[4,  5000] loss: 2.592674
981.8332183361053
[4,  5500] loss: 2.651290
993.4861614704132
[4,  6000] loss: 2.648238
1005.0474667549133
[4,  6500] loss: 2.588662
1016.5677011013031
[4,  7000] loss: 2.611609
1028.0802793502808
[4,  7500] loss: 2.619442
1039.615003824234
[4,  8000] loss: 2.613945
1051.0813419818878
[4,  8500] loss: 2.635084
1062.6054441928864
[4,  9000] loss: 2.614568
1074.1082224845886
[4,  9500] loss: 2.605790
1085.582286119461
[4, 10000] loss: 2.621684
1097.1393735408783
[4, 10500] loss: 2.631055
1108.600960969925
[4, 11000] loss: 2.611951
1120.1060202121735
[4, 11500] loss: 2.641297
1132.0173890590668
[4, 12000] loss: 2.613328
1143.9275574684143
[4, 12500] loss: 2.627454
1155.6679227352142
Epoch [4] loss: 8212286.748799
[5,   500] loss: 2.652133
1167.5987684726715
[5,  1000] loss: 2.624729
1179.179006099701
[5,  1500] loss: 2.643220
1190.8023693561554
[5,  2000] loss: 2.630691
1202.2792246341705
[5,  2500] loss: 2.604690
1213.7782027721405
[5,  3000] loss: 2.610105
1225.237741470337
[5,  3500] loss: 2.624390
1236.7199993133545
[5,  4000] loss: 2.601569
1248.2282366752625
[5,  4500] loss: 2.618649
1259.6994001865387
[5,  5000] loss: 2.653483
1271.1784410476685
[5,  5500] loss: 2.665689
1282.7108509540558
[5,  6000] loss: 2.615295
1294.2667291164398
[5,  6500] loss: 2.618364
1305.858969449997
[5,  7000] loss: 2.616493
1317.5377459526062
[5,  7500] loss: 2.626917
1329.1614499092102
[5,  8000] loss: 2.612377
1340.8887605667114
[5,  8500] loss: 2.644192
1352.6844747066498
[5,  9000] loss: 2.617074
1364.2666087150574
[5,  9500] loss: 2.636535
1375.917979478836
[5, 10000] loss: 2.635845
1387.4411704540253
[5, 10500] loss: 2.618014
1398.9709277153015
[5, 11000] loss: 2.622773
1410.5021979808807
[5, 11500] loss: 2.654736
1422.0918390750885
[5, 12000] loss: 2.605325
1433.6902015209198
[5, 12500] loss: 2.594318
1445.2816967964172
Epoch [5] loss: 8227848.603737
[6,   500] loss: 2.631759
1457.0158607959747
[6,  1000] loss: 2.658406
1468.7425439357758
[6,  1500] loss: 2.624355
1480.3618564605713
[6,  2000] loss: 2.596154
1491.9927024841309
[6,  2500] loss: 2.619487
1503.577267408371
[6,  3000] loss: 2.598057
1515.1284713745117
[6,  3500] loss: 2.607404
1526.6355764865875
[6,  4000] loss: 2.622164
1538.218082666397
[6,  4500] loss: 2.619167
1549.6453750133514
[6,  5000] loss: 2.652162
1561.0355327129364
[6,  5500] loss: 2.643365
1572.4328718185425
[6,  6000] loss: 2.623047
1583.9290881156921
[6,  6500] loss: 2.637836
1595.4824810028076
[6,  7000] loss: 2.617365
1607.046992778778
[6,  7500] loss: 2.642862
1618.5693018436432
[6,  8000] loss: 2.618226
1630.131953239441
[6,  8500] loss: 2.618399
1641.7707133293152
[6,  9000] loss: 2.632137
1653.3747568130493
[6,  9500] loss: 2.604390
1664.924171924591
[6, 10000] loss: 2.643101
1676.3781549930573
[6, 10500] loss: 2.624332
1687.9594705104828
[6, 11000] loss: 2.630708
1699.483214855194
[6, 11500] loss: 2.616298
1711.0854785442352
[6, 12000] loss: 2.633796
1722.6552150249481
[6, 12500] loss: 2.594162
1734.5564849376678
Epoch [6] loss: 8213940.170997
[7,   500] loss: 2.614298
1746.340143918991
[7,  1000] loss: 2.630361
1758.0314712524414
[7,  1500] loss: 2.606332
1769.7107300758362
[7,  2000] loss: 2.649214
1781.342148065567
[7,  2500] loss: 2.632086
1792.9683656692505
[7,  3000] loss: 2.656464
1804.5612947940826
[7,  3500] loss: 2.614262
1816.1142375469208
[7,  4000] loss: 2.629413
1827.8409705162048
[7,  4500] loss: 2.636188
1839.5548753738403
[7,  5000] loss: 2.647923
1851.2508399486542
[7,  5500] loss: 2.619645
1862.871081829071
[7,  6000] loss: 2.659860
1874.4490523338318
[7,  6500] loss: 2.603649
1886.0700211524963
[7,  7000] loss: 2.613481
1897.706478357315
[7,  7500] loss: 2.614634
1909.2245557308197
[7,  8000] loss: 2.617022
1920.777856349945
[7,  8500] loss: 2.625374
1932.256058216095
[7,  9000] loss: 2.614255
1943.8058502674103
[7,  9500] loss: 2.639190
1955.3801400661469
[7, 10000] loss: 2.652273
1966.9380524158478
[7, 10500] loss: 2.586189
1978.5510895252228
[7, 11000] loss: 2.611706
1990.1003756523132
[7, 11500] loss: 2.611044
2001.6672883033752
[7, 12000] loss: 2.621655
2013.2362415790558
[7, 12500] loss: 2.661589
2024.7200593948364
Epoch [7] loss: 8224696.681462
[8,   500] loss: 2.655279
2036.287296295166
[8,  1000] loss: 2.626935
2047.759155511856
[8,  1500] loss: 2.638597
2059.281711101532
[8,  2000] loss: 2.612127
2070.8071205615997
[8,  2500] loss: 2.627859
2082.357816696167
[8,  3000] loss: 2.605334
2093.9020705223083
[8,  3500] loss: 2.660937
2105.413616657257
[8,  4000] loss: 2.630676
2117.015045642853
[8,  4500] loss: 2.621959
2128.5017161369324
[8,  5000] loss: 2.607158
2140.0802578926086
[8,  5500] loss: 2.628877
2151.6050584316254
[8,  6000] loss: 2.622748
2163.177045583725
[8,  6500] loss: 2.613689
2174.6739881038666
[8,  7000] loss: 2.620549
2186.1822917461395
[8,  7500] loss: 2.652482
2197.676680803299
[8,  8000] loss: 2.638315
2209.344284057617
[8,  8500] loss: 2.630426
2220.926616668701
[8,  9000] loss: 2.614378
2232.514348268509
[8,  9500] loss: 2.626457
2244.1592915058136
[8, 10000] loss: 2.620265
2255.7770760059357
[8, 10500] loss: 2.607099
2267.45467376709
[8, 11000] loss: 2.617359
2279.1953539848328
[8, 11500] loss: 2.606662
2290.852557182312
[8, 12000] loss: 2.634939
2302.43604183197
[8, 12500] loss: 2.599098
2314.0616705417633
Epoch [8] loss: 8222569.158133
[9,   500] loss: 2.621500
2325.7570266723633
[9,  1000] loss: 2.630613
2337.6713943481445
[9,  1500] loss: 2.641960
2349.428522825241
[9,  2000] loss: 2.628232
2361.01575422287
[9,  2500] loss: 2.631238
2372.4174015522003
[9,  3000] loss: 2.596523
2384.0129022598267
[9,  3500] loss: 2.650125
2395.568404197693
[9,  4000] loss: 2.601082
2407.084356069565
[9,  4500] loss: 2.623031
2418.6353919506073
[9,  5000] loss: 2.606106
2430.2195432186127
[9,  5500] loss: 2.615351
2441.7990293502808
[9,  6000] loss: 2.631397
2453.2708089351654
[9,  6500] loss: 2.634106
2464.7776441574097
[9,  7000] loss: 2.610004
2476.2905724048615
[9,  7500] loss: 2.641196
2487.8019721508026
[9,  8000] loss: 2.604611
2499.2939896583557
[9,  8500] loss: 2.615612
2510.821278333664
[9,  9000] loss: 2.631897
2522.2835054397583
[9,  9500] loss: 2.628752
2533.671101331711
[9, 10000] loss: 2.629247
2545.229143857956
[9, 10500] loss: 2.645048
2556.6581740379333
[9, 11000] loss: 2.611211
2568.1019275188446
[9, 11500] loss: 2.608431
2579.563245534897
[9, 12000] loss: 2.625069
2591.130742788315
[9, 12500] loss: 2.616801
2602.7425701618195
Epoch [9] loss: 8220110.266339
[10,   500] loss: 2.613473
2614.5584967136383
[10,  1000] loss: 2.595037
2626.1039469242096
[10,  1500] loss: 2.632311
2637.8227400779724
[10,  2000] loss: 2.620660
2649.422516107559
[10,  2500] loss: 2.613186
2661.1045804023743
[10,  3000] loss: 2.628053
2672.7897198200226
[10,  3500] loss: 2.638022
2684.5036914348602
[10,  4000] loss: 2.603273
2696.238281726837
[10,  4500] loss: 2.597991
2707.875829219818
[10,  5000] loss: 2.654475
2719.462587118149
[10,  5500] loss: 2.624377
2730.9770560264587
[10,  6000] loss: 2.614802
2742.430700302124
[10,  6500] loss: 2.645058
2753.8574447631836
[10,  7000] loss: 2.588288
2765.3091831207275
[10,  7500] loss: 2.609649
2776.7848036289215
[10,  8000] loss: 2.624682
2788.3782217502594
[10,  8500] loss: 2.609372
2799.965455532074
[10,  9000] loss: 2.650775
2811.6249470710754
[10,  9500] loss: 2.612848
2823.2178881168365
[10, 10000] loss: 2.615263
2834.7332112789154
[10, 10500] loss: 2.639169
2846.2516746520996
[10, 11000] loss: 2.632756
2857.77788066864
[10, 11500] loss: 2.619593
2869.3375680446625
[10, 12000] loss: 2.627865
2880.833838224411
[10, 12500] loss: 2.658381
2892.338973760605
Epoch [10] loss: 8205166.478431
[11,   500] loss: 2.636817
2903.988983631134
[11,  1000] loss: 2.613205
2915.588723182678
[11,  1500] loss: 2.621505
2927.2751824855804
[11,  2000] loss: 2.637858
2939.0931584835052
[11,  2500] loss: 2.639102
2950.8652954101562
[11,  3000] loss: 2.625648
2962.6600563526154
[11,  3500] loss: 2.605530
2974.200495004654
[11,  4000] loss: 2.599593
2985.82253241539
[11,  4500] loss: 2.629376
2997.4796798229218
[11,  5000] loss: 2.644856
3009.1542251110077
[11,  5500] loss: 2.626673
3020.8951272964478
[11,  6000] loss: 2.648344
3032.865235567093
[11,  6500] loss: 2.619794
3044.479291677475
[11,  7000] loss: 2.641397
3056.1792917251587
[11,  7500] loss: 2.631713
3067.7989132404327
[11,  8000] loss: 2.632161
3079.4183809757233
[11,  8500] loss: 2.622636
3090.9667072296143
[11,  9000] loss: 2.640074
3102.4996943473816
[11,  9500] loss: 2.627803
3114.1490852832794
[11, 10000] loss: 2.620569
3125.75999212265
[11, 10500] loss: 2.606405
3137.2712302207947
[11, 11000] loss: 2.643478
3148.8466458320618
[11, 11500] loss: 2.630139
3160.3768615722656
[11, 12000] loss: 2.626200
3172.017100095749
[11, 12500] loss: 2.638355
3183.563933134079
Epoch [11] loss: 8231822.206084
[12,   500] loss: 2.635475
3195.2088963985443
[12,  1000] loss: 2.640355
3206.7674267292023
[12,  1500] loss: 2.613103
3218.3132798671722
[12,  2000] loss: 2.658031
3230.061206102371
[12,  2500] loss: 2.621963
3241.932863473892
[12,  3000] loss: 2.649386
3253.9042031764984
[12,  3500] loss: 2.643453
3265.894002199173
[12,  4000] loss: 2.600738
3277.792165517807
[12,  4500] loss: 2.647865
3289.711789369583
[12,  5000] loss: 2.650019
3301.6077013015747
[12,  5500] loss: 2.622831
3313.581404209137
[12,  6000] loss: 2.612237
3325.615772485733
[12,  6500] loss: 2.627432
3337.597840309143
[12,  7000] loss: 2.630605
3349.4973368644714
[12,  7500] loss: 2.620807
3361.3372757434845
[12,  8000] loss: 2.617426
3373.1798627376556
[12,  8500] loss: 2.654999
3385.0783042907715
[12,  9000] loss: 2.617278
3396.9001677036285
[12,  9500] loss: 2.623626
3408.7239830493927
[12, 10000] loss: 2.620805
3420.5548520088196
[12, 10500] loss: 2.603665
3432.299826860428
[12, 11000] loss: 2.603482
3443.9981989860535
[12, 11500] loss: 2.604750
3455.7540106773376
[12, 12000] loss: 2.579307
3467.6357016563416
[12, 12500] loss: 2.620892
3479.4995851516724
Epoch [12] loss: 8224949.995266
[13,   500] loss: 2.639080
3491.4763770103455
[13,  1000] loss: 2.618595
3503.1940157413483
[13,  1500] loss: 2.633875
3514.8925471305847
[13,  2000] loss: 2.612867
3526.6505587100983
[13,  2500] loss: 2.629203
3538.487197160721
[13,  3000] loss: 2.612782
3550.404839515686
[13,  3500] loss: 2.646699
3562.4577481746674
[13,  4000] loss: 2.583004
3574.311315059662
[13,  4500] loss: 2.664315
3586.1732318401337
[13,  5000] loss: 2.607912
3598.039872646332
[13,  5500] loss: 2.645704
3609.88521194458
[13,  6000] loss: 2.601017
3621.727920770645
[13,  6500] loss: 2.618906
3633.6549122333527
[13,  7000] loss: 2.623336
3645.595006465912
[13,  7500] loss: 2.600111
3657.5160760879517
[13,  8000] loss: 2.628519
3669.336796283722
[13,  8500] loss: 2.651270
3681.0916016101837
[13,  9000] loss: 2.631999
3692.8220772743225
[13,  9500] loss: 2.649802
3704.5459563732147
[13, 10000] loss: 2.622103
3716.3344008922577
[13, 10500] loss: 2.599633
3728.234799861908
[13, 11000] loss: 2.593567
3740.060243368149
[13, 11500] loss: 2.603319
3751.928564310074
[13, 12000] loss: 2.622770
3763.714548587799
[13, 12500] loss: 2.639451
3775.5444197654724
Epoch [13] loss: 8217015.551179
[14,   500] loss: 2.621886
3787.5038511753082
[14,  1000] loss: 2.601909
3799.3307485580444
[14,  1500] loss: 2.623265
3811.199957370758
[14,  2000] loss: 2.609082
3822.95543050766
[14,  2500] loss: 2.657577
3834.7175891399384
[14,  3000] loss: 2.608562
3846.490418434143
[14,  3500] loss: 2.613188
3858.358785867691
[14,  4000] loss: 2.608796
3871.8196170330048
[14,  4500] loss: 2.662657
3883.81178689003
[14,  5000] loss: 2.635560
3895.9571549892426
[14,  5500] loss: 2.633937
3907.975973367691
[14,  6000] loss: 2.612828
3920.3641521930695
[14,  6500] loss: 2.603709
3932.368264436722
[14,  7000] loss: 2.652314
3944.2861976623535
[14,  7500] loss: 2.635876
3956.23051404953
[14,  8000] loss: 2.583896
3967.992868900299
[14,  8500] loss: 2.619573
3979.8187115192413
[14,  9000] loss: 2.627837
3991.3533108234406
[14,  9500] loss: 2.609541
4002.915607690811
[14, 10000] loss: 2.620093
4014.5529401302338
[14, 10500] loss: 2.594654
4026.1759793758392
[14, 11000] loss: 2.611694
4037.694331884384
[14, 11500] loss: 2.658984
4049.251871109009
[14, 12000] loss: 2.602822
4060.7971062660217
[14, 12500] loss: 2.630445
4072.401288509369
Epoch [14] loss: 8208954.620219
[15,   500] loss: 2.645665
4086.0995903015137
[15,  1000] loss: 2.609626
4097.620884895325
[15,  1500] loss: 2.625123
4109.222108364105
[15,  2000] loss: 2.623729
4120.765733957291
[15,  2500] loss: 2.633205
4132.652992248535
[15,  3000] loss: 2.613248
4144.577268838882
[15,  3500] loss: 2.620353
4156.519626617432
[15,  4000] loss: 2.647598
4168.315565109253
[15,  4500] loss: 2.636973
4179.906713962555
[15,  5000] loss: 2.644919
4191.528602600098
[15,  5500] loss: 2.634539
4203.270776987076
[15,  6000] loss: 2.627023
4214.907172203064
[15,  6500] loss: 2.626349
4226.424368143082
[15,  7000] loss: 2.635140
4237.968664884567
[15,  7500] loss: 2.635652
4249.545422554016
[15,  8000] loss: 2.651891
4261.065857410431
[15,  8500] loss: 2.602799
4272.542495012283
[15,  9000] loss: 2.608679
4284.015627384186
[15,  9500] loss: 2.610215
4295.631892204285
[15, 10000] loss: 2.607661
4307.22940826416
[15, 10500] loss: 2.641961
4318.8380246162415
[15, 11000] loss: 2.595322
4330.478852272034
[15, 11500] loss: 2.628057
4342.122524261475
[15, 12000] loss: 2.608336
4353.823670864105
[15, 12500] loss: 2.619963
4365.460856437683
Epoch [15] loss: 8224809.891110
[16,   500] loss: 2.622886
4377.294648170471
[16,  1000] loss: 2.659311
4388.854316949844
[16,  1500] loss: 2.618600
4400.406373739243
[16,  2000] loss: 2.628832
4411.916811943054
[16,  2500] loss: 2.592727
4423.4266476631165
[16,  3000] loss: 2.595407
4435.033642053604
[16,  3500] loss: 2.612557
4446.601498365402
[16,  4000] loss: 2.646410
4458.269859075546
[16,  4500] loss: 2.649502
4469.791258096695
[16,  5000] loss: 2.612039
4481.316902637482
[16,  5500] loss: 2.639761
4492.923007011414
[16,  6000] loss: 2.628618
4504.569753646851
[16,  6500] loss: 2.611510
4516.139165878296
[16,  7000] loss: 2.637748
4527.712347269058
[16,  7500] loss: 2.630057
4539.3126339912415
[16,  8000] loss: 2.637638
4550.953939199448
[16,  8500] loss: 2.634796
4562.592847824097
[16,  9000] loss: 2.626246
4574.235955238342
[16,  9500] loss: 2.618880
4585.823357820511
[16, 10000] loss: 2.619497
4597.328728914261
[16, 10500] loss: 2.648406
4608.884167671204
[16, 11000] loss: 2.597186
4620.430046796799
[16, 11500] loss: 2.650543
4632.085634469986
[16, 12000] loss: 2.592669
4643.640280485153
[16, 12500] loss: 2.627561
4655.168314695358
Epoch [16] loss: 8225839.107334
[17,   500] loss: 2.618730
4667.281654834747
[17,  1000] loss: 2.642461
4678.946463346481
[17,  1500] loss: 2.607336
4690.544884443283
[17,  2000] loss: 2.624770
4702.125762701035
[17,  2500] loss: 2.616823
4713.820811033249
[17,  3000] loss: 2.642351
4725.334361076355
[17,  3500] loss: 2.625410
4737.368762254715
[17,  4000] loss: 2.597701
4749.338578224182
[17,  4500] loss: 2.647566
4761.122450828552
[17,  5000] loss: 2.628921
4772.736451625824
[17,  5500] loss: 2.602911
4784.24690079689
[17,  6000] loss: 2.610878
4795.757087469101
[17,  6500] loss: 2.614853
4807.446525096893
[17,  7000] loss: 2.623687
4819.104460716248
[17,  7500] loss: 2.637261
4830.7354118824005
[17,  8000] loss: 2.615186
4842.239957332611
[17,  8500] loss: 2.632988
4854.005958557129
[17,  9000] loss: 2.620368
4865.642958164215
[17,  9500] loss: 2.634039
4877.220086097717
[17, 10000] loss: 2.633557
4888.905690670013
[17, 10500] loss: 2.617276
4900.487186670303
[17, 11000] loss: 2.617206
4912.151154518127
[17, 11500] loss: 2.628333
4923.874986410141
[17, 12000] loss: 2.653535
4935.5171048641205
[17, 12500] loss: 2.597806
4947.135237455368
Epoch [17] loss: 8215752.936571
[18,   500] loss: 2.607658
4958.879622459412
[18,  1000] loss: 2.592133
4970.38636469841
[18,  1500] loss: 2.637228
4982.061143159866
[18,  2000] loss: 2.631263
4993.616580724716
[18,  2500] loss: 2.635421
5005.266930580139
[18,  3000] loss: 2.676790
5016.8699152469635
[18,  3500] loss: 2.601564
5028.479956865311
[18,  4000] loss: 2.605787
5040.118793487549
[18,  4500] loss: 2.629513
5051.778927564621
[18,  5000] loss: 2.641890
5063.3480660915375
[18,  5500] loss: 2.634684
5074.937719106674
[18,  6000] loss: 2.635948
5086.504548072815
[18,  6500] loss: 2.591299
5098.062026262283
[18,  7000] loss: 2.634297
5109.678753376007
[18,  7500] loss: 2.603351
5121.251809597015
[18,  8000] loss: 2.637415
5132.850415945053
[18,  8500] loss: 2.626322
5144.455798625946
[18,  9000] loss: 2.640648
5156.044360399246
[18,  9500] loss: 2.645283
5167.793523550034
[18, 10000] loss: 2.617039
5179.407064676285
[18, 10500] loss: 2.589275
5190.971994161606
[18, 11000] loss: 2.627088
5202.5122945308685
[18, 11500] loss: 2.584981
5214.024529695511
[18, 12000] loss: 2.646546
5225.59973692894
[18, 12500] loss: 2.624870
5237.252754211426
Epoch [18] loss: 8212359.927843
[19,   500] loss: 2.636648
5249.141255140305
[19,  1000] loss: 2.625395
5260.763721466064
[19,  1500] loss: 2.610881
5272.398600816727
[19,  2000] loss: 2.640442
5284.075753450394
[19,  2500] loss: 2.663089
5295.744419574738
[19,  3000] loss: 2.611698
5307.36990237236
[19,  3500] loss: 2.643424
5319.102213144302
[19,  4000] loss: 2.630643
5330.914711475372
[19,  4500] loss: 2.624742
5342.743382930756
[19,  5000] loss: 2.615354
5354.536025762558
[19,  5500] loss: 2.600990
5366.239216804504
[19,  6000] loss: 2.623121
5377.832110881805
[19,  6500] loss: 2.613574
5389.454414129257
[19,  7000] loss: 2.599951
5401.181823253632
[19,  7500] loss: 2.646735
5412.926802396774
[19,  8000] loss: 2.617903
5424.683630466461
[19,  8500] loss: 2.609083
5436.447272062302
[19,  9000] loss: 2.614311
5448.1865520477295
[19,  9500] loss: 2.615298
5459.987498521805
[19, 10000] loss: 2.624110
5471.461701154709
[19, 10500] loss: 2.628343
5482.976927995682
[19, 11000] loss: 2.621130
5494.676259279251
[19, 11500] loss: 2.601425
5506.2071306705475
[19, 12000] loss: 2.613454
5517.83146071434
[19, 12500] loss: 2.642320
5529.388050794601
Epoch [19] loss: 8216422.118496
[20,   500] loss: 2.652993
5540.993794202805
[20,  1000] loss: 2.626190
5552.48993730545
[20,  1500] loss: 2.617011
5564.056392431259
[20,  2000] loss: 2.618168
5575.731157064438
[20,  2500] loss: 2.600010
5587.405616044998
[20,  3000] loss: 2.634616
5598.982880592346
[20,  3500] loss: 2.642833
5610.538006544113
[20,  4000] loss: 2.621196
5622.119959592819
[20,  4500] loss: 2.615881
5633.796010971069
[20,  5000] loss: 2.642791
5645.43058347702
[20,  5500] loss: 2.625882
5657.088886022568
[20,  6000] loss: 2.615512
5668.614977121353
[20,  6500] loss: 2.608067
5680.088220834732
[20,  7000] loss: 2.636263
5691.610749006271
[20,  7500] loss: 2.616979
5703.151669740677
[20,  8000] loss: 2.617198
5714.777012586594
[20,  8500] loss: 2.615201
5726.34023976326
[20,  9000] loss: 2.593009
5737.886360645294
[20,  9500] loss: 2.610730
5749.4559597969055
[20, 10000] loss: 2.613913
5761.029834032059
[20, 10500] loss: 2.617567
5772.657312631607
[20, 11000] loss: 2.638346
5784.196433067322
[20, 11500] loss: 2.609891
5795.642930269241
[20, 12000] loss: 2.610345
5807.221167564392
[20, 12500] loss: 2.646002
5818.920793294907
Epoch [20] loss: 8212778.534313
Finished Training
Saving model to /data/s4091221/trained-models/wide_resnet50_22020-02-26 07:08:13.226483
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[ 1.4262, -0.1907,  0.0126,  0.3787,  0.6984,  0.9774,  0.1089,  0.4254,
         -0.8357, -0.8152],
        [ 1.9830,  0.0302, -0.5877, -0.0749, -0.3092,  0.2850,  0.3125, -1.0948,
          0.8615,  0.1787],
        [ 1.2860, -0.1573,  0.0901, -0.2066,  0.2532,  0.3538, -0.6857,  0.2537,
          0.5833, -0.7709],
        [ 1.6435, -0.8667, -1.3754,  0.8165, -0.1597,  0.8121,  0.2693, -0.3501,
          0.5037, -0.2661]], device='cuda:0', grad_fn=<AddmmBackward>)
Predicted:  plane plane plane plane
Accuracy of the network on the 4000.0 test images: 9 %
###########################################################################################################
{'use_cuda': True, 'peregrine': True, 'include_visuals': False, 'normalise': False, 'load_from_memory': False, 'pretrain': False, 'learning_rate_scheduler': False, 'batch_size': 4, 'workers': 2, 'model_archi': 34, 'trainset_size': 20000, 'epochs': 20, 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'optimizer_choice': 2}
Namespace(batch_size=4, epochs=20, include_visuals=False, learning_rate=0.001, learning_rate_scheduler=False, load_from_memory=False, model_archi=34, momentum=0, normalise=False, optimizer_choice=2, peregrine=True, pretrain=False, trainset_size=20000, use_cuda=True, weight_decay=0, workers=2)
Files already downloaded and verified
Files already downloaded and verified
cuda:0
12500
plane  frog   car   cat
['alexnet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'googlenet', 'inception_v3', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'wide_resnet101_2', 'wide_resnet50_2']
Model wide_resnet50_2 Loaded
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
Model wide_resnet50_2 Reshaped
Number of parameters: 66854730 
Sending model to GPU
Learning Rate: 0.001, Weight Decay: 0, Momentum: 0
Defined <class 'torch.optim.adam.Adam'> Optimizer
Starting Training at 1582697350.9858868
[1,   500] loss: 3.360035
20.259788036346436
[1,  1000] loss: 3.111641
40.56798243522644
[1,  1500] loss: 3.110244
61.12825965881348
[1,  2000] loss: 2.812674
81.53313136100769
[1,  2500] loss: 2.665864
101.44107675552368
[1,  3000] loss: 2.553393
121.36516427993774
[1,  3500] loss: 2.412577
141.2469344139099
[1,  4000] loss: 2.376558
161.10369610786438
[1,  4500] loss: 2.409374
181.0247142314911
[1,  5000] loss: 2.407260
200.78141832351685
[1,  5500] loss: 2.302597
220.6348693370819
[1,  6000] loss: 2.316706
240.7160747051239
[1,  6500] loss: 2.279852
260.78427028656006
[1,  7000] loss: 2.322898
280.84815430641174
[1,  7500] loss: 2.271336
300.94107365608215
[1,  8000] loss: 2.315818
321.0307376384735
[1,  8500] loss: 2.317421
341.5156545639038
[1,  9000] loss: 2.322232
361.8926317691803
[1,  9500] loss: 2.355517
382.4107336997986
[1, 10000] loss: 2.268618
402.6027488708496
[1, 10500] loss: 2.268259
422.43019104003906
[1, 11000] loss: 2.272096
442.388964176178
[1, 11500] loss: 2.279381
462.3287658691406
[1, 12000] loss: 2.284789
482.07233595848083
[1, 12500] loss: 2.313622
501.93789553642273
Epoch [1] loss: 7746290.954519
[2,   500] loss: 2.313125
522.0375463962555
[2,  1000] loss: 2.270409
542.2103519439697
[2,  1500] loss: 2.269774
562.2786769866943
[2,  2000] loss: 2.294675
582.4531426429749
[2,  2500] loss: 2.242024
602.6180913448334
[2,  3000] loss: 2.246000
622.4686710834503
[2,  3500] loss: 2.254144
642.5113916397095
[2,  4000] loss: 2.255465
662.7366809844971
[2,  4500] loss: 2.311152
683.0743730068207
[2,  5000] loss: 2.279861
702.953537940979
[2,  5500] loss: 2.280050
722.9405007362366
[2,  6000] loss: 2.274063
742.8561561107635
[2,  6500] loss: 2.234989
762.631936788559
[2,  7000] loss: 2.221664
782.5215229988098
[2,  7500] loss: 2.242715
802.5823636054993
[2,  8000] loss: 2.337192
822.5183634757996
[2,  8500] loss: 2.363263
842.7713871002197
[2,  9000] loss: 2.295591
862.540623664856
[2,  9500] loss: 2.277174
882.5715134143829
[2, 10000] loss: 2.271034
902.715824842453
[2, 10500] loss: 2.245666
922.7424190044403
[2, 11000] loss: 2.242545
942.5158433914185
[2, 11500] loss: 2.228746
962.4651732444763
[2, 12000] loss: 2.183588
982.5646393299103
[2, 12500] loss: 2.167510
1002.920530796051
Epoch [2] loss: 7093773.349620
[3,   500] loss: 2.155869
1023.2767241001129
[3,  1000] loss: 2.135143
1043.198884487152
[3,  1500] loss: 2.137362
1063.230967283249
[3,  2000] loss: 2.142965
1083.297001361847
[3,  2500] loss: 2.125413
1103.4199335575104
[3,  3000] loss: 2.139606
1123.5825934410095
[3,  3500] loss: 2.147674
1143.67982172966
[3,  4000] loss: 2.105039
1163.7284526824951
[3,  4500] loss: 2.162454
1183.750636100769
[3,  5000] loss: 2.178507
1203.82017827034
[3,  5500] loss: 2.148121
1223.671029806137
[3,  6000] loss: 2.160198
1243.7963106632233
[3,  6500] loss: 2.190981
1264.0245027542114
[3,  7000] loss: 2.128271
1284.5026304721832
[3,  7500] loss: 2.093820
1304.3774695396423
[3,  8000] loss: 2.090037
1324.2917466163635
[3,  8500] loss: 2.063716
1344.2095341682434
[3,  9000] loss: 2.065285
1364.1734335422516
[3,  9500] loss: 2.035565
1384.3271074295044
[3, 10000] loss: 2.072413
1404.155695438385
[3, 10500] loss: 2.210192
1423.9871258735657
[3, 11000] loss: 2.187403
1444.0170874595642
[3, 11500] loss: 2.257480
1464.1708688735962
[3, 12000] loss: 2.235454
1484.4919912815094
[3, 12500] loss: 2.205719
1504.6981761455536
Epoch [3] loss: 6708830.111780
[4,   500] loss: 2.201601
1524.765216588974
[4,  1000] loss: 2.197470
1544.9085533618927
[4,  1500] loss: 2.168775
1565.066784620285
[4,  2000] loss: 2.134105
1584.9860286712646
[4,  2500] loss: 2.137116
1604.6544530391693
[4,  3000] loss: 2.101831
1624.712378025055
[4,  3500] loss: 2.121468
1644.9014863967896
[4,  4000] loss: 2.098391
1665.2355422973633
[4,  4500] loss: 2.172046
1685.5114233493805
[4,  5000] loss: 2.248718
1705.3824982643127
[4,  5500] loss: 2.136636
1725.0834856033325
[4,  6000] loss: 2.106189
1745.3899984359741
[4,  6500] loss: 2.075180
1765.6444075107574
[4,  7000] loss: 2.083985
1785.5715305805206
[4,  7500] loss: 2.059470
1805.3917577266693
[4,  8000] loss: 2.054331
1825.392329454422
[4,  8500] loss: 2.095311
1845.1788713932037
[4,  9000] loss: 2.150540
1865.5672693252563
[4,  9500] loss: 2.149386
1886.003319978714
[4, 10000] loss: 2.075776
1905.9486210346222
[4, 10500] loss: 2.063762
1925.9074988365173
[4, 11000] loss: 2.066031
1945.7456238269806
[4, 11500] loss: 2.096333
1965.6708886623383
[4, 12000] loss: 2.141727
1985.7983758449554
[4, 12500] loss: 2.083652
2005.8411478996277
Epoch [4] loss: 6646720.942363
[5,   500] loss: 2.067057
2025.929186820984
[5,  1000] loss: 2.098889
2046.0307049751282
[5,  1500] loss: 2.071148
2066.077591896057
[5,  2000] loss: 2.056820
2086.1581988334656
[5,  2500] loss: 2.082660
2106.289642095566
[5,  3000] loss: 2.113127
2126.544186592102
[5,  3500] loss: 2.069899
2146.452032327652
[5,  4000] loss: 2.067246
2166.7914111614227
[5,  4500] loss: 2.098344
2187.041759252548
[5,  5000] loss: 2.114275
2207.2586085796356
[5,  5500] loss: 2.078363
2227.4387681484222
[5,  6000] loss: 2.100338
2247.3880383968353
[5,  6500] loss: 2.123319
2267.105805873871
[5,  7000] loss: 2.126931
2287.175277233124
[5,  7500] loss: 2.145280
2307.293361902237
[5,  8000] loss: 2.079419
2327.2402713298798
[5,  8500] loss: 2.050181
2347.4159650802612
[5,  9000] loss: 2.065924
2367.380210161209
[5,  9500] loss: 2.138104
2387.5254101753235
[5, 10000] loss: 2.172045
2407.530744075775
[5, 10500] loss: 2.136212
2427.4510860443115
[5, 11000] loss: 2.113244
2447.5863180160522
[5, 11500] loss: 2.097382
2468.258364677429
[5, 12000] loss: 2.130572
2488.56032204628
[5, 12500] loss: 2.101720
2508.625155687332
Epoch [5] loss: 6568056.483250
[6,   500] loss: 2.087078
2528.8806042671204
[6,  1000] loss: 2.082230
2548.985121011734
[6,  1500] loss: 2.091076
2568.949279308319
[6,  2000] loss: 2.047530
2589.0579283237457
[6,  2500] loss: 2.111399
2609.214210987091
[6,  3000] loss: 2.093555
2629.285546064377
[6,  3500] loss: 2.057597
2649.326477766037
[6,  4000] loss: 2.057417
2669.175573348999
[6,  4500] loss: 2.025407
2689.07745218277
[6,  5000] loss: 2.077751
2709.0466809272766
[6,  5500] loss: 2.090651
2729.2428085803986
[6,  6000] loss: 2.116392
2749.2835655212402
[6,  6500] loss: 2.098813
2769.3876581192017
[6,  7000] loss: 2.063551
2789.4843928813934
[6,  7500] loss: 2.077471
2809.351779460907
[6,  8000] loss: 2.072903
2829.2543206214905
[6,  8500] loss: 2.076092
2849.3478293418884
[6,  9000] loss: 2.022706
2869.294032096863
[6,  9500] loss: 2.043470
2889.197812795639
[6, 10000] loss: 2.085511
2909.1212706565857
[6, 10500] loss: 2.047330
2928.98970580101
[6, 11000] loss: 2.053783
2949.190132856369
[6, 11500] loss: 2.025237
2969.3315155506134
[6, 12000] loss: 1.991311
2989.495929479599
[6, 12500] loss: 2.000527
3009.450650215149
Epoch [6] loss: 6466250.922737
[7,   500] loss: 2.003907
3029.5071969032288
[7,  1000] loss: 1.999217
3049.4358117580414
[7,  1500] loss: 2.000876
3070.1420612335205
[7,  2000] loss: 1.984973
3090.434093236923
[7,  2500] loss: 1.989424
3110.2473821640015
[7,  3000] loss: 2.048945
3130.0100269317627
[7,  3500] loss: 1.983754
3150.32630109787
[7,  4000] loss: 2.017589
3170.2236275672913
[7,  4500] loss: 1.974934
3190.268831729889
[7,  5000] loss: 2.029814
3210.0609891414642
[7,  5500] loss: 2.002402
3229.907737493515
[7,  6000] loss: 1.989312
3249.778378009796
[7,  6500] loss: 1.984763
3269.749630689621
[7,  7000] loss: 2.002786
3289.5135474205017
[7,  7500] loss: 1.979424
3309.7111275196075
[7,  8000] loss: 1.953903
3330.01496386528
[7,  8500] loss: 1.955983
3350.214801311493
[7,  9000] loss: 1.980189
3370.1882429122925
[7,  9500] loss: 1.953423
3390.330510854721
[7, 10000] loss: 1.985890
3410.373843193054
[7, 10500] loss: 1.977183
3430.3130803108215
[7, 11000] loss: 1.961963
3450.2854974269867
[7, 11500] loss: 1.999034
3470.4360361099243
[7, 12000] loss: 1.937418
3490.4747853279114
[7, 12500] loss: 1.969971
3510.4159207344055
Epoch [7] loss: 6229480.725136
[8,   500] loss: 1.970664
3530.7554709911346
[8,  1000] loss: 1.995901
3550.7660830020905
[8,  1500] loss: 1.954053
3570.634111881256
[8,  2000] loss: 1.977029
3590.484874010086
[8,  2500] loss: 1.942323
3610.293982744217
[8,  3000] loss: 1.939396
3630.3585324287415
[8,  3500] loss: 1.922124
3650.449101448059
[8,  4000] loss: 1.952004
3671.0836353302
[8,  4500] loss: 1.988284
3691.6295671463013
[8,  5000] loss: 1.900828
3711.721223115921
[8,  5500] loss: 1.930772
3731.590642929077
[8,  6000] loss: 1.922623
3751.5138046741486
[8,  6500] loss: 1.911318
3771.4544970989227
[8,  7000] loss: 1.940074
3791.3531064987183
[8,  7500] loss: 1.926520
3811.403239250183
[8,  8000] loss: 1.938512
3831.3270330429077
[8,  8500] loss: 1.947396
3851.4679007530212
[8,  9000] loss: 1.914136
3871.7109336853027
[8,  9500] loss: 1.940166
3892.1050803661346
[8, 10000] loss: 1.934925
3917.120929002762
[8, 10500] loss: 1.899486
3937.232929944992
[8, 11000] loss: 1.879165
3957.5105283260345
[8, 11500] loss: 1.895687
3977.6714413166046
[8, 12000] loss: 2.023689
3997.7315731048584
[8, 12500] loss: 1.987718
4017.8527290821075
Epoch [8] loss: 6081942.684010
[9,   500] loss: 1.953959
4039.905648469925
[9,  1000] loss: 1.884241
4059.9559276103973
[9,  1500] loss: 1.883559
4079.9484593868256
[9,  2000] loss: 1.890050
4100.0189163684845
[9,  2500] loss: 1.891366
4120.219855070114
[9,  3000] loss: 1.861214
4140.019203901291
[9,  3500] loss: 1.863153
4160.0481033325195
[9,  4000] loss: 1.835284
4180.019961357117
[9,  4500] loss: 1.856914
4200.127820253372
[9,  5000] loss: 1.838657
4220.100313663483
[9,  5500] loss: 1.898328
4240.456715583801
[9,  6000] loss: 1.861315
4260.9415509700775
[9,  6500] loss: 1.859156
4281.448334932327
[9,  7000] loss: 1.789200
4301.96794462204
[9,  7500] loss: 1.953364
4322.131957530975
[9,  8000] loss: 1.905228
4342.048987388611
[9,  8500] loss: 1.902436
4362.266190290451
[9,  9000] loss: 1.849000
4382.207581043243
[9,  9500] loss: 1.845723
4402.256870031357
[9, 10000] loss: 1.840362
4422.437269449234
[9, 10500] loss: 1.912180
4442.698680877686
[9, 11000] loss: 2.004362
4462.670772314072
[9, 11500] loss: 1.932770
4482.56560087204
[9, 12000] loss: 1.961510
4502.656798839569
[9, 12500] loss: 1.880452
4522.966000080109
Epoch [9] loss: 5912114.057055
[10,   500] loss: 1.865609
4543.463503360748
[10,  1000] loss: 1.858697
4563.72470164299
[10,  1500] loss: 1.875729
4583.915918827057
[10,  2000] loss: 1.821114
4604.216923236847
[10,  2500] loss: 1.829574
4624.4271557331085
[10,  3000] loss: 1.900851
4644.404030799866
[10,  3500] loss: 1.813657
4664.496633529663
[10,  4000] loss: 1.793256
4684.404825687408
[10,  4500] loss: 1.812912
4704.470826625824
[10,  5000] loss: 1.853791
4724.502420663834
[10,  5500] loss: 1.876734
4744.4606902599335
[10,  6000] loss: 1.838493
4764.633534193039
[10,  6500] loss: 1.822312
4784.904737472534
[10,  7000] loss: 1.802335
4805.180966615677
[10,  7500] loss: 1.851316
4825.38476896286
[10,  8000] loss: 1.912046
4845.6839900016785
[10,  8500] loss: 1.893709
4866.278880357742
[10,  9000] loss: 1.911162
4886.868407964706
[10,  9500] loss: 1.908423
4907.067852735519
[10, 10000] loss: 1.865575
4927.292019128799
[10, 10500] loss: 1.844286
4947.385583162308
[10, 11000] loss: 1.811988
4967.492187738419
[10, 11500] loss: 1.802541
4987.584604024887
[10, 12000] loss: 1.820522
5007.72366309166
[10, 12500] loss: 1.825415
5027.759619235992
Epoch [10] loss: 5798333.076219
[11,   500] loss: 1.794813
5047.897790431976
[11,  1000] loss: 1.776909
5068.011582374573
[11,  1500] loss: 1.729884
5088.176496744156
[11,  2000] loss: 1.805628
5108.506347179413
[11,  2500] loss: 1.798224
5128.8714373111725
[11,  3000] loss: 1.802112
5149.318661212921
[11,  3500] loss: 1.816143
5169.685663700104
[11,  4000] loss: 1.772379
5189.797279119492
[11,  4500] loss: 1.804892
5209.993007183075
[11,  5000] loss: 1.851289
5230.028953790665
[11,  5500] loss: 1.860738
5250.1700558662415
[11,  6000] loss: 1.800455
5269.95424580574
[11,  6500] loss: 1.834493
5289.860554456711
[11,  7000] loss: 1.782093
5309.863166809082
[11,  7500] loss: 1.863291
5329.8660888671875
[11,  8000] loss: 1.898781
5349.906569242477
[11,  8500] loss: 1.997529
5370.0790338516235
[11,  9000] loss: 1.952249
5390.215883016586
[11,  9500] loss: 1.923155
5410.567706823349
[11, 10000] loss: 1.925834
5430.495305299759
[11, 10500] loss: 1.922243
5450.567987203598
[11, 11000] loss: 1.986630
5471.078792333603
[11, 11500] loss: 2.023487
5491.545161724091
[11, 12000] loss: 1.983432
5511.754342079163
[11, 12500] loss: 1.990085
5532.015321731567
Epoch [11] loss: 5845760.562216
[12,   500] loss: 1.959701
5552.301717758179
[12,  1000] loss: 1.956861
5572.510292053223
[12,  1500] loss: 1.986543
5592.659220457077
[12,  2000] loss: 1.931201
5612.756954669952
[12,  2500] loss: 1.967768
5632.984896183014
[12,  3000] loss: 1.928236
5653.3101596832275
[12,  3500] loss: 1.891586
5673.57327914238
[12,  4000] loss: 1.945112
5693.692873716354
[12,  4500] loss: 1.887870
5713.936505794525
[12,  5000] loss: 1.890482
5734.00569152832
[12,  5500] loss: 1.881880
5754.163902282715
[12,  6000] loss: 1.891455
5774.405854940414
[12,  6500] loss: 1.891142
5794.753373384476
[12,  7000] loss: 1.863547
5815.133399248123
[12,  7500] loss: 1.916286
5835.392632484436
[12,  8000] loss: 1.885547
5855.82200551033
[12,  8500] loss: 1.849188
5875.809822559357
[12,  9000] loss: 1.942438
5895.794729709625
[12,  9500] loss: 1.903753
5915.994324684143
[12, 10000] loss: 1.872295
5935.986865758896
[12, 10500] loss: 1.848808
5955.9624190330505
[12, 11000] loss: 1.864245
5976.137694358826
[12, 11500] loss: 1.840076
5996.208330154419
[12, 12000] loss: 1.795492
6016.121305465698
[12, 12500] loss: 1.784092
6036.2801921367645
Epoch [12] loss: 5941282.023431
[13,   500] loss: 1.806092
6056.791168451309
[13,  1000] loss: 1.802581
6077.68367934227
[13,  1500] loss: 1.787922
6098.0193548202515
[13,  2000] loss: 1.773718
6118.196279287338
[13,  2500] loss: 1.818791
6138.359822034836
[13,  3000] loss: 1.829017
6158.465893507004
[13,  3500] loss: 1.762596
6178.667559146881
[13,  4000] loss: 1.798464
6198.683226823807
[13,  4500] loss: 1.755911
6218.461706399918
[13,  5000] loss: 1.742096
6238.465233564377
[13,  5500] loss: 1.796320
6258.5141389369965
[13,  6000] loss: 1.779594
6278.798795223236
[13,  6500] loss: 1.800731
6299.130841255188
[13,  7000] loss: 1.789218
6319.347438812256
[13,  7500] loss: 1.821750
6339.73427939415
[13,  8000] loss: 1.743443
6360.258238077164
[13,  8500] loss: 1.742845
6380.330750465393
[13,  9000] loss: 1.757059
6400.475886583328
[13,  9500] loss: 1.815822
6420.420859098434
[13, 10000] loss: 1.753125
6440.579233646393
[13, 10500] loss: 1.756973
6460.940525531769
[13, 11000] loss: 1.772568
6481.184124469757
[13, 11500] loss: 1.734460
6501.36593580246
[13, 12000] loss: 1.781654
6521.532187461853
[13, 12500] loss: 1.786050
6541.734944343567
Epoch [13] loss: 5571586.756654
[14,   500] loss: 1.778607
6561.852496147156
[14,  1000] loss: 1.805171
6581.930858373642
[14,  1500] loss: 1.760460
6602.231637001038
[14,  2000] loss: 1.719021
6622.506281375885
[14,  2500] loss: 1.729770
6643.020380735397
[14,  3000] loss: 1.754356
6663.728496313095
[14,  3500] loss: 1.713089
6684.221586704254
[14,  4000] loss: 1.709619
6704.490716218948
[14,  4500] loss: 1.705585
6724.573927640915
[14,  5000] loss: 1.759592
6744.721910715103
[14,  5500] loss: 1.806102
6765.016414165497
[14,  6000] loss: 1.795097
6785.110094547272
[14,  6500] loss: 1.780488
6805.455365180969
[14,  7000] loss: 1.765536
6825.903274774551
[14,  7500] loss: 1.761909
6846.135972738266
[14,  8000] loss: 1.736151
6866.184221506119
[14,  8500] loss: 1.740553
6886.173976182938
[14,  9000] loss: 1.699175
6906.292773723602
[14,  9500] loss: 1.810710
6926.519973039627
[14, 10000] loss: 1.759003
6946.529341459274
[14, 10500] loss: 1.764528
6966.585406780243
[14, 11000] loss: 1.765710
6986.647310733795
[14, 11500] loss: 1.684083
7006.993518829346
[14, 12000] loss: 1.760704
7027.2141234874725
[14, 12500] loss: 1.732793
7047.255815505981
Epoch [14] loss: 5485919.466323
[15,   500] loss: 1.732821
7067.702356815338
[15,  1000] loss: 1.675814
7087.845920085907
[15,  1500] loss: 1.698103
7107.964106559753
[15,  2000] loss: 1.771193
7128.198246002197
[15,  2500] loss: 1.796279
7148.606086015701
[15,  3000] loss: 1.755152
7168.779591798782
[15,  3500] loss: 1.725055
7189.03556895256
[15,  4000] loss: 1.726317
7209.234048128128
[15,  4500] loss: 1.777421
7229.3069677352905
[15,  5000] loss: 1.712801
7249.459510803223
[15,  5500] loss: 1.714840
7270.062204837799
[15,  6000] loss: 1.698514
7290.742022275925
[15,  6500] loss: 1.679055
7311.069776773453
[15,  7000] loss: 1.719039
7331.398487567902
[15,  7500] loss: 1.693533
7351.653025865555
[15,  8000] loss: 1.716615
7371.8710651397705
[15,  8500] loss: 1.700235
7391.925293922424
[15,  9000] loss: 1.676456
7412.48292016983
[15,  9500] loss: 1.705572
7432.9685072898865
[15, 10000] loss: 1.697530
7453.2454533576965
[15, 10500] loss: 1.685146
7473.149809837341
[15, 11000] loss: 1.683046
7493.255480527878
[15, 11500] loss: 1.703765
7513.417704582214
[15, 12000] loss: 1.741060
7533.667859315872
[15, 12500] loss: 1.736371
7554.069890022278
Epoch [15] loss: 5356341.077816
[16,   500] loss: 1.738058
7574.532871961594
[16,  1000] loss: 1.715696
7594.9436757564545
[16,  1500] loss: 1.692104
7615.061717510223
[16,  2000] loss: 1.725951
7635.437751531601
[16,  2500] loss: 1.700718
7655.625025272369
[16,  3000] loss: 1.728194
7675.651982784271
[16,  3500] loss: 1.713569
7695.715191364288
[16,  4000] loss: 1.707969
7715.797036409378
[16,  4500] loss: 1.649995
7735.842325925827
[16,  5000] loss: 1.700592
7755.966085910797
[16,  5500] loss: 1.648250
7776.119432687759
[16,  6000] loss: 1.683843
7796.9245002269745
[16,  6500] loss: 1.688386
7818.344318628311
[16,  7000] loss: 1.695229
7838.864678859711
[16,  7500] loss: 1.736956
7859.919454097748
[16,  8000] loss: 1.695943
7880.671344280243
[16,  8500] loss: 1.689313
7901.296467542648
[16,  9000] loss: 1.696456
7921.432742595673
[16,  9500] loss: 1.717918
7953.475036382675
[16, 10000] loss: 1.729773
7973.909888505936
[16, 10500] loss: 1.707538
7994.068367481232
[16, 11000] loss: 1.692534
8014.36852312088
[16, 11500] loss: 1.731483
8034.490755081177
[16, 12000] loss: 1.729778
8054.450729846954
[16, 12500] loss: 1.736738
8074.547437906265
Epoch [16] loss: 5344184.747756
[17,   500] loss: 1.710191
8096.274992227554
[17,  1000] loss: 1.721895
8116.384863615036
[17,  1500] loss: 1.685841
8136.463360786438
[17,  2000] loss: 1.719913
8156.632955312729
[17,  2500] loss: 1.774121
8176.676990032196
[17,  3000] loss: 1.757562
8196.96770477295
[17,  3500] loss: 1.740103
8217.233453035355
[17,  4000] loss: 1.689587
8237.434152126312
[17,  4500] loss: 1.690717
8257.625168085098
[17,  5000] loss: 1.711539
8277.769134998322
[17,  5500] loss: 1.653036
8297.949583530426
[17,  6000] loss: 1.677621
8318.168603658676
[17,  6500] loss: 1.624860
8338.186980962753
[17,  7000] loss: 1.644697
8358.285459280014
[17,  7500] loss: 1.713823
8378.385419607162
[17,  8000] loss: 1.653040
8398.27788734436
[17,  8500] loss: 1.715930
8418.268194913864
[17,  9000] loss: 1.742483
8438.060618162155
[17,  9500] loss: 1.674444
8458.177794218063
[17, 10000] loss: 1.703554
8478.720275878906
[17, 10500] loss: 1.699515
8498.90085530281
[17, 11000] loss: 1.669938
8519.178369045258
[17, 11500] loss: 1.687478
8539.652326583862
[17, 12000] loss: 1.705229
8560.207599878311
[17, 12500] loss: 1.671953
8580.52109336853
Epoch [17] loss: 5312006.104625
[18,   500] loss: 1.661171
8600.793604850769
[18,  1000] loss: 1.627757
8621.135308027267
[18,  1500] loss: 1.732394
8641.046252965927
[18,  2000] loss: 1.713870
8660.980170965195
[18,  2500] loss: 1.735398
8680.992758989334
[18,  3000] loss: 1.685698
8700.98844742775
[18,  3500] loss: 1.664395
8720.9243516922
[18,  4000] loss: 1.687057
8740.923485279083
[18,  4500] loss: 1.665198
8761.019982814789
[18,  5000] loss: 1.675190
8780.892168045044
[18,  5500] loss: 1.662319
8800.87028169632
[18,  6000] loss: 1.670086
8821.137125730515
[18,  6500] loss: 1.647806
8841.48333287239
[18,  7000] loss: 1.637319
8861.904254674911
[18,  7500] loss: 1.625319
8882.304180145264
[18,  8000] loss: 1.592374
8902.676930427551
[18,  8500] loss: 1.659005
8922.9956407547
[18,  9000] loss: 1.663777
8943.423742055893
[18,  9500] loss: 1.613166
8963.714304685593
[18, 10000] loss: 1.636150
8983.947119474411
[18, 10500] loss: 1.647641
9004.044738769531
[18, 11000] loss: 1.601106
9024.23583984375
[18, 11500] loss: 1.644562
9044.547717094421
[18, 12000] loss: 1.626833
9065.164610385895
[18, 12500] loss: 1.636687
9085.910879611969
Epoch [18] loss: 5180058.090380
[19,   500] loss: 1.648462
9106.262412786484
[19,  1000] loss: 1.659853
9126.506785154343
[19,  1500] loss: 1.604217
9146.872640609741
[19,  2000] loss: 1.610828
9167.173823595047
[19,  2500] loss: 1.609582
9187.55358171463
[19,  3000] loss: 1.585697
9207.836308956146
[19,  3500] loss: 1.668805
9228.186985731125
[19,  4000] loss: 1.633637
9248.510612487793
[19,  4500] loss: 1.611417
9268.774958133698
[19,  5000] loss: 1.595050
9289.040798425674
[19,  5500] loss: 1.629999
9309.365319013596
[19,  6000] loss: 1.603872
9329.46819472313
[19,  6500] loss: 1.587223
9349.764425992966
[19,  7000] loss: 1.607635
9370.266843795776
[19,  7500] loss: 1.557890
9390.68969964981
[19,  8000] loss: 1.603576
9411.156724691391
[19,  8500] loss: 1.753530
9431.343205690384
[19,  9000] loss: 1.699084
9451.528397798538
[19,  9500] loss: 1.638132
9471.818977355957
[19, 10000] loss: 1.604096
9492.158307790756
[19, 10500] loss: 1.620867
9512.425435066223
[19, 11000] loss: 1.605611
9532.780333518982
[19, 11500] loss: 1.609605
9553.302073478699
[19, 12000] loss: 1.627409
9573.815472602844
[19, 12500] loss: 1.591723
9594.321257591248
Epoch [19] loss: 5087997.481602
[20,   500] loss: 1.572149
9614.82258439064
[20,  1000] loss: 1.598270
9635.054558038712
[20,  1500] loss: 1.662875
9655.515936374664
[20,  2000] loss: 1.629185
9676.52114534378
[20,  2500] loss: 1.600544
9697.092334985733
[20,  3000] loss: 1.595443
9717.243844985962
[20,  3500] loss: 1.572000
9737.47916507721
[20,  4000] loss: 1.563945
9757.536743164062
[20,  4500] loss: 1.586652
9777.74969792366
[20,  5000] loss: 1.574497
9798.140918016434
[20,  5500] loss: 1.603634
9818.68503689766
[20,  6000] loss: 1.566204
9839.059028863907
[20,  6500] loss: 1.562023
9859.1877617836
[20,  7000] loss: 1.567536
9879.566054344177
[20,  7500] loss: 1.626105
9900.100302934647
[20,  8000] loss: 1.585436
9920.369752168655
[20,  8500] loss: 1.563461
9940.584692955017
[20,  9000] loss: 1.544299
9960.912928819656
[20,  9500] loss: 1.563631
9981.233334064484
[20, 10000] loss: 1.577047
10001.653080701828
[20, 10500] loss: 1.583538
10022.113645792007
[20, 11000] loss: 1.614794
10042.380281686783
[20, 11500] loss: 1.541016
10062.257733345032
[20, 12000] loss: 1.650791
10082.393597364426
[20, 12500] loss: 1.729774
10102.509013414383
Epoch [20] loss: 4983338.485514
Finished Training
Saving model to /data/s4091221/trained-models/wide_resnet50_22020-02-26 09:57:33.543116
GroundTruth:    cat  ship  ship plane
Sending data to GPU
Sending model to GPU
tensor([[ -1.9716,  -2.1828,  -0.3753,   0.5940,  -0.4200,   0.3568,  -0.4443,
          -0.1355,  -1.8874,  -1.6667],
        [ -2.4667,  -2.7786,  -6.9749,  -9.5835,  -7.3975, -10.1524,  -8.3095,
          -8.1840,  -2.2266,  -3.5949],
        [ -1.4909,  -2.0633,  -3.4589,  -4.2581,  -3.3965,  -5.0980,  -4.4335,
          -3.4980,  -1.2170,  -2.1688],
        [ -9.3272, -10.9242, -12.0919, -15.0548, -12.6755, -14.0802, -14.8678,
         -13.2550,  -8.9538, -11.9799]], device='cuda:0',
       grad_fn=<AddmmBackward>)
Predicted:    cat  ship  ship  ship
Accuracy of the network on the 4000.0 test images: 40 %


###############################################################################
Peregrine Cluster
Job 9751127 for user 's4091221'
Finished at: Wed Feb 26 09:58:21 CET 2020

Job details:
============

Name                : wide_resnet50.sh
User                : s4091221
Partition           : gpu
Nodes               : pg-gpu34
Cores               : 12
State               : COMPLETED
Submit              : 2020-02-25T16:13:53
Start               : 2020-02-25T21:38:11
End                 : 2020-02-26T09:58:21
Reserved walltime   : 20:00:00
Used walltime       : 12:20:10
Used CPU time       : 13:04:26 (efficiency:  8.83%)
% User (Computation): 95.56%
% System (I/O)      :  4.44%
Mem reserved        : 12000M/node
Max Mem used        : 3.00G (pg-gpu34)
Max Disk Write      : 2.45G (pg-gpu34)
Max Disk Read       : 6.33G (pg-gpu34)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
